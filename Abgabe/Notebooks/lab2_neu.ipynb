{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Preprocessing Pipeline\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook provides a comprehensive workflow for text data analysis and preprocessing.\n",
    "\n",
    "- **Data Loading**: Set up the environment, load and explore text datasets, perform statistical analysis, and create reusable data loading functions\n",
    "- **Preprocessing**: Apply various NLP preprocessing techniques using NLTK and SpaCy, analyze the impact of different methods, and develop a reusable preprocessing pipeline\n",
    "\n",
    "The notebook is structured to systematically work through each step of the text analysis pipeline, from the instructions of the second lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "This section covers the initial setup and data loading phase. We will install necessary libraries, load the dataset, explore its structure and characteristics, perform statistical analysis, filter the data to text-only content, and create reusable functions for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Explore Dataset\n",
    "\n",
    "Loading the dataset into a pandas DataFrame and performing initial exploration. This includes viewing the first few rows, checking data types, examining the shape of the dataset, and identifying any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Basic Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Row count, Column count:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nFirst 10 Rows:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Initial Data Exploration with NLTK\n",
    "\n",
    "Using NLTK to perform basic text exploration and tokenization on a sample of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH NLTK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show sample tokenization on a few tweets\n",
    "print(\"\\nSample Tweet Tokenization:\")\n",
    "sample_text = df['text'].iloc[0]\n",
    "print(f\"Original: {sample_text}\")\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nNumber of English stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Initial Data Exploration with spaCy\n",
    "\n",
    "Using spaCy to perform basic linguistic analysis on sample tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH spaCy\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze a sample tweet\n",
    "sample_text = df['text'].iloc[5]\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(f\"\\nSample Tweet: {sample_text}\")\n",
    "print(f\"\\nTokens: {[token.text for token in doc]}\")\n",
    "print(f\"\\nPOS Tags: {[(token.text, token.pos_) for token in doc][:10]}\")\n",
    "print(f\"\\nEntities: {[(ent.text, ent.label_) for ent in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculate Statistics\n",
    "\n",
    "Computing descriptive statistics on the dataset to understand the data distribution. This includes category distributions, temporal patterns, duplicate analysis, text length statistics, word frequency analysis, and named entity statistics using pandas and NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Category/Topic Distribution Analysis\n",
    "\n",
    "Analyzing how many tweets are available for each topic category using pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Convert label_name to lists (they're already arrays, just ensure they're lists)\n",
    "df['topics_list'] = df['label_name'].apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "\n",
    "# Count tweets per category\n",
    "all_topics = []\n",
    "for topics in df['topics_list']:\n",
    "    all_topics.extend(topics)\n",
    "\n",
    "topic_counts = Counter(all_topics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "topic_df = pd.DataFrame(\n",
    "    topic_counts.items(), \n",
    "    columns=['Topic', 'Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TWEETS PER CATEGORY\")\n",
    "print(\"=\"*50)\n",
    "print(topic_df.to_string(index=False))\n",
    "print(f\"\\nTotal unique topics: {len(topic_counts)}\")\n",
    "\n",
    "# Multi-label statistics\n",
    "df['num_topics'] = df['topics_list'].apply(len)\n",
    "print(f\"\\nTweets with 1 topic: {(df['num_topics'] == 1).sum()} ({(df['num_topics'] == 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"Tweets with 2+ topics: {(df['num_topics'] > 1).sum()} ({(df['num_topics'] > 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nMax topics per tweet: {df['num_topics'].max()}\")\n",
    "print(f\"Average topics per tweet: {df['num_topics'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Temporal Distribution Analysis\n",
    "\n",
    "Analyzing the distribution of tweets across different time periods using pandas datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date_parsed'].dt.year\n",
    "df['month'] = df['date_parsed'].dt.month\n",
    "df['day_of_week'] = df['date_parsed'].dt.day_name()\n",
    "\n",
    "# Count tweets per year\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nTweets per Year:\")\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} tweets ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDate range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")\n",
    "print(f\"Total days covered: {(df['date_parsed'].max() - df['date_parsed'].min()).days} days\")\n",
    "\n",
    "# Month distribution\n",
    "print(\"\\nTweets per Month (across all years):\")\n",
    "month_counts = df['month'].value_counts().sort_index()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for month, count in month_counts.items():\n",
    "    print(f\"  {month_names[month-1]}: {count:,} tweets\")\n",
    "\n",
    "# Day of week distribution\n",
    "print(\"\\nTweets per Day of Week:\")\n",
    "dow_counts = df['day_of_week'].value_counts()\n",
    "for day, count in dow_counts.items():\n",
    "    print(f\"  {day}: {count:,} tweets ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Duplicate Detection Statistics\n",
    "\n",
    "Identifying and quantifying exact and potential near-duplicate tweets using pandas string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for exact text duplicates\n",
    "duplicate_texts = df['text'].duplicated().sum()\n",
    "print(f\"Exact duplicate tweets: {duplicate_texts} ({duplicate_texts / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = df['id'].duplicated().sum()\n",
    "print(f\"Duplicate IDs: {duplicate_ids}\")\n",
    "\n",
    "# Show sample duplicates if any exist\n",
    "if duplicate_texts > 0:\n",
    "    print(\"\\nSample Duplicate Tweets:\")\n",
    "    duplicated_mask = df['text'].duplicated(keep=False)\n",
    "    sample_duplicates = df[duplicated_mask].groupby('text').head(2)\n",
    "    print(sample_duplicates[['text', 'date', 'label_name']].head(6).to_string())\n",
    "else:\n",
    "    print(\"\\nâœ“ No exact duplicate tweets found!\")\n",
    "\n",
    "# Near-duplicate detection (same first 50 characters)\n",
    "df['text_start'] = df['text'].str.lower().str.strip().str[:50]\n",
    "potential_near_dupes = df['text_start'].duplicated().sum()\n",
    "print(f\"\\nPotential near-duplicates (same first 50 chars): {potential_near_dupes} ({potential_near_dupes / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# List all near-duplicate IDs grouped\n",
    "if potential_near_dupes > 0:\n",
    "    print(\"\\nAll Near-Duplicate Groups (sorted by group size):\")\n",
    "    near_dupe_mask = df['text_start'].duplicated(keep=False)\n",
    "    near_dupe_df = df[near_dupe_mask][['text_start', 'id', 'text']]\n",
    "    \n",
    "    # Group by text_start and collect IDs\n",
    "    grouped = near_dupe_df.groupby('text_start')\n",
    "    \n",
    "    # Sort groups by size (largest first)\n",
    "    group_sizes = grouped.size().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal near-duplicate groups: {len(group_sizes)}\")\n",
    "    print(f\"Largest group size: {group_sizes.max()}\\n\")\n",
    "    \n",
    "    for i, (text_start, size) in enumerate(group_sizes.items(), 1):\n",
    "        group_ids = grouped.get_group(text_start)['id'].tolist()\n",
    "        print(f\"Group {i} (Size: {size}):\")\n",
    "        print(f\"  Text preview: {text_start}...\")\n",
    "        print(f\"  IDs: {group_ids}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nâœ“ No potential near-duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Text Length and Composition Statistics\n",
    "\n",
    "Analyzing text characteristics using pandas string methods and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "df['avg_word_length'] = df['text'].str.replace(' ', '').str.len() / df['word_count']\n",
    "\n",
    "# Count special characters\n",
    "df['hashtag_count'] = df['text'].str.count('#')\n",
    "df['mention_count'] = df['text'].str.count('@')\n",
    "df['url_count'] = df['text'].str.count('http')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT LENGTH AND COMPOSITION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nCharacter Length Statistics:\")\n",
    "print(f\"  Mean: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {df['text_length'].median():.1f} characters\")\n",
    "print(f\"  Std Dev: {df['text_length'].std():.1f} characters\")\n",
    "print(f\"  Min: {df['text_length'].min()} characters\")\n",
    "print(f\"  Max: {df['text_length'].max()} characters\")\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(f\"  Mean: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"  Median: {df['word_count'].median():.1f} words\")\n",
    "print(f\"  Std Dev: {df['word_count'].std():.1f} words\")\n",
    "print(f\"  Min: {df['word_count'].min()} words\")\n",
    "print(f\"  Max: {df['word_count'].max()} words\")\n",
    "\n",
    "print(\"\\nAverage Word Length:\")\n",
    "print(f\"  Mean: {df['avg_word_length'].mean():.2f} characters per word\")\n",
    "print(f\"  Median: {df['avg_word_length'].median():.2f} characters per word\")\n",
    "\n",
    "print(\"\\nSpecial Character Statistics:\")\n",
    "print(f\"  Tweets with hashtags: {(df['hashtag_count'] > 0).sum()} ({(df['hashtag_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average hashtags per tweet: {df['hashtag_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with mentions: {(df['mention_count'] > 0).sum()} ({(df['mention_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average mentions per tweet: {df['mention_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with URLs: {(df['url_count'] > 0).sum()} ({(df['url_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show extreme examples\n",
    "print(\"\\nExtreme Examples:\")\n",
    "print(f\"\\nShortest tweet ({df['text_length'].min()} chars):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmin(), 'text']}\")\n",
    "print(f\"\\nLongest tweet ({df['text_length'].max()} chars, first 150):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmax(), 'text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Word Frequency Statistics (NLTK)\n",
    "\n",
    "Analyzing word frequency patterns using NLTK tokenization and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORD FREQUENCY STATISTICS (NLTK)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tokenize all tweets\n",
    "all_tokens = []\n",
    "for text in df['text']:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords, keep only meaningful words\n",
    "    tokens = [\n",
    "        t for t in tokens \n",
    "        if t not in string.punctuation \n",
    "        and t not in stop_words \n",
    "        and len(t) > 2\n",
    "    ]\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "top_words = pd.DataFrame(\n",
    "    token_freq.most_common(30), \n",
    "    columns=['Word', 'Frequency']\n",
    ")\n",
    "\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"  Total tokens (after filtering): {len(all_tokens):,}\")\n",
    "print(f\"  Unique words (vocabulary size): {len(token_freq):,}\")\n",
    "print(f\"  Average token frequency: {len(all_tokens) / len(token_freq):.2f}\")\n",
    "print(f\"  Words appearing only once: {sum(1 for count in token_freq.values() if count == 1):,}\")\n",
    "print(f\"  Words appearing 10+ times: {sum(1 for count in token_freq.values() if count >= 10):,}\")\n",
    "\n",
    "print(f\"\\nTop 30 Most Common Words:\")\n",
    "print(top_words.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6 Named Entity Statistics (spaCy)\n",
    "\n",
    "Analyzing named entity distributions using spaCy NER and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NAMED ENTITY STATISTICS (spaCy)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAnalyzing {len(df)} tweets... This may take a few minutes...\")\n",
    "\n",
    "# Extract entities from all tweets\n",
    "all_entities = []\n",
    "tweets_with_entities = 0\n",
    "entity_counts_per_tweet = []\n",
    "\n",
    "# Use nlp.pipe for better performance\n",
    "texts = df['text'].apply(lambda x: x[:500]).tolist()\n",
    "\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=50), total=len(df), desc=\"Processing\"):\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    entity_counts_per_tweet.append(len(entities))\n",
    "    if entities:\n",
    "        tweets_with_entities += 1\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "# Add entity count to dataframe\n",
    "df['entity_count'] = entity_counts_per_tweet\n",
    "\n",
    "print(f\"\\nEntity Detection Statistics:\")\n",
    "print(f\"  Tweets with entities: {tweets_with_entities:,} ({tweets_with_entities/len(df)*100:.1f}%)\")\n",
    "print(f\"  Tweets without entities: {len(df) - tweets_with_entities:,} ({(len(df) - tweets_with_entities)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total entities found: {len(all_entities):,}\")\n",
    "print(f\"  Average entities per tweet: {df['entity_count'].mean():.2f}\")\n",
    "print(f\"  Median entities per tweet: {df['entity_count'].median():.0f}\")\n",
    "print(f\"  Max entities in a tweet: {df['entity_count'].max()}\")\n",
    "\n",
    "# Count entity types\n",
    "entity_types = Counter([label for text, label in all_entities])\n",
    "entity_type_df = pd.DataFrame(\n",
    "    entity_types.most_common(), \n",
    "    columns=['Entity Type', 'Count']\n",
    ")\n",
    "entity_type_df['Percentage'] = (entity_type_df['Count'] / len(all_entities) * 100).round(1)\n",
    "\n",
    "print(\"\\nEntity Type Distribution:\")\n",
    "print(entity_type_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nEntity Type Legend:\")\n",
    "print(\"\\nEntity Type Legend (Complete):\")\n",
    "print(\"  PERSON      = People, including fictional characters\")\n",
    "print(\"  ORG         = Companies, agencies, institutions, organizations\")\n",
    "print(\"  GPE         = Countries, cities, states (Geo-Political Entities)\")\n",
    "print(\"  DATE        = Absolute or relative dates or periods\")\n",
    "print(\"  CARDINAL    = Numerals that do not fall under another type\")\n",
    "print(\"  MONEY       = Monetary values, including unit\")\n",
    "print(\"  TIME        = Times smaller than a day\")\n",
    "print(\"  NORP        = Nationalities, religious or political groups\")\n",
    "print(\"  ORDINAL     = First, second, third, etc.\")\n",
    "print(\"  WORK_OF_ART = Titles of books, songs, movies, etc.\")\n",
    "print(\"  EVENT       = Named hurricanes, battles, wars, sports events\")\n",
    "print(\"  PRODUCT     = Objects, vehicles, foods, etc. (not services)\")\n",
    "print(\"  LOC         = Non-GPE locations, mountain ranges, bodies of water\")\n",
    "print(\"  FAC         = Buildings, airports, highways, bridges, etc.\")\n",
    "print(\"  QUANTITY    = Measurements, as of weight or distance\")\n",
    "print(\"  LAW         = Named documents made into laws\")\n",
    "print(\"  PERCENT     = Percentage, including '%'\")\n",
    "print(\"  LANGUAGE    = Any named language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filter to Text Only\n",
    "\n",
    "Filtering the dataset to extract only text columns and remove any non-textual data. This step ensures that subsequent processing focuses exclusively on textual content and handles any data type conversions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name'):\n",
    "    \"\"\"\n",
    "    Filter dataset to only text and label columns, removing all numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe to filter\n",
    "    text_col : str\n",
    "        Name of the text column (default: 'text')\n",
    "    label_col : str\n",
    "        Name of the label column (default: 'label_name')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Filtered dataframe with only text columns and no numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select only the text and label_name columns\n",
    "    df_filtered = dataframe[[text_col, label_col]].copy()\n",
    "    \n",
    "    # Step 2: Remove all numbers from the text column using regex\n",
    "    # This removes all digits (0-9) from the text\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    # Step 3: Handle label_name - convert to string if it's a list, then remove numbers\n",
    "    # First check if it's already a list or needs conversion\n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        # Keep as list, no number removal needed (labels are text)\n",
    "        pass\n",
    "    else:\n",
    "        # If it's a string representation, convert and clean\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    # Step 4: Clean up any extra whitespace created by removing numbers\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Apply the filtering function\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILTERING TO TEXT ONLY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show original dataset info\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Apply the filter\n",
    "df_text_only = filter_to_text_only(df)\n",
    "\n",
    "# Show filtered dataset info\n",
    "print(\"\\nFiltered Dataset (Text Only - No Numbers):\")\n",
    "print(f\"  Shape: {df_text_only.shape}\")\n",
    "print(f\"  Columns: {df_text_only.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df_text_only['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Show examples of number removal\n",
    "print(\"\\nExamples of Number Removal:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original:  {df['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Filtered:  {df_text_only['text'].iloc[i][:80]}...\")\n",
    "\n",
    "# Show label_name comparison\n",
    "print(\"\\nLabel Name Comparison:\")\n",
    "print(f\"  Original label_name: {df['label_name'].iloc[0]}\")\n",
    "print(f\"  Filtered label_name: {df_text_only['label_name'].iloc[0]}\")\n",
    "\n",
    "# Statistics on number removal\n",
    "original_chars = df['text'].str.len().sum()\n",
    "filtered_chars = df_text_only['text'].str.len().sum()\n",
    "chars_removed = original_chars - filtered_chars\n",
    "\n",
    "print(f\"\\nCharacter Statistics:\")\n",
    "print(f\"  Original total characters: {original_chars:,}\")\n",
    "print(f\"  Filtered total characters: {filtered_chars:,}\")\n",
    "print(f\"  Characters removed (numbers): {chars_removed:,} ({chars_removed/original_chars*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ Dataset successfully filtered to text only (numbers removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Reusable Loading and Filtering Functions\n",
    "\n",
    "Reusable functions that encapsulate the data loading and text-filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "tweets_raw = dataset.to_pandas()\n",
    "\n",
    "# Filter the dataset to text only (no numbers)\n",
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name', label_num_col='label'):\n",
    "    df_filtered = dataframe[[text_col, label_col, label_num_col]].copy()\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        pass\n",
    "    else:\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "tweets_text_only = filter_to_text_only(tweets_raw)\n",
    "\n",
    "print(\"\\nâœ“ Dataset successfully loaded and filtered to text only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "This section focuses on text preprocessing techniques. We will review common preprocessing methods, apply them systematically using NLTK and SpaCy, analyze how the order of operations affects results, evaluate the usefulness of each method for different scenarios, and create a reusable preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Methods Used (in Order)\n",
    "\n",
    "#### 1. **Remove RT indicator**\n",
    "   - **Purpose:** Removes retweet markers that don't carry semantic meaning\n",
    "   - **Order:** FIRST - removes noise before any text processing\n",
    "\n",
    "#### 2. **Remove placeholders (USERNAME, URL, mentions)**\n",
    "   - **Purpose:** Removes dataset-specific placeholders that don't contribute to topic classification\n",
    "   - **Order:** EARLY - clean structural noise before text normalization\n",
    "\n",
    "#### 3. **Convert emojis to text**\n",
    "   - **Purpose:** Transforms emojis into descriptive words that preserve semantic meaning (ðŸŽ® â†’ video game)\n",
    "   - **Order:** AFTER placeholders, BEFORE hashtags - emojis may appear anywhere in text\n",
    "\n",
    "#### 4. **Extract hashtag text**\n",
    "   - **Purpose:** Preserves topic-relevant keywords from hashtags (#Gaming â†’ Gaming)\n",
    "   - **Order:** AFTER emoji conversion - hashtags rarely contain emojis but order matters for consistency\n",
    "\n",
    "#### 5. **Segment CamelCase words**\n",
    "   - **Purpose:** Splits compound words for better tokenization (GameOfThrones â†’ Game Of Thrones)\n",
    "   - **Order:** BEFORE lowercase - capitalization patterns guide segmentation\n",
    "\n",
    "#### 6. **Normalize whitespace and lowercase**\n",
    "   - **Purpose:** Standardizes text format for consistent processing\n",
    "   - **Order:** AFTER special token handling - ensures uniform text before tokenization\n",
    "\n",
    "#### 7. **Tokenize with SpaCy**\n",
    "   - **Purpose:** Splits text into linguistic tokens with POS and lemma information\n",
    "   - **Order:** AFTER normalization - requires clean, lowercase text\n",
    "\n",
    "#### 8. **Filter and lemmatize tokens**\n",
    "   - **Purpose:** Removes noise tokens and reduces words to base forms\n",
    "   - **Order:** LAST - operates on clean, filtered tokens\n",
    "\n",
    "### Why This Order Matters\n",
    "\n",
    "**Critical Dependencies:**\n",
    "- Emoji conversion **before** lowercase â†’ emoji descriptions use underscores that get normalized\n",
    "- Special tokens **before** tokenization â†’ removes as complete units\n",
    "- Lowercase **before** tokenization â†’ consistent stopword matching\n",
    "- Tokenization **before** filtering â†’ need tokens to filter\n",
    "- Lemmatization **last** â†’ final transformation on clean tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reusable Preprocessing Pipeline\n",
    "\n",
    "Development of a modular, configurable preprocessing function that can be easily reused in future labs. The pipeline allows for flexible selection of preprocessing steps and parameters, making it adaptable to different text analysis tasks and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Load SpaCy model\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"âœ“ SpaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"Installing SpaCy model...\")\n",
    "    import os\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"âœ“ SpaCy model loaded successfully\")\n",
    "\n",
    "# Import emoji package for emoji handling\n",
    "import emoji\n",
    "\n",
    "def is_latin_alphabet(word):\n",
    "    \"\"\"\n",
    "    Check if a word contains only Latin alphabet characters.\n",
    "    Filters out words with Cyrillic, Arabic, Chinese, etc.\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return False\n",
    "    return all(ord('a') <= ord(c.lower()) <= ord('z') for c in word)\n",
    "\n",
    "def segment_camelcase(text):\n",
    "    \"\"\"\n",
    "    Segment CamelCase words into separate words without regex.\n",
    "    Example: 'GameOfThrones' â†’ 'Game Of Thrones'\n",
    "    This is important for hashtags like #GameOfThrones after removing the #\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        # Add current character\n",
    "        result.append(char)\n",
    "        \n",
    "        # Check if we need to insert a space\n",
    "        if i < len(text) - 1:\n",
    "            current = char\n",
    "            next_char = text[i + 1]\n",
    "            \n",
    "            # Case 1: lowercase â†’ uppercase (e.g., 'e' â†’ 'O' in 'GameOf')\n",
    "            if current.islower() and next_char.isupper():\n",
    "                result.append(' ')\n",
    "            \n",
    "            # Case 2: uppercase â†’ uppercase â†’ lowercase (e.g., 'HTML' â†’ 'Parser')\n",
    "            elif i < len(text) - 2:\n",
    "                after_next = text[i + 2]\n",
    "                if current.isupper() and next_char.isupper() and after_next.islower():\n",
    "                    result.append(' ')\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"\n",
    "    Topic-optimized preprocessing for tweet classification.\n",
    "    Preserves topic-relevant information while removing noise.\n",
    "    Removes special characters, emojis, and non-Latin script words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Step 1: Remove RT (retweet indicator)\n",
    "    text = text.replace('RT ', ' ').replace('rt ', ' ')\n",
    "    \n",
    "    # Step 2: Remove URLs and placeholders\n",
    "    text = text.replace('{{URL}}', ' ')\n",
    "    text = text.replace('{{USERNAME}}', ' ')\n",
    "    for protocol in ['https://', 'http://', 'www.']:\n",
    "        if protocol in text:\n",
    "            parts = text.split(protocol)\n",
    "            text = parts[0] + ' ' + ' '.join([' '.join(p.split()[1:]) if p.split() else '' for p in parts[1:]])\n",
    "    \n",
    "    # Step 3: Remove mentions\n",
    "    words_list = text.split()\n",
    "    words_list = [w for w in words_list if not (w.startswith('{@') or w.startswith('@'))]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 3.5: Convert emojis to text descriptions (ðŸŽ® â†’ video game)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = text.replace('_', ' ')\n",
    "    \n",
    "    # Step 4: Extract hashtag text (#Gaming â†’ Gaming, #GameOfThrones â†’ GameOfThrones)\n",
    "    words_list = text.split()\n",
    "    words_list = [w[1:] if w.startswith('#') else w for w in words_list]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 4.5: Segment CamelCase words\n",
    "    # GameOfThrones â†’ Game Of Thrones\n",
    "    text = segment_camelcase(text)\n",
    "    \n",
    "    # Step 5: Normalize whitespace and lowercase\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 6: Tokenize with SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 7: Filter and lemmatize tokens\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        # Skip punctuation\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        \n",
    "        # Skip if not alphabetic (removes special characters, emojis, numbers)\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "        \n",
    "        # Skip tokens shorter than 2 characters\n",
    "        if len(token.text) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Remove stopwords (using SpaCy's stopword detection)\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Check if word uses Latin alphabet (filters out Cyrillic, Arabic, Chinese, etc.)\n",
    "        if not is_latin_alphabet(token.text):\n",
    "            continue\n",
    "        \n",
    "        # Use lemmatized form\n",
    "        processed_tokens.append(token.lemma_)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "tweets_preprocessed_train = tweets_text_only.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "tweets_preprocessed_train['text'] = tweets_preprocessed_train['text'].apply(preprocess_tweet)\n",
    "\n",
    "print(\"\\nâœ“ Preprocessing complete!\")\n",
    "print(f\"âœ“ Processed {len(tweets_preprocessed_train)} tweets\")\n",
    "print(f\"âœ“ Original 'tweets_text_only' unchanged | Processed data in 'tweets_preprocessed_train'\")\n",
    "\n",
    "# Save the DataFrame to the Data folder\n",
    "import os\n",
    "\n",
    "# Create Data folder if it does not exist\n",
    "os.makedirs('../Data', exist_ok=True)\n",
    "\n",
    "# Save the data as parquet\n",
    "output_path = '../Data/tweets_preprocessed_train.parquet'\n",
    "tweets_preprocessed_train.to_parquet(output_path, index=False)\n",
    "\n",
    "# Save the data as CSV\n",
    "output_path_csv = '../Data/tweets_preprocessed_train.csv'\n",
    "tweets_preprocessed_train.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_train.shape}\")\n",
    "\n",
    "print(f\"\\nâœ“ Training DataFrame saved to: {output_path}\")\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_train.columns)}\")\n",
    "print(f\"âœ“ Training DataFrame saved to: {output_path_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing for Test and Validation Splits\n",
    "\n",
    "Apply the same preprocessing pipeline to the test and validation splits from HuggingFace to ensure consistency between training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and validation splits from HuggingFace\n",
    "print(\"Loading test and validation splits from HuggingFace...\")\n",
    "test_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"test_2021\")\n",
    "val_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"validation_2021\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "test_raw = test_dataset.to_pandas()\n",
    "val_raw = val_dataset.to_pandas()\n",
    "\n",
    "print(f\"Test samples: {len(test_raw):,}\")\n",
    "print(f\"Validation samples: {len(val_raw):,}\")\n",
    "\n",
    "# Apply the same text-only filter (remove numbers)\n",
    "test_text_only = filter_to_text_only(test_raw)\n",
    "val_text_only = filter_to_text_only(val_raw)\n",
    "\n",
    "# Apply the same preprocessing pipeline\n",
    "tweets_preprocessed_test = test_text_only.copy()\n",
    "tweets_preprocessed_test['text'] = tweets_preprocessed_test['text'].apply(preprocess_tweet)\n",
    "\n",
    "tweets_preprocessed_validation = val_text_only.copy()\n",
    "tweets_preprocessed_validation['text'] = tweets_preprocessed_validation['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Save test data as parquet\n",
    "test_output_path = '../Data/tweets_preprocessed_test.parquet'\n",
    "tweets_preprocessed_test.to_parquet(test_output_path, index=False)\n",
    "\n",
    "# Save test data as CSV\n",
    "test_output_path_csv = '../Data/tweets_preprocessed_test.csv'\n",
    "tweets_preprocessed_test.to_csv(test_output_path_csv, index=False)\n",
    "\n",
    "# Save validation data as parquet\n",
    "val_output_path = '../Data/tweets_preprocessed_validation.parquet'\n",
    "tweets_preprocessed_validation.to_parquet(val_output_path, index=False)\n",
    "\n",
    "# Save validation data as CSV\n",
    "val_output_path_csv = '../Data/tweets_preprocessed_validation.csv'\n",
    "tweets_preprocessed_validation.to_csv(val_output_path_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Test data preprocessing complete!\")\n",
    "\n",
    "print(f\"âœ“ Saved to: {test_output_path}\")\n",
    "\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_validation.columns)}\")\n",
    "\n",
    "print(f\"âœ“ Saved to: {test_output_path_csv}\")\n",
    "\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_validation.shape}\")\n",
    "\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_test.shape}\")\n",
    "\n",
    "print(f\"âœ“ Saved to: {val_output_path_csv}\")\n",
    "\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_test.columns)}\")\n",
    "\n",
    "print(f\"âœ“ Saved to: {val_output_path}\")\n",
    "\n",
    "print(f\"\\nâœ“ Validation data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Data for Training\n",
    "\n",
    "This section addresses a common issue in multi-label classification: **class imbalance**. Some labels in our dataset have very few training samples, which can negatively impact model performance during training.\n",
    "\n",
    "**Why this matters:**\n",
    "- Labels with too few samples cannot be learned effectively by the model\n",
    "- Class imbalance can lead to biased predictions toward majority classes\n",
    "- Having consistent labels across train/validation/test splits is crucial for proper evaluation\n",
    "\n",
    "**What this function does:**\n",
    "1. **Visualizes** the distribution of labels using matplotlib to identify potential imbalances\n",
    "2. **Removes** labels with fewer than a specified threshold (default: 180 tweets) from all data splits\n",
    "3. **Updates** the binary label vectors to reflect the removed labels\n",
    "4. **Ensures consistency** by applying the same filtering to train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def parse_label_names(label_str):\n",
    "    \"\"\"\n",
    "    Parse the label_name string into a list of labels.\n",
    "    The format is numpy-style: ['label1' 'label2'] instead of ['label1', 'label2']\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    label_str : str\n",
    "        String representation of labels in numpy array format\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of label strings\n",
    "    \"\"\"\n",
    "    if isinstance(label_str, str):\n",
    "        label_str = label_str.strip()\n",
    "        if label_str.startswith('[') and label_str.endswith(']'):\n",
    "            content = label_str[1:-1]\n",
    "            items = re.findall(r\"'([^']*)'\" , content)\n",
    "            return items\n",
    "    return []\n",
    "\n",
    "\n",
    "def prepare_data_for_training(df_train, df_validation, df_test, min_samples=180, show_plot=True):\n",
    "    \"\"\"\n",
    "    Prepare datasets for training by removing labels with insufficient samples.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Counts the number of tweets per label in the training set\n",
    "    2. Visualizes the label distribution using matplotlib\n",
    "    3. Identifies labels with fewer than min_samples tweets\n",
    "    4. Removes tweets containing only the underrepresented labels from all splits\n",
    "    5. Updates the label vectors and label_name columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training dataframe with 'text', 'label_name', and 'label' columns\n",
    "    df_validation : pd.DataFrame\n",
    "        Validation dataframe with same columns\n",
    "    df_test : pd.DataFrame\n",
    "        Test dataframe with same columns\n",
    "    min_samples : int, default=180\n",
    "        Minimum number of tweets required for a label to be kept\n",
    "    show_plot : bool, default=True\n",
    "        Whether to display the matplotlib visualization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (pd.DataFrame, pd.DataFrame, pd.DataFrame, list)\n",
    "        Filtered train, validation, test dataframes, and list of removed labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    train = df_train.copy()\n",
    "    val = df_validation.copy()\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    # Parse labels for all datasets\n",
    "    train['parsed_labels'] = train['label_name'].apply(parse_label_names)\n",
    "    val['parsed_labels'] = val['label_name'].apply(parse_label_names)\n",
    "    test['parsed_labels'] = test['label_name'].apply(parse_label_names)\n",
    "    \n",
    "    # Count tweets per label in training set\n",
    "    label_counts = Counter()\n",
    "    for labels in train['parsed_labels']:\n",
    "        for label in labels:\n",
    "            label_counts[label] += 1\n",
    "    \n",
    "    # Sort labels by count for visualization\n",
    "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels_list = [item[0] for item in sorted_labels]\n",
    "    counts_list = [item[1] for item in sorted_labels]\n",
    "    \n",
    "    # Identify labels to remove\n",
    "    labels_to_remove = [label for label, count in label_counts.items() if count < min_samples]\n",
    "    labels_to_keep = [label for label, count in label_counts.items() if count >= min_samples]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LABEL DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal unique labels: {len(label_counts)}\")\n",
    "    print(f\"Minimum samples threshold: {min_samples}\")\n",
    "    print(f\"Labels to keep: {len(labels_to_keep)}\")\n",
    "    print(f\"Labels to remove: {len(labels_to_remove)}\")\n",
    "    \n",
    "    if labels_to_remove:\n",
    "        print(f\"\\nLabels being removed (< {min_samples} tweets):\")\n",
    "        for label in sorted(labels_to_remove, key=lambda x: label_counts[x], reverse=True):\n",
    "            print(f\"  - {label}: {label_counts[label]} tweets\")\n",
    "    \n",
    "    # Visualization with matplotlib\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Create colors based on whether label will be kept or removed\n",
    "        colors = ['#2ecc71' if count >= min_samples else '#e74c3c' for count in counts_list]\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(labels_list))\n",
    "        bars = ax.barh(y_pos, counts_list, color=colors, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axvline(x=min_samples, color='#3498db', linestyle='--', linewidth=2, \n",
    "                   label=f'Threshold ({min_samples} tweets)')\n",
    "        \n",
    "        # Customize chart\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(labels_list)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Number of Tweets', fontsize=12)\n",
    "        ax.set_ylabel('Label', fontsize=12)\n",
    "        ax.set_title('Label Distribution in Training Data\\n(Green: Keep, Red: Remove)', fontsize=14)\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        # Add count annotations\n",
    "        for i, (count, bar) in enumerate(zip(counts_list, bars)):\n",
    "            ax.annotate(f'{count}', xy=(count + 20, bar.get_y() + bar.get_height()/2),\n",
    "                       va='center', ha='left', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"\\nâœ“ Plot displayed\")\n",
    "    \n",
    "    # Define the original label order (from the dataset)\n",
    "    all_labels_ordered = [\n",
    "        'arts_&_culture', 'business_&_entrepreneurs', 'celebrity_&_pop_culture',\n",
    "        'diaries_&_daily_life', 'family', 'fashion_&_style', 'film_tv_&_video',\n",
    "        'fitness_&_health', 'food_&_dining', 'gaming', 'learning_&_educational',\n",
    "        'music', 'news_&_social_concern', 'other_hobbies', 'relationships',\n",
    "        'science_&_technology', 'sports', 'travel_&_adventure', 'youth_&_student_life'\n",
    "    ]\n",
    "    \n",
    "    # Get indices of labels to keep\n",
    "    keep_indices = [i for i, label in enumerate(all_labels_ordered) if label in labels_to_keep]\n",
    "    new_labels_ordered = [all_labels_ordered[i] for i in keep_indices]\n",
    "    \n",
    "    def filter_and_update_labels(df, labels_to_remove_set, keep_indices, new_labels_ordered):\n",
    "        \"\"\"\n",
    "        Filter out rows where all labels are in labels_to_remove and update label vectors.\n",
    "        \"\"\"\n",
    "        # Check if tweet has at least one label that will be kept\n",
    "        def has_valid_label(parsed_labels):\n",
    "            return any(label not in labels_to_remove_set for label in parsed_labels)\n",
    "        \n",
    "        # Filter rows\n",
    "        mask = df['parsed_labels'].apply(has_valid_label)\n",
    "        df_filtered = df[mask].copy()\n",
    "        \n",
    "        # Update label_name to only include kept labels\n",
    "        def update_label_names(parsed_labels):\n",
    "            kept = [label for label in parsed_labels if label not in labels_to_remove_set]\n",
    "            return str(kept).replace(', ', ' ').replace(',', '')\n",
    "        \n",
    "        df_filtered['label_name'] = df_filtered['parsed_labels'].apply(update_label_names)\n",
    "        \n",
    "        # Update label vectors - parse old vector and create new one with only kept indices\n",
    "        def update_label_vector(label_str):\n",
    "            # Parse the vector string like '[0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]'\n",
    "            label_str = str(label_str).strip()\n",
    "            if label_str.startswith('[') and label_str.endswith(']'):\n",
    "                values = label_str[1:-1].split()\n",
    "                values = [int(v) for v in values]\n",
    "                # Keep only the values at keep_indices\n",
    "                new_values = [values[i] for i in keep_indices]\n",
    "                return '[' + ' '.join(map(str, new_values)) + ']'\n",
    "            return label_str\n",
    "        \n",
    "        df_filtered['label'] = df_filtered['label'].apply(update_label_vector)\n",
    "        \n",
    "        # Drop the temporary parsed_labels column\n",
    "        df_filtered = df_filtered.drop(columns=['parsed_labels'])\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    labels_to_remove_set = set(labels_to_remove)\n",
    "    \n",
    "    # Apply filtering to all datasets\n",
    "    train_filtered = filter_and_update_labels(train, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    val_filtered = filter_and_update_labels(val, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    test_filtered = filter_and_update_labels(test, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FILTERING RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"  Before: {len(train)} tweets\")\n",
    "    print(f\"  After:  {len(train_filtered)} tweets ({len(train) - len(train_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nValidation set:\")\n",
    "    print(f\"  Before: {len(val)} tweets\")\n",
    "    print(f\"  After:  {len(val_filtered)} tweets ({len(val) - len(val_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nTest set:\")\n",
    "    print(f\"  Before: {len(test)} tweets\")\n",
    "    print(f\"  After:  {len(test_filtered)} tweets ({len(test) - len(test_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Label vector size reduced from {len(all_labels_ordered)} to {len(new_labels_ordered)}\")\n",
    "    print(f\"\\nRemaining labels ({len(new_labels_ordered)}):\")\n",
    "    for label in new_labels_ordered:\n",
    "        print(f\"  - {label}\")\n",
    "    \n",
    "    return train_filtered, val_filtered, test_filtered, labels_to_remove\n",
    "\n",
    "\n",
    "# Apply the prepare_data_for_training function\n",
    "print(\"Loading preprocessed datasets...\\n\")\n",
    "\n",
    "# Load the preprocessed datasets\n",
    "train_df = pd.read_csv('../Data/tweets_preprocessed_train.csv')\n",
    "val_df = pd.read_csv('../Data/tweets_preprocessed_validation.csv')\n",
    "test_df = pd.read_csv('../Data/tweets_preprocessed_test.csv')\n",
    "\n",
    "print(f\"Loaded train: {len(train_df)} tweets\")\n",
    "print(f\"Loaded validation: {len(val_df)} tweets\")\n",
    "print(f\"Loaded test: {len(test_df)} tweets\\n\")\n",
    "\n",
    "# Prepare data for training (remove labels with < 180 tweets)\n",
    "train_filtered, val_filtered, test_filtered, removed_labels = prepare_data_for_training(\n",
    "    train_df, val_df, test_df, \n",
    "    min_samples=180, \n",
    "    show_plot=True\n",
    ")\n",
    "\n",
    "# Save the filtered datasets (overwrite the original preprocessed files)\n",
    "train_filtered.to_parquet('../Data/tweets_preprocessed_train.parquet', index=False)\n",
    "val_filtered.to_parquet('../Data/tweets_preprocessed_validation.parquet', index=False)\n",
    "test_filtered.to_parquet('../Data/tweets_preprocessed_test.parquet', index=False)\n",
    "\n",
    "train_filtered.to_csv('../Data/tweets_preprocessed_train.csv', index=False)\n",
    "val_filtered.to_csv('../Data/tweets_preprocessed_validation.csv', index=False)\n",
    "test_filtered.to_csv('../Data/tweets_preprocessed_test.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVED FILES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ“ Filtered datasets saved to:\")\n",
    "print(\"  - ../Data/tweets_preprocessed_train.parquet\")\n",
    "print(\"  - ../Data/tweets_preprocessed_validation.parquet\")\n",
    "print(\"  - ../Data/tweets_preprocessed_test.parquet\")\n",
    "print(\"  - ../Data/tweets_preprocessed_train.csv\")\n",
    "print(\"  - ../Data/tweets_preprocessed_validation.csv\")\n",
    "print(\"  - ../Data/tweets_preprocessed_test.csv\")\n",
    "print(\"\\nâœ“ Data preparation for training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
