{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Neural Network Classification with scikit-learn\n",
    "\n",
    "---\n",
    "## 1.  Notebook Overview\n",
    "\n",
    "### 1.1 Objective\n",
    "Build a Multi-Layer Perceptron (MLP) neural network for multi-label tweet topic classification using scikit-learn's `MLPClassifier`.\n",
    "\n",
    "### 1.2 Prerequisites\n",
    "This notebook assumes you have already executed:\n",
    "- **Lab 2**: Data preprocessing → `../Data/tweets_preprocessed_train.parquet`\n",
    "- **Lab 3** (Sunny): Language modeling\n",
    "- **Lab 4** (Hikmet): Feature extraction and Naive Bayes baseline\n",
    "\n",
    "### 1. 3 Architecture\n",
    "We implement a neural network with:\n",
    "- **Input layer**: TF-IDF vectorized features\n",
    "- **Hidden layers**: 128 → 64 → 128 neurons (as specified)\n",
    "- **Output layer**: 19 neurons (one per topic class)\n",
    "\n",
    "### 1.4 Data Splits\n",
    "- **Training**: `../Data/tweets_preprocessed_train.parquet` (from Lab 2)\n",
    "- **Validation**: `validation_2021` split from HuggingFace\n",
    "- **Test**: `test_2021` split from HuggingFace\n",
    "\n",
    "### 1.5 Section Roadmap\n",
    "1. `Section 2`: Load data from preprocessed parquet and HuggingFace\n",
    "2. `Section 3`: Feature extraction with TF-IDF\n",
    "3. `Section 4`: Build and train the MLP neural network\n",
    "4. `Section 5`: Evaluate on validation and test sets\n",
    "5.  `Section 6`: Architecture comparison and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading\n",
    "\n",
    "Load preprocessed training data from Lab 2 and fetch test/validation splits from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn pandas numpy datasets matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "TRAIN_DATA_PATH = \"../Data/tweets_preprocessed_train.parquet\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# The 19 topic labels in the dataset\n",
    "TOPIC_LABELS = [\n",
    "    'arts_&_culture', 'business_&_entrepreneurs', 'celebrity_&_pop_culture',\n",
    "    'diaries_&_daily_life', 'family', 'fashion_&_style', 'film_tv_&_video',\n",
    "    'fitness_&_health', 'food_&_dining', 'gaming', 'learning_&_educational',\n",
    "    'music', 'news_&_social_concern', 'other_hobbies', 'relationships',\n",
    "    'science_&_technology', 'sports', 'travel_&_adventure', 'youth_&_student_life'\n",
    "]\n",
    "NUM_CLASSES = len(TOPIC_LABELS)\n",
    "\n",
    "print(f\"Number of topic classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_vector(value) -> np.ndarray:\n",
    "    \"\"\"Parse the label column (binary vector) to numpy array.\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value. astype(np.float32)\n",
    "    if isinstance(value, list):\n",
    "        return np.array(value, dtype=np.float32)\n",
    "    if isinstance(value, str):\n",
    "        # Handle string representation like '[0 0 0 1 0 ...]'\n",
    "        cleaned = value.strip('[]'). split()\n",
    "        return np.array([float(x) for x in cleaned], dtype=np.float32)\n",
    "    return np.zeros(NUM_CLASSES, dtype=np. float32)\n",
    "\n",
    "\n",
    "# Load preprocessed training data from Lab 2\n",
    "print(\"Loading preprocessed training data from Lab 2...\")\n",
    "df_train = pd.read_parquet(TRAIN_DATA_PATH)\n",
    "print(f\"Training samples: {len(df_train):,}\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and validation splits from HuggingFace\n",
    "print(\"Loading test and validation data from HuggingFace...\")\n",
    "\n",
    "test_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"test_2021\")\n",
    "val_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"validation_2021\")\n",
    "\n",
    "df_test = test_dataset.to_pandas()\n",
    "df_val = val_dataset.to_pandas()\n",
    "\n",
    "print(f\"Test samples: {len(df_test):,}\")\n",
    "print(f\"Validation samples: {len(df_val):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels - use the 'label' column which contains binary vectors\n",
    "y_train = np.array([parse_label_vector(lbl) for lbl in df_train['label']])\n",
    "y_test = np.array([parse_label_vector(lbl) for lbl in df_test['label']])\n",
    "y_val = np.array([parse_label_vector(lbl) for lbl in df_val['label']])\n",
    "\n",
    "print(f\"\\nLabel matrix shapes:\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "\n",
    "# Verify label distribution\n",
    "print(f\"\\nTraining set label distribution:\")\n",
    "for i, label in enumerate(TOPIC_LABELS[:5]):  # Show first 5\n",
    "    count = int(y_train[:, i].sum())\n",
    "    print(f\"  {label}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  ...  (showing 5 of {NUM_CLASSES} classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Extraction\n",
    "\n",
    "Transform text into numerical features using TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    ")\n",
    "\n",
    "# Fit on training data only, transform all splits\n",
    "print(\"Vectorizing text data...\")\n",
    "X_train = tfidf_vectorizer. fit_transform(df_train['text']. astype(str))\n",
    "X_test = tfidf_vectorizer.transform(df_test['text'].astype(str))\n",
    "X_val = tfidf_vectorizer. transform(df_val['text'].astype(str))\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"  X_train: {X_train. shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"\\nVocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.  Neural Network Model\n",
    "\n",
    "### 4.1 Architecture\n",
    "As specified in the lab requirements:\n",
    "- **Input layer**: Determined by TF-IDF features\n",
    "- **Hidden layer 1**: 128 neurons\n",
    "- **Hidden layer 2**: 64 neurons\n",
    "- **Hidden layer 3**: 128 neurons\n",
    "- **Output layer**: 19 neurons (one per topic)\n",
    "\n",
    "### 4.2 Why no early_stopping?\n",
    "We have dedicated validation (`validation_2021`) and test (`test_2021`) splits from HuggingFace. There's no need for scikit-learn to internally split the training data for early stopping validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# Architecture as specified: 128 -> 64 -> 128\n",
    "HIDDEN_LAYERS = (128, 64, 128)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NEURAL NETWORK CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input layer: {X_train.shape[1]} features\")\n",
    "print(f\"Hidden layers: {HIDDEN_LAYERS}\")\n",
    "print(f\"Output layer: {NUM_CLASSES} classes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create MLP classifier\n",
    "# No early_stopping - we use HuggingFace's validation_2021 for model selection\n",
    "mlp_base = MLPClassifier(\n",
    "    hidden_layer_sizes=HIDDEN_LAYERS,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Wrap for multi-label classification\n",
    "mlp_classifier = MultiOutputClassifier(mlp_base, n_jobs=-1)\n",
    "\n",
    "print(\"\\n✓ Model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "training_time = time. time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Check convergence\n",
    "converged = sum(1 for est in mlp_classifier.estimators_ if est.n_iter_ < est.max_iter)\n",
    "print(f\"✓ {converged}/{NUM_CLASSES} classifiers converged before max_iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation\n",
    "\n",
    "Evaluate on both validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    hamming_loss, classification_report\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_pred_val = mlp_classifier.predict(X_val)\n",
    "y_pred_test = mlp_classifier.predict(X_test)\n",
    "\n",
    "def evaluate(y_true, y_pred, split_name):\n",
    "    \"\"\"Print evaluation metrics for a given split.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{split_name. upper()} SET METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Subset Accuracy':<20}: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"{'Hamming Loss':<20}: {hamming_loss(y_true, y_pred):.4f}\")\n",
    "    print(f\"{'Micro F1':<20}: {f1_score(y_true, y_pred, average='micro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Macro F1':<20}: {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Weighted F1':<20}: {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"{'Micro Precision':<20}: {precision_score(y_true, y_pred, average='micro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Micro Recall':<20}: {recall_score(y_true, y_pred, average='micro', zero_division=0):. 4f}\")\n",
    "\n",
    "# Evaluate on validation set (for model selection)\n",
    "evaluate(y_val, y_pred_val, \"Validation\")\n",
    "\n",
    "# Evaluate on test set (final performance)\n",
    "evaluate(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS PERFORMANCE (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_pred_test,\n",
    "    target_names=TOPIC_LABELS,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "per_class_f1 = f1_score(y_test, y_pred_test, average=None, zero_division=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(TOPIC_LABELS, per_class_f1, color='steelblue')\n",
    "ax.set_xlabel('F1 Score')\n",
    "ax.set_title('Per-Class F1 Scores (Test Set)')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for bar, score in zip(bars, per_class_f1):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar. get_height()/2,\n",
    "            f'{score:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt. show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Architecture Comparison\n",
    "\n",
    "Compare different hidden layer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    (64,),               # Single layer\n",
    "    (128, 64),           # Two layers\n",
    "    (128, 64, 128),      # Three layers (baseline)\n",
    "    (256, 128, 64),      # Wider first layer\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining: {arch}\")\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    clf = MultiOutputClassifier(mlp, n_jobs=-1)\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time. time() - start\n",
    "    \n",
    "    # Evaluate on VALIDATION set (for model selection)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    f1_micro = f1_score(y_val, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'Architecture': str(arch),\n",
    "        'Micro F1': f1_micro,\n",
    "        'Macro F1': f1_macro,\n",
    "        'Train Time (s)': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Val Micro F1: {f1_micro:.4f}, Macro F1: {f1_macro:.4f}, Time: {train_time:.1f}s\")\n",
    "\n",
    "results_df = pd. DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY (Validation Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = range(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0]. bar([i - width/2 for i in x], results_df['Micro F1'], width, label='Micro F1', color='steelblue')\n",
    "axes[0].bar([i + width/2 for i in x], results_df['Macro F1'], width, label='Macro F1', color='coral')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Architecture'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_title('F1 Scores by Architecture (Validation Set)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].bar(x, results_df['Train Time (s)'], color='seagreen')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1]. set_xticklabels(results_df['Architecture'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Training Time (seconds)')\n",
    "axes[1].set_title('Training Time by Architecture')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_to_labels(binary_vec):\n",
    "    \"\"\"Convert binary vector to label names.\"\"\"\n",
    "    return [TOPIC_LABELS[i] for i, val in enumerate(binary_vec) if val == 1] or ['[none]']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sample_idx = np.random. choice(len(df_test), size=10, replace=False)\n",
    "\n",
    "for idx in sample_idx:\n",
    "    text = df_test['text'].iloc[idx]\n",
    "    true = binary_to_labels(y_test[idx])\n",
    "    pred = binary_to_labels(y_pred_test[idx])\n",
    "    \n",
    "    match = \"✓\" if set(true) == set(pred) else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{match} Sample {idx}:\")\n",
    "    print(f\"   Text: {str(text)[:80]}...\")\n",
    "    print(f\"   True: {true}\")\n",
    "    print(f\"   Pred: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8.  Conclusion\n",
    "\n",
    "### 8.1 Summary\n",
    "We built an MLP neural network with the specified 128-64-128 architecture for multi-label tweet classification.\n",
    "\n",
    "### 8.2 Key Design Decisions\n",
    "- **No early_stopping**: We use HuggingFace's `validation_2021` split for model selection and `test_2021` for final evaluation\n",
    "- **MultiOutputClassifier wrapper**: Handles multi-label by training 19 independent binary classifiers\n",
    "- **TF-IDF features**: Captures term importance relative to the corpus\n",
    "\n",
    "### 8.3 Limitations of scikit-learn's MLP\n",
    "- No GPU acceleration\n",
    "- No dropout or batch normalization\n",
    "- Limited to fully-connected layers\n",
    "\n",
    "### 8.4 Next Steps\n",
    "- Tune regularization (alpha parameter)\n",
    "- Try different activation functions\n",
    "- Consider deep learning frameworks (PyTorch/TensorFlow) for more flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "test_f1_micro = f1_score(y_test, y_pred_test, average='micro', zero_division=0)\n",
    "test_f1_macro = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Architecture: Input({X_train. shape[1]}) → 128 → 64 → 128 → Output({NUM_CLASSES})\")\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Validation samples: {len(y_val):,}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "print(f\"Test Micro F1: {test_f1_micro:.4f}\")\n",
    "print(f\"Test Macro F1: {test_f1_macro:. 4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}