{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6b8ff8",
   "metadata": {},
   "source": [
    "# Text Classification Lab (Hikmet)\n",
    "\n",
    "---\n",
    "## 1. Notebook Orientation\n",
    "\n",
    "### 1.1 Focus of this notebook\n",
    "We revisit the preprocessed tweets from Lab 3 and limit ourselves to the token analysis stage:\n",
    "\n",
    "1. Load the dataset and normalise the label lists.\n",
    "2. Derive the 1000 most frequent tokens, with optional per-class previews.\n",
    "\n",
    "Later tasks (training a Naive Bayes classifier, evaluating it) remain intentionally open and appear only as placeholders.\n",
    "\n",
    "### 1.2 Dataset\n",
    "- Source: `../Data/df_preprocessed.parquet`\n",
    "- Columns: `text` (whitespace-tokenised strings) and `label_name` (list of categories)\n",
    "\n",
    "### 1.3 Section overview\n",
    "1. **Section 2** – Load/prepare the data frame.\n",
    "2. **Section 3** – Reuse the Lab 3 helper classes (`UnigramLM`).\n",
    "3. **Section 4** – Compute the top 1000 tokens globally and preview them per class.\n",
    "4. **Sections 5 & 6** – Placeholders for future classification steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a969f",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "### 2.1 Goal\n",
    "Load the preprocessed tweets, standardise the label column, and create a single-label view that can act as training data later on.\n",
    "\n",
    "### 2.2 Steps\n",
    "1. Import libraries (Pandas, NumPy, collections helper).\n",
    "2. Convert `label_name` into consistent Python lists.\n",
    "3. Build a DataFrame with a `label` column for single-label examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27917adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/df_preprocessed.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mprimary_label\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m items: items[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m items \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m df_raw = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_raw)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_raw.head(\u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> pd.DataFrame:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load tweets from parquet and normalise the label column.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_labels\u001b[39m(value) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../Data/df_preprocessed.parquet'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"../Data/df_preprocessed.parquet\"\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalise the label column.\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    def parse_labels(value) -> List[str]:\n",
    "        if isinstance(value, list):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, tuple):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(v) for v in parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                return [value]\n",
    "        return [str(value)]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_count\"] = df[\"labels\"].apply(len)\n",
    "    df[\"primary_label\"] = df[\"labels\"].apply(lambda items: items[0] if items else \"unknown\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_raw = load_dataset(DATA_PATH)\n",
    "print(f\"Loaded {len(df_raw):,} documents from {DATA_PATH}.\")\n",
    "print(df_raw.head(3))\n",
    "\n",
    "single_label_df = df_raw[df_raw[\"label_count\"] == 1][[\"text\", \"primary_label\"]].rename(\n",
    "    columns={\"primary_label\": \"label\"}\n",
    ")\n",
    "print(f\"Single-label subset: {len(single_label_df):,} rows (label column = 'label').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118d7e",
   "metadata": {},
   "source": [
    "## 3. Reusing Language-Model Helpers (Lab 3)\n",
    "\n",
    "### 3.1 Background\n",
    "`lab3_sunny.ipynb` defined a `UnigramLM` class that counts token frequencies and computes Laplace-smoothed log probabilities. We reuse the same implementation here to keep the logic consistent across notebooks.\n",
    "\n",
    "### 3.2 How it works\n",
    "- `ensure_tokens` converts strings to token lists.\n",
    "- `UnigramLM` aggregates token counts (`self.unigram_counts`) across the corpus.\n",
    "- Calling `.unigram_counts.most_common(n)` returns the top-n tokens along with their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55c9d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Sequence, Union\n",
    "import math\n",
    "\n",
    "\n",
    "def ensure_tokens(sentence: Union[Sequence[str], str]) -> List[str]:\n",
    "    \"\"\"Convert whitespace-separated text or token sequences into a list.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "class UnigramLM:\n",
    "    \"\"\"Laplace-smoothed unigram language model operating in log-space.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: Sequence[Sequence[str]]):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            tokens = ensure_tokens(sentence)\n",
    "            self.unigram_counts.update(tokens)\n",
    "            self.total_tokens += len(tokens)\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        if self.total_tokens == 0:\n",
    "            raise ValueError(\"Cannot train a UnigramLM on an empty corpus.\")\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def log_prob(self, word: str) -> float:\n",
    "        count = self.unigram_counts.get(word, 0)\n",
    "        return math.log((count + 1) / (self.total_tokens + self.vocab_size))\n",
    "\n",
    "    def sentence_log_prob(self, sentence: Union[Sequence[str], str]) -> float:\n",
    "        tokens = ensure_tokens(sentence)\n",
    "        if not tokens:\n",
    "            return float('-inf')\n",
    "        return sum(self.log_prob(token) for token in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072f95e",
   "metadata": {},
   "source": [
    "## 4. Task – Top 1000 Tokens\n",
    "\n",
    "### 4.1 Goal\n",
    "Identify the most frequent tokens in the corpus (with optional class-wise previews) and store them for later feature engineering.\n",
    "\n",
    "### 4.2 Approach\n",
    "1. Train the `UnigramLM` on the single-label subset.\n",
    "2. Retrieve `most_common(1000)` and inspect the first items.\n",
    "3. Optionally repeat the process for the most frequent classes to understand their characteristic vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa338605",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_label_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m MAX_FEATURES = \u001b[32m1000\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Gesamtvokabular\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m corpus_tokens = [ensure_tokens(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43msingle_label_df\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m      5\u001b[39m unigram_model = UnigramLM(corpus_tokens)\n\u001b[32m      7\u001b[39m top_unigrams = unigram_model.unigram_counts.most_common(MAX_FEATURES)\n",
      "\u001b[31mNameError\u001b[39m: name 'single_label_df' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 1000\n",
    "\n",
    "# Gesamtvokabular\n",
    "corpus_tokens = [ensure_tokens(text) for text in single_label_df[\"text\"]]\n",
    "unigram_model = UnigramLM(corpus_tokens)\n",
    "\n",
    "top_unigrams = unigram_model.unigram_counts.most_common(MAX_FEATURES)\n",
    "print(f\"Collected top {len(top_unigrams)} tokens (showing the first 20):\")\n",
    "for token, freq in top_unigrams[:20]:\n",
    "    print(f\"  {token:<15} -> {freq}\")\n",
    "\n",
    "# Optional: per class preview for the three most frequent labels\n",
    "label_counts = single_label_df[\"label\"].value_counts().head(3)\n",
    "print(\"\\nPer-class token preview (Top 10 tokens for the most frequent labels):\")\n",
    "for label, count in label_counts.items():\n",
    "    label_corpus = [ensure_tokens(text) for text in single_label_df.loc[single_label_df[\"label\"] == label, \"text\"]]\n",
    "    label_model = UnigramLM(label_corpus)\n",
    "    label_top = label_model.unigram_counts.most_common(10)\n",
    "    formatted = \", \".join([f\"{tok} ({freq})\" for tok, freq in label_top])\n",
    "    print(f\"- {label} ({count} docs): {formatted}\")\n",
    "\n",
    "# Speichern des Vokabulars für spätere Schritte (falls benötigt)\n",
    "TOP_VOCABULARY = [token for token, _ in top_unigrams]\n",
    "print(f\"\\nStored vocabulary length: {len(TOP_VOCABULARY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207133",
   "metadata": {},
   "source": [
    "## 5. Task – Naive Bayes Setup (Placeholder)\n",
    "\n",
    "> To be added later: build the pipeline, split the data, and train the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b82aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TOP_VOCABULARY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mTOP_VOCABULARY\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TOP_VOCABULARY' is not defined"
     ]
    }
   ],
   "source": [
    "print(TOP_VOCABULARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c367c",
   "metadata": {},
   "source": [
    "Schritt 1: Labels vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df0ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090 Trainingsbeispiele\n",
      "573 Validierungsbeispiele\n",
      "1679 Testbeispiele\n"
     ]
    }
   ],
   "source": [
    "# === 1️⃣ Dataset laden\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"cardiffnlp/tweet_topic_multi\")\n",
    "\n",
    "train_ds = ds[\"train_all\"]\n",
    "val_ds   = ds[\"validation_2020\"]\n",
    "test_ds  = ds[\"test_2021\"]\n",
    "\n",
    "X_train = train_ds[\"text\"]\n",
    "y_train = train_ds[\"label_name\"]\n",
    "\n",
    "X_val = val_ds[\"text\"]\n",
    "y_val = val_ds[\"label_name\"]\n",
    "\n",
    "X_test = test_ds[\"text\"]\n",
    "y_test = test_ds[\"label_name\"]\n",
    "\n",
    "print(len(X_train), \"Trainingsbeispiele\")\n",
    "print(len(X_val), \"Validierungsbeispiele\")\n",
    "print(len(X_test), \"Testbeispiele\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5533d4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TOP_VOCABULARY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[32m      3\u001b[39m vectorizer = CountVectorizer(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     vocabulary=\u001b[43mTOP_VOCABULARY\u001b[49m,  \u001b[38;5;66;03m# hier wird dein gespeichertes Vokabular verwendet\u001b[39;00m\n\u001b[32m      5\u001b[39m     lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m     token_pattern=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(?u)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Texte in Zahlen umwandeln (Bag-of-Words)\u001b[39;00m\n\u001b[32m     10\u001b[39m X_train_bow = vectorizer.transform(X_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'TOP_VOCABULARY' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=TOP_VOCABULARY,  # hier wird dein gespeichertes Vokabular verwendet\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "\n",
    "# Texte in Zahlen umwandeln (Bag-of-Words)\n",
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_val_bow   = vectorizer.transform(X_val)\n",
    "X_test_bow  = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Train-Matrix:\", X_train_bow.shape)\n",
    "print(\"Val-Matrix:\", X_val_bow.shape)\n",
    "print(\"Test-Matrix:\", X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c87f5",
   "metadata": {},
   "source": [
    "Multi-Label-Encoding für die Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Klassen: 405\n",
      "Beispiel Label: ['sports']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['arts_&_culturebusiness_&_entrepreneurs', 'arts_&_culturediaries_&_daily_lifefashion_&_style', 'arts_&_culturefamilyfashion_&_style', 'arts_&_culturefilm_tv_&_videonews_&_social_concernsports', 'arts_&_culturefilm_tv_&_videoother_hobbies', 'arts_&_culturefilm_tv_&_videorelationshipstravel_&_adventure', 'arts_&_culturefilm_tv_&_videotravel_&_adventure', 'arts_&_culturefitness_&_health', 'arts_&_cultureother_hobbiesrelationships', 'arts_&_culturesports', 'business_&_entrepreneurscelebrity_&_pop_culturemusic', 'business_&_entrepreneursdiaries_&_daily_lifefitness_&_healthnews_&_social_concern', 'business_&_entrepreneursdiaries_&_daily_lifefood_&_dining', 'business_&_entrepreneursdiaries_&_daily_lifenews_&_social_concernscience_&_technology', 'business_&_entrepreneursgamingsports', 'celebrity_&_pop_culturediaries_&_daily_lifefilm_tv_&_videomusicrelationships', 'celebrity_&_pop_culturediaries_&_daily_lifefood_&_dining', 'celebrity_&_pop_culturefilm_tv_&_videoother_hobbies', 'celebrity_&_pop_culturefood_&_diningother_hobbies', 'celebrity_&_pop_culturegamingnews_&_social_concernsports', 'celebrity_&_pop_culturemusicnews_&_social_concernrelationships', 'celebrity_&_pop_culturemusicnews_&_social_concernsports', 'celebrity_&_pop_cultureother_hobbiessports', 'diaries_&_daily_lifefamilyfilm_tv_&_videorelationships', 'diaries_&_daily_lifefamilymusicother_hobbies', 'diaries_&_daily_lifefamilynews_&_social_concernrelationships', 'diaries_&_daily_lifefashion_&_styleother_hobbies', 'diaries_&_daily_lifefashion_&_stylerelationships', 'diaries_&_daily_lifefitness_&_healthmusicsports', 'diaries_&_daily_lifefitness_&_healthnews_&_social_concernscience_&_technology', 'diaries_&_daily_lifefood_&_diningmusic', 'diaries_&_daily_lifegamingsports', 'diaries_&_daily_lifeother_hobbiesyouth_&_student_life', 'familyfilm_tv_&_videogamingrelationships', 'familyfilm_tv_&_videomusic', 'fashion_&_styleother_hobbiessports', 'film_tv_&_videofitness_&_healthscience_&_technology', 'film_tv_&_videolearning_&_educationalnews_&_social_concernscience_&_technology', 'film_tv_&_videotravel_&_adventure', 'fitness_&_healthlearning_&_educationalyouth_&_student_life', 'fitness_&_healthnews_&_social_concerntravel_&_adventure', 'gamingrelationships', 'learning_&_educationalmusic', 'other_hobbiesscience_&_technologytravel_&_adventure'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train)\n",
    "y_val_bin   = mlb.transform(y_val)\n",
    "y_test_bin  = mlb.transform(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530085c",
   "metadata": {},
   "source": [
    "Naive-Bayes-Klassifikator trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Label Naive-Bayes-Modell trainiert!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "nb_clf = OneVsRestClassifier(MultinomialNB(alpha=1.0))\n",
    "nb_clf.fit(X_train_bow, y_train_bin)\n",
    "\n",
    "print(\"✅ Naive Bayes trainiert!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d59f9",
   "metadata": {},
   "source": [
    "Erste Test-Vorhersagen prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b653f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: fresh find discover new music hot talent listen popstar dnb remix dooz...\n",
      "→ Vorhergesagte Labels: ('celebrity_&_pop_culturemusic', 'music')\n",
      "\n",
      "Text: stop love thing game tell bill room growth go to look forward see gobi...\n",
      "→ Vorhergesagte Labels: ()\n",
      "\n",
      "Text: putin say nord stream gas pipeline europe complete end year quarter wa...\n",
      "→ Vorhergesagte Labels: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "for text, pred in zip(X_test[:3], y_pred_labels[:3]):\n",
    "    print(f\"Text: {text[:70]}...\")\n",
    "    print(f\"→ Vorhergesagte Labels: {pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169817fd",
   "metadata": {},
   "source": [
    "## 6. Task – Evaluation & Error Analysis (Placeholder)\n",
    "\n",
    "> Once a classifier is trained, we will add metrics and example analyses here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf13550",
   "metadata": {},
   "source": [
    "Vorhersagen für dein Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23fe5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel-Vorhersagen:\n",
      "Text: fresh find discover new music hot talent listen popstar dnb remix doozy hiphop r...\n",
      "  Wahr: ('music',)\n",
      "  Vorhergesagt: ('celebrity_&_pop_culturemusic', 'music')\n",
      "\n",
      "Text: stop love thing game tell bill room growth go to look forward see gobill...\n",
      "  Wahr: ('gamingsports',)\n",
      "  Vorhergesagt: ()\n",
      "\n",
      "Text: putin say nord stream gas pipeline europe complete end year quarter wall street...\n",
      "  Wahr: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "  Vorhergesagt: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss, classification_report\n",
    "\n",
    "# Vorhersage (0/1-Matrix)\n",
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "\n",
    "# Labels wieder in Textform zurückwandeln\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "y_true_labels = mlb.inverse_transform(y_test_bin)\n",
    "\n",
    "print(\"Beispiel-Vorhersagen:\")\n",
    "for text, true, pred in zip(X_test[:3], y_true_labels[:3], y_pred_labels[:3]):\n",
    "    print(f\"Text: {text[:80]}...\")\n",
    "    print(f\"  Wahr: {true}\")\n",
    "    print(f\"  Vorhergesagt: {pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
