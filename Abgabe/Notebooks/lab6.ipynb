{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357dbe25",
   "metadata": {},
   "source": [
    "# Lab 6: Representations\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Notebook Overview\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Objective\n",
    "\n",
    "Understand how pretrained word embeddings can be used both for semantic similarity inspection and as inputs to the tweet topic classifier built in previous labs.\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Prerequisites\n",
    "\n",
    "This notebook builds on:\n",
    "\n",
    "- **Lab 2**: Data preprocessing → `../Data/multi_label/tweets_preprocessed_train.parquet`\n",
    "- **Lab 3**: Language modeling\n",
    "- **Lab 4**: Feature extraction → `../Data/top_1000_vocabulary.json`\n",
    "- **Lab 5**: Neural network classification pipeline that we will adapt to embedding inputs\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Resources\n",
    "\n",
    "- **Gensim documentation** for word2vec models: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "- **Pretrained model**: `word2vec-google-news-300` (download instructions in the docs)\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 Task Summary\n",
    "\n",
    "- Install gensim and review the documentation for its word2vec embeddings.\n",
    "- Load the pretrained `word2vec-google-news-300` embedding, experiment with similarities using `model.wv.similarity()` and `model.wv.most_similar()`, and discuss the observed relations.\n",
    "- Re-use the classifier from Lab 5, replace text features with word2vec embeddings (convert tweets to vectors, e.g., mean of word embeddings or a matrix input), and reason about the dimensionality changes.\n",
    "- Compare the resulting performance with previous labs, inspect selected examples manually, and summarize insights.\n",
    "\n",
    "\n",
    "\n",
    "### 1.5 Section Roadmap\n",
    "\n",
    "1. `Section 2`: Environment setup and gensim installation\n",
    "2. `Section 3`: Load pretrained embeddings and explore similarity queries\n",
    "3. `Section 4`: Convert tweets to word2vec representations and integrate with the MLP pipeline\n",
    "4. `Section 5`: Evaluate, compare with baseline results, and manually analyze sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca4ce7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup and Data Access\n",
    "\n",
    "### 2.1 Libraries\n",
    "- `gensim` (with `gensim.downloader`) for retrieving the pretrained `word2vec-google-news-300` vectors.\n",
    "- `scikit-learn`, `pandas`, and `numpy` from previous labs.\n",
    "\n",
    "### 2.2 Installation Steps\n",
    "1. Install the required Python packages (`gensim`, `scikit-learn`, etc.) into the current environment.\n",
    "2. Load `word2vec-google-news-300` via `gensim.downloader.load(...)` so the model is cached automatically under `~/gensim-data`.\n",
    "3. Verify that the preprocessed tweet file `../Data/multi_label/tweets_preprocessed_train.parquet` and the vocabulary file `../Data/top_1000_vocabulary.json` from earlier labs are present.\n",
    "4. Ensure HuggingFace API access (`cardiffnlp/tweet_topic_multi`) is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50232389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet gensim numpy pandas tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ea0ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pfade konfiguriert (konsistent mit Lab 5)\n",
      "  Train: ../Data/multi_label/tweets_preprocessed_train.parquet\n",
      "  Test: ../Data/multi_label/tweets_preprocessed_test.parquet\n",
      "  Validation: ../Data/multi_label/tweets_preprocessed_validation.parquet\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "import time\n",
    "import tqdm\n",
    "import ast\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier  # Wie in Lab 5\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    hamming_loss\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants - paths to Lab 2 outputs (konsistent mit Lab 5)\n",
    "TRAIN_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_train.parquet\"\n",
    "TEST_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_test.parquet\"\n",
    "VALIDATION_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_validation.parquet\"\n",
    "VOCABULARY_PATH = \"../Data/top_1000_vocabulary.json\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"✓ Pfade konfiguriert (konsistent mit Lab 5)\")\n",
    "print(f\"  Train: {TRAIN_DATA_PATH}\")\n",
    "print(f\"  Test: {TEST_DATA_PATH}\")\n",
    "print(f\"  Validation: {VALIDATION_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01892f",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Load Pretrained Embeddings and Explore Similarities\n",
    "\n",
    "### 3.1 Load `word2vec-google-news-300`\n",
    "- Fetch the pretrained model directly via `gensim.downloader.load`. Gensim caches it under `~/gensim-data`. \n",
    "\n",
    "### 3.2 Inspect Vocabulary Coverage\n",
    "- Report vocabulary size and the 300‑dimensional vector width to confirm the load.\n",
    "- Track which probe words from the Twitter domain are present. \n",
    "\n",
    "### 3.3 Similarity Probing\n",
    "- Reuse helper functions (`model.wv.similarity`, `model.wv.most_similar`) to explore semantic neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c2d2dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3,000,000\n",
      "Vector dimension: 300\n",
      "Words found: ['twitter', 'sport', 'game', 'new', 'love']\n",
      "\n",
      "Nearest neighbors for 'twitter':\n",
      "             Twitter  0.891\n",
      "         Twitter.com  0.754\n",
      "               tweet  0.743\n",
      "            tweeting  0.716\n",
      "             tweeted  0.714\n",
      "            facebook  0.699\n",
      "              tweets  0.697\n",
      "             Tweeted  0.695\n",
      "               Tweet  0.688\n",
      "            Tweeting  0.685\n",
      "\n",
      "Nearest neighbors for 'sport':\n",
      "              sports  0.691\n",
      "     Snooki_wannabes  0.592\n",
      "  painkillers_throat_lozenges  0.564\n",
      "              racing  0.562\n",
      "            sporting  0.560\n",
      "           athletics  0.552\n",
      "   alpine_ski_racing  0.551\n",
      "       Pole_vaulting  0.546\n",
      "          motorsport  0.538\n",
      "              boxing  0.533\n",
      "\n",
      "Nearest neighbors for 'game':\n",
      "               games  0.764\n",
      "                play  0.650\n",
      "               match  0.649\n",
      "             matchup  0.612\n",
      "               agame  0.586\n",
      "            ballgame  0.573\n",
      "             thegame  0.572\n",
      "              opener  0.568\n",
      "             matches  0.558\n",
      "          tournament  0.550\n",
      "\n",
      "Nearest neighbors for 'new':\n",
      "            revamped  0.575\n",
      "              thenew  0.571\n",
      "              newest  0.567\n",
      "               newly  0.488\n",
      "             revamps  0.477\n",
      "               newer  0.474\n",
      "      brand_spanking  0.474\n",
      "            existing  0.472\n",
      "        long_awaited  0.466\n",
      "      groundbreaking  0.454\n",
      "\n",
      "Nearest neighbors for 'love':\n",
      "               loved  0.691\n",
      "               adore  0.682\n",
      "               loves  0.662\n",
      "             passion  0.610\n",
      "                hate  0.600\n",
      "              loving  0.589\n",
      "               Ilove  0.570\n",
      "           affection  0.566\n",
      "        undying_love  0.555\n",
      "    absolutely_adore  0.554\n",
      "similarity('twitter', 'facebook') = 0.699\n",
      "similarity('sport', 'game') = 0.322\n",
      "similarity('love', 'hate') = 0.600\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained embedding\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "print(f\"Vocabulary size: {len(glove_vectors.key_to_index):,}\")\n",
    "print(f\"Vector dimension: {glove_vectors.vector_size}\")\n",
    "\n",
    "# Experiment with word similarities\n",
    "probe_words = [\"twitter\", \"sport\", \"game\", \"new\", \"love\"]\n",
    "present = [word for word in probe_words if word in glove_vectors]\n",
    "\n",
    "print(f\"Words found: {present}\")\n",
    "\n",
    "\n",
    "def inspect_similarity(token: str, topn: int = 10) -> None:\n",
    "    \"\"\"Print nearest neighbors for a token if it exists in the vocabulary.\"\"\"\n",
    "    if token not in glove_vectors:\n",
    "        print(f\"'{token}' not in vocabulary.\")\n",
    "        return\n",
    "    print(f\"\\nNearest neighbors for '{token}':\")\n",
    "    for candidate, score in glove_vectors.most_similar(token, topn=topn):\n",
    "        print(f\"  {candidate:>18}  {score:.3f}\")\n",
    "\n",
    "\n",
    "for token in present:\n",
    "    inspect_similarity(token)\n",
    "    \n",
    "\n",
    "pair_queries = [\n",
    "    (\"twitter\", \"facebook\"),\n",
    "    (\"sport\", \"game\"),\n",
    "    (\"love\", \"hate\"),\n",
    "]\n",
    "\n",
    "for a, b in pair_queries:\n",
    "    if a in glove_vectors and b in glove_vectors:\n",
    "        score = glove_vectors.similarity(a, b)\n",
    "        print(f\"similarity({a!r}, {b!r}) = {score:.3f}\")\n",
    "    else:\n",
    "        missing = [w for w in (a, b) if w not in glove_vectors]\n",
    "        print(f\"Cannot compute similarity for {a!r}, {b!r}; missing {missing}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6647a69",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Convert Tweets to Word2Vec Representations and Integrate with the MLP Pipeline\n",
    "\n",
    "### 4.1 Data Loading\n",
    "- Reuse the preprocessed tweet dataset (`../Data/multi_label/tweets_preprocessed_train.parquet`) and topic labels from previous labs.\n",
    "- Split into train/validation/test with the same seeds and proportions as Lab 5 so results remain comparable.\n",
    "\n",
    "### 4.2 Tweet Vectorization\n",
    "- Implement a helper that converts each tokenized tweet into a 300‑dimensional vector by averaging all available word2vec embeddings (fallback to zero vector when no token is in the vocabulary).\n",
    "- Optionally track coverage statistics (share of tokens found vs. missing).\n",
    "\n",
    "### 4.3 Pipeline Integration\n",
    "- Replace the Lab 5 feature stage with the new embedding transformer (`FunctionTransformer` or precomputed matrix).\n",
    "- Adjust the MLP input dimension to 300 and keep other hyperparameters unless experimentation is required.\n",
    "- Train the MLP with the embedding features and capture training logs.\n",
    "- Compute validation/test metrics (accuracy, macro/micro F1, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3961ed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LADE VORVERARBEITETE DATEN (wie Lab 5)\n",
      "============================================================\n",
      "\n",
      "✓ Training: 5,465 samples\n",
      "✓ Test: 1,511 samples\n",
      "✓ Validation: 178 samples\n",
      "\n",
      "Columns: ['text', 'label_name', 'label', 'labels', 'label_binary']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "label_binary",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "341c13f3-b47c-4ece-ab80-7142121ef27c",
       "rows": [
        [
         "0",
         "lumber beat rapid game western division final evan edwards hit hr wp josh roberson ip bb mw lplayoffs mw lscoreboard",
         "['sports']",
         "[0 0 0 0 0 1]",
         "['sports']",
         "[0 0 0 0 0 1]"
        ],
        [
         "1",
         "hear eli gold announce auburn game dumbass",
         "['sports']",
         "[0 0 0 0 0 1]",
         "['sports']",
         "[0 0 0 0 0 1]"
        ],
        [
         "2",
         "phone away try look home game ticket october",
         "['sports']",
         "[0 0 0 0 0 1]",
         "['sports']",
         "[0 0 0 0 0 1]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lumber beat rapid game western division final ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>[0 0 0 0 0 1]</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hear eli gold announce auburn game dumbass</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>[0 0 0 0 0 1]</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phone away try look home game ticket october</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>[0 0 0 0 0 1]</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_name  \\\n",
       "0  lumber beat rapid game western division final ...  ['sports']   \n",
       "1         hear eli gold announce auburn game dumbass  ['sports']   \n",
       "2       phone away try look home game ticket october  ['sports']   \n",
       "\n",
       "           label    labels        label_binary  \n",
       "0  [0 0 0 0 0 1]  [sports]  [0, 0, 0, 0, 0, 1]  \n",
       "1  [0 0 0 0 0 1]  [sports]  [0, 0, 0, 0, 0, 1]  \n",
       "2  [0 0 0 0 0 1]  [sports]  [0, 0, 0, 0, 0, 1]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATEN LADEN (KONSISTENT MIT LAB 5)\n",
    "# ============================================================\n",
    "# Lädt alle Daten aus den vorverarbeiteten Parquet-Dateien\n",
    "\n",
    "def parse_labels(value) -> List[str]:\n",
    "    \"\"\"Parse label_name column into consistent Python lists (wie in Lab 5).\"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, tuple):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.startswith('[') and value.endswith(']'):\n",
    "            inner = value[1:-1].strip()\n",
    "            if not inner:\n",
    "                return []\n",
    "            inner = inner.replace(\"'\", \"\").replace('\"', '')\n",
    "            labels = [l.strip() for l in inner.split() if l.strip()]\n",
    "            return labels\n",
    "        try:\n",
    "            parsed = ast.literal_eval(value)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(v) for v in parsed]\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "        return [value] if value else []\n",
    "    return [str(value)] if value else []\n",
    "\n",
    "def parse_binary_label(value) -> np.ndarray:\n",
    "    \"\"\"Parse binary label array from string representation (wie in Lab 5).\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        inner = value.strip()[1:-1]\n",
    "        return np.array([int(x) for x in inner.split()])\n",
    "    return np.array(value)\n",
    "\n",
    "def load_preprocessed_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalize the label columns (wie in Lab 5).\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_binary\"] = df[\"label\"].apply(parse_binary_label)\n",
    "    return df\n",
    "\n",
    "# Load all datasets from preprocessed parquet files\n",
    "print(\"=\"*60)\n",
    "print(\"LADE VORVERARBEITETE DATEN (wie Lab 5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_train = load_preprocessed_dataset(TRAIN_DATA_PATH)\n",
    "df_test = load_preprocessed_dataset(TEST_DATA_PATH)\n",
    "df_val = load_preprocessed_dataset(VALIDATION_DATA_PATH)\n",
    "\n",
    "print(f\"\\n✓ Training: {len(df_train):,} samples\")\n",
    "print(f\"✓ Test: {len(df_test):,} samples\")\n",
    "print(f\"✓ Validation: {len(df_val):,} samples\")\n",
    "print(f\"\\nColumns: {df_train.columns.tolist()}\")\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cefeb263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Anzahl Klassen (aus label_binary): 6\n",
      "✓ Klassennamen extrahiert: 6\n",
      "✓ Klassen: ['celebrity_&_pop_culture', 'diaries_&_daily_life', 'film_tv_&_video', 'music', 'news_&_social_concern', 'sports']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# KLASSEN-ERKENNUNG (wie Lab 5)\n",
    "# ============================================================\n",
    "\n",
    "# Bestimme die Anzahl der Klassen aus den binären Label-Vektoren\n",
    "num_classes = len(df_train['label_binary'].iloc[0])\n",
    "print(f\"\\n✓ Anzahl Klassen (aus label_binary): {num_classes}\")\n",
    "\n",
    "# Extrahiere alle einzigartigen Klassennamen aus label_name\n",
    "all_class_names = set()\n",
    "for df in [df_train, df_test, df_val]:\n",
    "    for labels in df['labels']:\n",
    "        all_class_names.update(labels)\n",
    "\n",
    "TOPIC_CLASSES = sorted(list(all_class_names))\n",
    "NUM_CLASSES = num_classes\n",
    "\n",
    "print(f\"✓ Klassennamen extrahiert: {len(TOPIC_CLASSES)}\")\n",
    "print(f\"✓ Klassen: {TOPIC_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a02c648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vectors: (5465, 300), coverage 93.05%\n",
      "Val vectors:   (178, 300), coverage 92.06%\n",
      "Test vectors:  (1511, 300), coverage 92.71%\n"
     ]
    }
   ],
   "source": [
    "# Tweet Vectorization\n",
    "def tokenize(tweet: str) -> List[str]:\n",
    "    \"\"\"Simple whitespace tokenizer matching the preprocessed text format.\"\"\"\n",
    "    return tweet.strip().lower().split()\n",
    "\n",
    "def encode_tweet(text: str) -> np.ndarray:\n",
    "    \"\"\"Return the mean word2vec vector for a tweet; fall back to zeros.\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    vectors = [glove_vectors[token] for token in tokens if token in glove_vectors]\n",
    "    if not vectors:\n",
    "        return np.zeros(glove_vectors.vector_size, dtype=np.float32)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def compute_coverage(series: pd.Series) -> float:\n",
    "    total_tokens = 0\n",
    "    covered_tokens = 0\n",
    "    for text in series:\n",
    "        tokens = tokenize(text)\n",
    "        total_tokens += len(tokens)\n",
    "        covered_tokens += sum(token in glove_vectors for token in tokens)\n",
    "    return 0.0 if total_tokens == 0 else covered_tokens / total_tokens\n",
    "\n",
    "X_train = np.vstack(df_train[\"text\"].apply(encode_tweet).to_numpy())\n",
    "X_val = np.vstack(df_val[\"text\"].apply(encode_tweet).to_numpy())\n",
    "X_test = np.vstack(df_test[\"text\"].apply(encode_tweet).to_numpy())\n",
    "\n",
    "train_coverage = compute_coverage(df_train[\"text\"])\n",
    "val_coverage = compute_coverage(df_val[\"text\"])\n",
    "test_coverage = compute_coverage(df_test[\"text\"])\n",
    "\n",
    "print(f\"Train vectors: {X_train.shape}, coverage {train_coverage:.2%}\")\n",
    "print(f\"Val vectors:   {X_val.shape}, coverage {val_coverage:.2%}\")\n",
    "print(f\"Test vectors:  {X_test.shape}, coverage {test_coverage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7361e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Label Shapes:\n",
      "  y_train: (5465, 6)\n",
      "  y_val: (178, 6)\n",
      "  y_test: (1511, 6)\n",
      "\n",
      "============================================================\n",
      "NEURAL NETWORK CONFIGURATION (wie Lab 5)\n",
      "============================================================\n",
      "Input layer: 300 features (mean word2vec vectors)\n",
      "Hidden layers: (128, 64, 128)\n",
      "Output layer: 6 classes\n",
      "============================================================\n",
      "\n",
      "✓ Model erstellt (OneVsRestClassifier wie Lab 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LABEL ENCODING (wie Lab 5)\n",
    "# ============================================================\n",
    "\n",
    "# Verwende die vorbereiteten binären Labels direkt (wie in Lab 5)\n",
    "y_train = np.vstack(df_train['label_binary'].values)\n",
    "y_val = np.vstack(df_val['label_binary'].values)\n",
    "y_test = np.vstack(df_test['label_binary'].values)\n",
    "\n",
    "# Erstelle MultiLabelBinarizer für inverse_transform (wie in Lab 5)\n",
    "mlb = MultiLabelBinarizer(classes=TOPIC_CLASSES)\n",
    "mlb.fit([TOPIC_CLASSES])\n",
    "\n",
    "print(f\"✓ Label Shapes:\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "# Architecture as specified: 128 -> 64 -> 128\n",
    "HIDDEN_LAYERS = (128, 64, 128)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEURAL NETWORK CONFIGURATION (wie Lab 5)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input layer: {X_train.shape[1]} features (mean word2vec vectors)\")\n",
    "print(f\"Hidden layers: {HIDDEN_LAYERS}\")\n",
    "print(f\"Output layer: {y_train.shape[1]} classes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create MLP classifier (wie Lab 5)\n",
    "# early_stopping=False wegen Multi-Label Kompatibilität\n",
    "mlp_base = MLPClassifier(\n",
    "    hidden_layer_sizes=HIDDEN_LAYERS,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    early_stopping=False,  # Disabled für Multi-Label\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Wrap mit OneVsRestClassifier für multi-label support (wie Lab 5)\n",
    "mlp_classifier = OneVsRestClassifier(mlp_base, n_jobs=-1)\n",
    "\n",
    "print(\"\\n✓ Model erstellt (OneVsRestClassifier wie Lab 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "341deab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "Iteration 1, loss = 0.51802445\n",
      "Iteration 1, loss = 0.58959413\n",
      "Iteration 1, loss = 0.54069504\n",
      "Iteration 1, loss = 0.49035708\n",
      "Iteration 1, loss = 0.59700034\n",
      "Iteration 1, loss = 0.51865758\n",
      "Iteration 2, loss = 0.43056289\n",
      "Iteration 2, loss = 0.42144479\n",
      "Iteration 2, loss = 0.38862748\n",
      "Iteration 2, loss = 0.36490871\n",
      "Iteration 2, loss = 0.38605920\n",
      "Iteration 2, loss = 0.32859732\n",
      "Iteration 3, loss = 0.35093025\n",
      "Iteration 3, loss = 0.32173197\n",
      "Iteration 3, loss = 0.24938795\n",
      "Iteration 3, loss = 0.39801830\n",
      "Iteration 3, loss = 0.20241098\n",
      "Iteration 3, loss = 0.33564763\n",
      "Iteration 4, loss = 0.17642719\n",
      "Iteration 4, loss = 0.19983667\n",
      "Iteration 4, loss = 0.37545355\n",
      "Iteration 4, loss = 0.29732922\n",
      "Iteration 4, loss = 0.32217401\n",
      "Iteration 4, loss = 0.30434218\n",
      "Iteration 5, loss = 0.18387771\n",
      "Iteration 5, loss = 0.35893621\n",
      "Iteration 5, loss = 0.28712758\n",
      "Iteration 5, loss = 0.16578317\n",
      "Iteration 5, loss = 0.31203219\n",
      "Iteration 5, loss = 0.27959053\n",
      "Iteration 6, loss = 0.15851253\n",
      "Iteration 6, loss = 0.16955907\n",
      "Iteration 6, loss = 0.26899726\n",
      "Iteration 6, loss = 0.34045034\n",
      "Iteration 6, loss = 0.26349875\n",
      "Iteration 6, loss = 0.29591136\n",
      "Iteration 7, loss = 0.24722375\n",
      "Iteration 7, loss = 0.16018933\n",
      "Iteration 7, loss = 0.15270940\n",
      "Iteration 7, loss = 0.25663343\n",
      "Iteration 7, loss = 0.32537725\n",
      "Iteration 7, loss = 0.28498536\n",
      "Iteration 8, loss = 0.31577990\n",
      "Iteration 8, loss = 0.14700062\n",
      "Iteration 8, loss = 0.23698141\n",
      "Iteration 8, loss = 0.13807193\n",
      "Iteration 8, loss = 0.24472084\n",
      "Iteration 8, loss = 0.27189580\n",
      "Iteration 9, loss = 0.14048336\n",
      "Iteration 9, loss = 0.12672182\n",
      "Iteration 9, loss = 0.30267550\n",
      "Iteration 9, loss = 0.21380508\n",
      "Iteration 9, loss = 0.22774850\n",
      "Iteration 9, loss = 0.25471334\n",
      "Iteration 10, loss = 0.12595180\n",
      "Iteration 10, loss = 0.11978926\n",
      "Iteration 10, loss = 0.20933117\n",
      "Iteration 10, loss = 0.27437784\n",
      "Iteration 10, loss = 0.19444758\n",
      "Iteration 10, loss = 0.23814531\n",
      "Iteration 11, loss = 0.10915383\n",
      "Iteration 11, loss = 0.19728349\n",
      "Iteration 11, loss = 0.10832603\n",
      "Iteration 11, loss = 0.25598783\n",
      "Iteration 11, loss = 0.17161111\n",
      "Iteration 11, loss = 0.22074714\n",
      "Iteration 12, loss = 0.16408878\n",
      "Iteration 12, loss = 0.08991209\n",
      "Iteration 12, loss = 0.09906341\n",
      "Iteration 12, loss = 0.22815735\n",
      "Iteration 12, loss = 0.15953612\n",
      "Iteration 12, loss = 0.20531037\n",
      "Iteration 13, loss = 0.14402615\n",
      "Iteration 13, loss = 0.07416987\n",
      "Iteration 13, loss = 0.08371548\n",
      "Iteration 13, loss = 0.19610158\n",
      "Iteration 13, loss = 0.13026251\n",
      "Iteration 13, loss = 0.18712610\n",
      "Iteration 14, loss = 0.12157907\n",
      "Iteration 14, loss = 0.07418851\n",
      "Iteration 14, loss = 0.07585983\n",
      "Iteration 14, loss = 0.17135743\n",
      "Iteration 14, loss = 0.10908902\n",
      "Iteration 14, loss = 0.15161858\n",
      "Iteration 15, loss = 0.09833626\n",
      "Iteration 15, loss = 0.05591925\n",
      "Iteration 15, loss = 0.07035603\n",
      "Iteration 15, loss = 0.13662332\n",
      "Iteration 15, loss = 0.08936357\n",
      "Iteration 15, loss = 0.12969772\n",
      "Iteration 16, loss = 0.08471874\n",
      "Iteration 16, loss = 0.05545031\n",
      "Iteration 16, loss = 0.04304125\n",
      "Iteration 16, loss = 0.11147193\n",
      "Iteration 16, loss = 0.06900271\n",
      "Iteration 16, loss = 0.10839897\n",
      "Iteration 17, loss = 0.06795526\n",
      "Iteration 17, loss = 0.04567139\n",
      "Iteration 17, loss = 0.03367336\n",
      "Iteration 17, loss = 0.05682298\n",
      "Iteration 17, loss = 0.08474814\n",
      "Iteration 17, loss = 0.08874014\n",
      "Iteration 18, loss = 0.05590895Iteration 18, loss = 0.03461340\n",
      "\n",
      "Iteration 18, loss = 0.02803591\n",
      "Iteration 18, loss = 0.04110100\n",
      "Iteration 18, loss = 0.06213166\n",
      "Iteration 18, loss = 0.06847148\n",
      "Iteration 19, loss = 0.02565818\n",
      "Iteration 19, loss = 0.04001751\n",
      "Iteration 19, loss = 0.02792490\n",
      "Iteration 19, loss = 0.03127195\n",
      "Iteration 19, loss = 0.04694477\n",
      "Iteration 19, loss = 0.05016232\n",
      "Iteration 20, loss = 0.03493586\n",
      "Iteration 20, loss = 0.02012736\n",
      "Iteration 20, loss = 0.02322343\n",
      "Iteration 20, loss = 0.02618486\n",
      "Iteration 20, loss = 0.03248771\n",
      "Iteration 20, loss = 0.04241809\n",
      "Iteration 21, loss = 0.02607145\n",
      "Iteration 21, loss = 0.01826238\n",
      "Iteration 21, loss = 0.02245583\n",
      "Iteration 21, loss = 0.01751979\n",
      "Iteration 21, loss = 0.02325448\n",
      "Iteration 21, loss = 0.02834742\n",
      "Iteration 22, loss = 0.01316184\n",
      "Iteration 22, loss = 0.01604725\n",
      "Iteration 22, loss = 0.01832405\n",
      "Iteration 22, loss = 0.01201513\n",
      "Iteration 22, loss = 0.01804751\n",
      "Iteration 22, loss = 0.02123716\n",
      "Iteration 23, loss = 0.01055525\n",
      "Iteration 23, loss = 0.01195133\n",
      "Iteration 23, loss = 0.01228963\n",
      "Iteration 23, loss = 0.00955352\n",
      "Iteration 23, loss = 0.01199774\n",
      "Iteration 23, loss = 0.01834029\n",
      "Iteration 24, loss = 0.00665440\n",
      "Iteration 24, loss = 0.00882880Iteration 24, loss = 0.00677146\n",
      "\n",
      "Iteration 24, loss = 0.00927615\n",
      "Iteration 24, loss = 0.01040696\n",
      "Iteration 24, loss = 0.01191325\n",
      "Iteration 25, loss = 0.00532891\n",
      "Iteration 25, loss = 0.00808755\n",
      "Iteration 25, loss = 0.00704506\n",
      "Iteration 25, loss = 0.01397530\n",
      "Iteration 25, loss = 0.00417424\n",
      "Iteration 25, loss = 0.00915499\n",
      "Iteration 26, loss = 0.00443055\n",
      "Iteration 26, loss = 0.00541291\n",
      "Iteration 26, loss = 0.00687242\n",
      "Iteration 26, loss = 0.00928212\n",
      "Iteration 26, loss = 0.00483496\n",
      "Iteration 26, loss = 0.00670727\n",
      "Iteration 27, loss = 0.00869171\n",
      "Iteration 27, loss = 0.00307110\n",
      "Iteration 27, loss = 0.00438098\n",
      "Iteration 27, loss = 0.01024934\n",
      "Iteration 27, loss = 0.00303368\n",
      "Iteration 27, loss = 0.00823390\n",
      "Iteration 28, loss = 0.00431526\n",
      "Iteration 28, loss = 0.00413808\n",
      "Iteration 28, loss = 0.00565305\n",
      "Iteration 28, loss = 0.00815278\n",
      "Iteration 28, loss = 0.00515752\n",
      "Iteration 28, loss = 0.00806929\n",
      "Iteration 29, loss = 0.01760836\n",
      "Iteration 29, loss = 0.00318959\n",
      "Iteration 29, loss = 0.00543643\n",
      "Iteration 29, loss = 0.00442188\n",
      "Iteration 29, loss = 0.00968762\n",
      "Iteration 29, loss = 0.00903202\n",
      "Iteration 30, loss = 0.00414772\n",
      "Iteration 30, loss = 0.00306540\n",
      "Iteration 30, loss = 0.00564017\n",
      "Iteration 30, loss = 0.00588945\n",
      "Iteration 30, loss = 0.00283071\n",
      "Iteration 30, loss = 0.00713867\n",
      "Iteration 31, loss = 0.00468842\n",
      "Iteration 31, loss = 0.00283852\n",
      "Iteration 31, loss = 0.00376948\n",
      "Iteration 31, loss = 0.00615528\n",
      "Iteration 31, loss = 0.00546139\n",
      "Iteration 31, loss = 0.00494291\n",
      "Iteration 32, loss = 0.00597513\n",
      "Iteration 32, loss = 0.00279508\n",
      "Iteration 32, loss = 0.00501616\n",
      "Iteration 32, loss = 0.00221753\n",
      "Iteration 32, loss = 0.00407302\n",
      "Iteration 32, loss = 0.00257774\n",
      "Iteration 33, loss = 0.00352675\n",
      "Iteration 33, loss = 0.00306647\n",
      "Iteration 33, loss = 0.00457995\n",
      "Iteration 33, loss = 0.00456042\n",
      "Iteration 33, loss = 0.00231316\n",
      "Iteration 33, loss = 0.00190523\n",
      "Iteration 34, loss = 0.00204641\n",
      "Iteration 34, loss = 0.00324098\n",
      "Iteration 34, loss = 0.00237208\n",
      "Iteration 34, loss = 0.00317437\n",
      "Iteration 34, loss = 0.00427967\n",
      "Iteration 34, loss = 0.00124321\n",
      "Iteration 35, loss = 0.00189553\n",
      "Iteration 35, loss = 0.00294504\n",
      "Iteration 35, loss = 0.00220813\n",
      "Iteration 35, loss = 0.00313477\n",
      "Iteration 35, loss = 0.00442762\n",
      "Iteration 35, loss = 0.00097026\n",
      "Iteration 36, loss = 0.00125108\n",
      "Iteration 36, loss = 0.00155274\n",
      "Iteration 36, loss = 0.00328718\n",
      "Iteration 36, loss = 0.00324974\n",
      "Iteration 36, loss = 0.00365426\n",
      "Iteration 36, loss = 0.00089912\n",
      "Iteration 37, loss = 0.00253448\n",
      "Iteration 37, loss = 0.00194605\n",
      "Iteration 37, loss = 0.00297266\n",
      "Iteration 37, loss = 0.00355077\n",
      "Iteration 37, loss = 0.00377120\n",
      "Iteration 37, loss = 0.00149300\n",
      "Iteration 38, loss = 0.00087710\n",
      "Iteration 38, loss = 0.00462491\n",
      "Iteration 38, loss = 0.00222427\n",
      "Iteration 38, loss = 0.00461367\n",
      "Iteration 38, loss = 0.00334784\n",
      "Iteration 38, loss = 0.00087293\n",
      "Iteration 39, loss = 0.00063002\n",
      "Iteration 39, loss = 0.00393223\n",
      "Iteration 39, loss = 0.00304229\n",
      "Iteration 39, loss = 0.00462566\n",
      "Iteration 39, loss = 0.00355680\n",
      "Iteration 39, loss = 0.00073414\n",
      "Iteration 40, loss = 0.00063370\n",
      "Iteration 40, loss = 0.00347065\n",
      "Iteration 40, loss = 0.00215233\n",
      "Iteration 40, loss = 0.00276927\n",
      "Iteration 40, loss = 0.00385069\n",
      "Iteration 40, loss = 0.00059324\n",
      "Iteration 41, loss = 0.00051477\n",
      "Iteration 41, loss = 0.00410435\n",
      "Iteration 41, loss = 0.00263896\n",
      "Iteration 41, loss = 0.00413502\n",
      "Iteration 41, loss = 0.00268896\n",
      "Iteration 41, loss = 0.00053428\n",
      "Iteration 42, loss = 0.00045354\n",
      "Iteration 42, loss = 0.03286975\n",
      "Iteration 42, loss = 0.00201347\n",
      "Iteration 42, loss = 0.00284334\n",
      "Iteration 42, loss = 0.00224291\n",
      "Iteration 42, loss = 0.00050194\n",
      "Iteration 43, loss = 0.00043944\n",
      "Iteration 43, loss = 0.01422086\n",
      "Iteration 43, loss = 0.00220271\n",
      "Iteration 43, loss = 0.00261652\n",
      "Iteration 43, loss = 0.00126244\n",
      "Iteration 43, loss = 0.00047772\n",
      "Iteration 44, loss = 0.00040842\n",
      "Iteration 44, loss = 0.00576571\n",
      "Iteration 44, loss = 0.00223012\n",
      "Iteration 44, loss = 0.00242254\n",
      "Iteration 44, loss = 0.00136422\n",
      "Iteration 44, loss = 0.00045802\n",
      "Iteration 45, loss = 0.00038319\n",
      "Iteration 45, loss = 0.00297749\n",
      "Iteration 45, loss = 0.00161333\n",
      "Iteration 45, loss = 0.00396197\n",
      "Iteration 45, loss = 0.00217236\n",
      "Iteration 45, loss = 0.00044094\n",
      "Iteration 46, loss = 0.00037209\n",
      "Iteration 46, loss = 0.00291210\n",
      "Iteration 46, loss = 0.00153396\n",
      "Iteration 46, loss = 0.00275301\n",
      "Iteration 46, loss = 0.00100291\n",
      "Iteration 46, loss = 0.00042220\n",
      "Iteration 47, loss = 0.00037018\n",
      "Iteration 47, loss = 0.00158095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.00244557\n",
      "Iteration 47, loss = 0.00182306\n",
      "Iteration 47, loss = 0.00217644\n",
      "Iteration 47, loss = 0.00040797\n",
      "Iteration 48, loss = 0.00034115\n",
      "Iteration 48, loss = 0.00194361\n",
      "Iteration 48, loss = 0.00517846\n",
      "Iteration 48, loss = 0.00250267\n",
      "Iteration 48, loss = 0.00039665\n",
      "Iteration 49, loss = 0.00033393\n",
      "Iteration 49, loss = 0.00233893\n",
      "Iteration 49, loss = 0.00373411\n",
      "Iteration 49, loss = 0.00276273\n",
      "Iteration 49, loss = 0.00038547\n",
      "Iteration 50, loss = 0.00033363\n",
      "Iteration 50, loss = 0.00290994\n",
      "Iteration 50, loss = 0.00332916\n",
      "Iteration 50, loss = 0.00131896\n",
      "Iteration 50, loss = 0.00037074\n",
      "Iteration 51, loss = 0.00034218\n",
      "Iteration 51, loss = 0.00252688\n",
      "Iteration 51, loss = 0.00709431\n",
      "Iteration 51, loss = 0.00133790\n",
      "Iteration 51, loss = 0.00035993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.00034852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.00240568\n",
      "Iteration 52, loss = 0.01063304\n",
      "Iteration 52, loss = 0.00069430\n",
      "Iteration 53, loss = 0.00130226\n",
      "Iteration 53, loss = 0.07689297\n",
      "Iteration 53, loss = 0.00119298\n",
      "Iteration 54, loss = 0.00213925\n",
      "Iteration 54, loss = 0.08088181\n",
      "Iteration 54, loss = 0.00125472\n",
      "Iteration 55, loss = 0.00238916\n",
      "Iteration 55, loss = 0.02482533\n",
      "Iteration 55, loss = 0.00094403\n",
      "Iteration 56, loss = 0.00312756\n",
      "Iteration 56, loss = 0.01498518\n",
      "Iteration 56, loss = 0.00052279\n",
      "Iteration 57, loss = 0.00291912\n",
      "Iteration 57, loss = 0.00649021\n",
      "Iteration 57, loss = 0.00087367\n",
      "Iteration 58, loss = 0.00294272\n",
      "Iteration 58, loss = 0.00496248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.00123840\n",
      "Iteration 59, loss = 0.00545625\n",
      "Iteration 59, loss = 0.00121067\n",
      "Iteration 60, loss = 0.02670732\n",
      "Iteration 60, loss = 0.00087005\n",
      "Iteration 61, loss = 0.05391586\n",
      "Iteration 61, loss = 0.00097623\n",
      "Iteration 62, loss = 0.04936769\n",
      "Iteration 62, loss = 0.00111888\n",
      "Iteration 63, loss = 0.02507716\n",
      "Iteration 63, loss = 0.00092789\n",
      "Iteration 64, loss = 0.01335361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.00058853\n",
      "Iteration 65, loss = 0.00090259\n",
      "Iteration 66, loss = 0.00092253\n",
      "Iteration 67, loss = 0.00077153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "✓ Training completed in 3.61 seconds\n",
      "✓ 6/6 classifiers converged before max_iter\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "training_time = time. time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Check convergence\n",
    "converged = sum(1 for est in mlp_classifier.estimators_ if est.n_iter_ < est.max_iter)\n",
    "print(f\"✓ {converged}/{NUM_CLASSES} classifiers converged before max_iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a003895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION SET METRICS\n",
      "============================================================\n",
      "Subset Accuracy     : 0.5112\n",
      "Hamming Loss        : 0.1199\n",
      "Micro F1            : 0.7181\n",
      "Macro F1            : 0.6504\n",
      "Weighted F1         : 0.7142\n",
      "Micro Precision     : 0.7546\n",
      "Micro Recall        : 0.6849\n",
      "\n",
      "============================================================\n",
      "TEST SET METRICS\n",
      "============================================================\n",
      "Subset Accuracy     : 0.4957\n",
      "Hamming Loss        : 0.1263\n",
      "Micro F1            : 0.6976\n",
      "Macro F1            : 0.6315\n",
      "Weighted F1         : 0.6946\n",
      "Micro Precision     : 0.7315\n",
      "Micro Recall        : 0.6668\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred_val = mlp_classifier.predict(X_val)\n",
    "y_pred_test = mlp_classifier.predict(X_test)\n",
    "\n",
    "def evaluate(y_true, y_pred, split_name):\n",
    "    \"\"\"Print evaluation metrics for a given split.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{split_name. upper()} SET METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Subset Accuracy':<20}: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"{'Hamming Loss':<20}: {hamming_loss(y_true, y_pred):.4f}\")\n",
    "    print(f\"{'Micro F1':<20}: {f1_score(y_true, y_pred, average='micro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Macro F1':<20}: {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Weighted F1':<20}: {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"{'Micro Precision':<20}: {precision_score(y_true, y_pred, average='micro', zero_division=0):.4f}\")\n",
    "    print(f\"{'Micro Recall':<20}: {recall_score(y_true, y_pred, average='micro', zero_division=0):.4f}\")\n",
    "\n",
    "# Evaluate on validation set (for model selection)\n",
    "evaluate(y_val, y_pred_val, \"Validation\")\n",
    "\n",
    "# Evaluate on test set (final performance)\n",
    "evaluate(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d50aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PER-CLASS PERFORMANCE (Test Set)\n",
      "============================================================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "celebrity_&_pop_culture       0.44      0.38      0.41       245\n",
      "   diaries_&_daily_life       0.48      0.46      0.47       149\n",
      "        film_tv_&_video       0.63      0.55      0.59       298\n",
      "                  music       0.83      0.81      0.82       380\n",
      "  news_&_social_concern       0.68      0.55      0.61       327\n",
      "                 sports       0.92      0.87      0.89       582\n",
      "\n",
      "              micro avg       0.73      0.67      0.70      1981\n",
      "              macro avg       0.66      0.60      0.63      1981\n",
      "           weighted avg       0.73      0.67      0.69      1981\n",
      "            samples avg       0.71      0.71      0.69      1981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Per-class performance on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS PERFORMANCE (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_pred_test,\n",
    "    target_names=TOPIC_CLASSES,  # Geändert von TOPIC_LABELS zu TOPIC_CLASSES\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b5a0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "per_class_f1 = f1_score(y_test, y_pred_test, average=None, zero_division=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(TOPIC_CLASSES, per_class_f1, color='steelblue')  # Geändert zu TOPIC_CLASSES\n",
    "ax.set_xlabel('F1 Score')\n",
    "ax.set_title('Per-Class F1 Scores (Test Set)')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for bar, score in zip(bars, per_class_f1):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{score:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315d859",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation and Discussion\n",
    "\n",
    "### 5.1 Quantitative Comparison\n",
    "- Summarize validation/test metrics (Subset Accuracy, Hamming Loss, F1 variants, Precision/Recall) from Section 4 and contrast them with the Lab 5 TF‑IDF baseline.\n",
    "- Highlight key differences: e.g., Word2Vec inputs deliver slightly lower Subset Accuracy but competitive Micro-F1; Macro-F1 remains challenging for rare labels.\n",
    "- Note potential reasons (embedding coverage, averaging strategy, differences in token distributions between splits).\n",
    "\n",
    "### 5.2 Qualitative Inspection\n",
    "- Manually inspect a handful of tweets with their true vs. predicted labels to understand where embeddings help or hurt.\n",
    "- Record interesting successes/failures for the report (e.g., sports-related tweets predicted correctly despite unusual wording, or slang-heavy texts where coverage drops).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "396e15ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE PREDICTIONS (Test Set)\n",
      "============================================================\n",
      "\n",
      "✓ Sample 907:\n",
      "   Text: watch iowa football spring practice strength coach raimond brathwaite speak medi...\n",
      "   True: ('sports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✓ Sample 617:\n",
      "   Text: monterrey giant awake monterrey second large city old mexico large city natural ...\n",
      "   True: ('news_&_social_concern',)\n",
      "   Pred: ('news_&_social_concern',)\n",
      "\n",
      "✗ Sample 1385:\n",
      "   Text: congrat lee kiefer fence woman foil individual gold medal win oakley pump awesom...\n",
      "   True: ('sports',)\n",
      "   Pred: ('news_&_social_concern', 'sports')\n",
      "\n",
      "✓ Sample 941:\n",
      "   Text: worry covid vaccine blood clot...\n",
      "   True: ('news_&_social_concern',)\n",
      "   Pred: ('news_&_social_concern',)\n",
      "\n",
      "✓ Sample 303:\n",
      "   Text: enter sweepstake win whiskey pc perfect paddle packrafting bikerafting enter tod...\n",
      "   True: ('sports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✓ Sample 1478:\n",
      "   Text: check new album welcome playoff champ distribute live apple music...\n",
      "   True: ('music',)\n",
      "   Pred: ('music',)\n",
      "\n",
      "✓ Sample 175:\n",
      "   Text: episode today episode deep dive packer bucs matchup look closely update injury r...\n",
      "   True: ('sports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✗ Sample 342:\n",
      "   Text: vote dynamite good music video tdy award good music video...\n",
      "   True: ('celebrity_&_pop_culture', 'music')\n",
      "   Pred: ('music',)\n",
      "\n",
      "✓ Sample 1307:\n",
      "   Text: accord timeline alot mother love permission dance beautiful thankyou bt give mot...\n",
      "   True: ('music',)\n",
      "   Pred: ('music',)\n",
      "\n",
      "✗ Sample 1229:\n",
      "   Text: glad secretly get visit long deceased family uk manage pub crawl time haven see ...\n",
      "   True: ('news_&_social_concern',)\n",
      "   Pred: ('film_tv_&_video',)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAMPLE PREDICTIONS (wie Lab 5)\n",
    "# ============================================================\n",
    "\n",
    "# Verwende mlb.inverse_transform für lesbare Labels (wie Lab 5)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_test)\n",
    "y_true_labels = mlb.inverse_transform(y_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sample_idx = np.random.choice(len(df_test), size=10, replace=False)\n",
    "\n",
    "for idx in sample_idx:\n",
    "    text = df_test['text'].iloc[idx]\n",
    "    true = y_true_labels[idx] if y_true_labels[idx] else ('none',)\n",
    "    pred = y_pred_labels[idx] if y_pred_labels[idx] else ('none',)\n",
    "    \n",
    "    match = \"✓\" if set(true) == set(pred) else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{match} Sample {idx}:\")\n",
    "    print(f\"   Text: {str(text)[:80]}...\")\n",
    "    print(f\"   True: {true}\")\n",
    "    print(f\"   Pred: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6615eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL MODEL SUMMARY\n",
      "============================================================\n",
      "Embedding: gensim 'word2vec-google-news-300' (mean pooled per tweet)\n",
      "Architecture: Input(300) → 128 → 64 → 128 → Output(6)\n",
      "Training samples: 5,465\n",
      "Validation samples: 178\n",
      "Test samples: 1,511\n",
      "Test Micro F1: 0.6976\n",
      "Test Macro F1: 0.6315\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10d8418a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1078d98a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10420d8a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1040d98a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1050798a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1110718a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10906d8a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x103a6d8a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x11026d8a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1079418a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x111e6d8a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1067a98a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 84, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 93, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 118, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "test_f1_micro = f1_score(y_test, y_pred_test, average='micro', zero_division=0)\n",
    "test_f1_macro = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Embedding: gensim 'word2vec-google-news-300' (mean pooled per tweet)\")\n",
    "print(f\"Architecture: Input({X_train.shape[1]}) → 128 → 64 → 128 → Output({NUM_CLASSES})\")\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Validation samples: {len(y_val):,}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "print(f\"Test Micro F1: {test_f1_micro:.4f}\")\n",
    "print(f\"Test Macro F1: {test_f1_macro:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8e5dc",
   "metadata": {},
   "source": [
    "---\n",
    "### Closing Notes\n",
    "- Word2Vec inputs (mean-pooled `word2vec-google-news-300`) **outperformed** the Lab 5 TF-IDF baseline across all metrics:\n",
    "  - **Subset Accuracy**: 0.496 vs 0.453 (+4.3%)\n",
    "  - **Micro F1**: 0.698 vs 0.645 (+5.3%)\n",
    "  - **Macro F1**: 0.632 vs 0.564 (+6.8%)\n",
    "  - **Hamming Loss**: 0.126 vs 0.144 (lower is better, -12.5%)\n",
    "- The dense semantic representations from pretrained Word2Vec capture topic-relevant information more effectively than sparse bag-of-words features.\n",
    "- Qualitative samples show that straightforward business/sports tweets are captured well, while slang-heavy or highly emotional tweets may lose some topic signal when averaged.\n",
    "- Future work: combine TF-IDF and embeddings, fine-tune classification thresholds, or use contextualized embeddings (BERT) for further improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
