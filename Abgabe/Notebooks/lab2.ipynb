{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Preprocessing Pipeline\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook provides a comprehensive workflow for text data analysis and preprocessing.\n",
    "\n",
    "- **Data Loading**: Set up the environment, load and explore text datasets, perform statistical analysis, and create reusable data loading functions\n",
    "- **Preprocessing**: Apply various NLP preprocessing techniques using NLTK and SpaCy, analyze the impact of different methods, and develop a reusable preprocessing pipeline\n",
    "\n",
    "The notebook is structured to systematically work through each step of the text analysis pipeline, from the instructions of the second lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "This section covers the initial setup and data loading phase. We will install necessary libraries, load the dataset, explore its structure and characteristics, perform statistical analysis, filter the data to text-only content, and create reusable functions for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Libraries\n",
    "\n",
    "Installation of pandas for data manipulation, NLTK for natural language processing, and SpaCy for advanced NLP tasks. This cell ensures all necessary dependencies are available for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.13/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.13/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas \n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "%pip install pyarrow\n",
    "%pip install tqdm\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Explore Dataset\n",
    "\n",
    "Loading the dataset into a pandas DataFrame and performing initial exploration. This includes viewing the first few rows, checking data types, examining the shape of the dataset, and identifying any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Basic Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count, Column count: (6090, 5)\n",
      "\n",
      "Column Names: ['text', 'date', 'label', 'label_name', 'id']\n",
      "\n",
      "Data Types:\n",
      "text          object\n",
      "date          object\n",
      "label         object\n",
      "label_name    object\n",
      "id            object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "text          0\n",
      "date          0\n",
      "label         0\n",
      "label_name    0\n",
      "id            0\n",
      "dtype: int64\n",
      "\n",
      "First 10 Rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "label_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a6788bfa-34be-46a3-b315-3314a0958a1b",
       "rows": [
        [
         "0",
         "The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} 4-0 in Game 1 of the Western Division finals. Evan Edwards hit a 2-run HR. WP Josh Roberson: 5 IP, 3 H, 0 R, 0 BB, 10 K #MWLplayoffs #MWLscoreboard",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516324419866624"
        ],
        [
         "1",
         "I would rather hear Eli Gold announce this Auburn game than these dumbasses. {@ESPN@}",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516440690176006"
        ],
        [
         "2",
         "Someone take my phone away, I’m trying to not look at {@Chicago Blackhawks@} home game tickets in October ",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516543387709440"
        ],
        [
         "3",
         "A year ago, Louisville struggled to beat an FCS opponent, ISU.  Yes they won 31-7, but score wasn’t indicative of the game flow.  Today, they are demoralizing a better FCS opponent in EKU. {@Coach Satterfield@} thank you!! {{USERNAME}} , glad you’re gone.",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516620466429953"
        ],
        [
         "4",
         "Anyone know why the #Dodgers #Orioles game next Thursday 9/12 is on Fox?? That’s arguably the last game on the entire schedule I’d imagine being on Fox. {@Bill Shaikin@} {@Eric Stephen@} {@David Vassegh@}",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516711411310592"
        ],
        [
         "5",
         "I don’t care. you gave him a shot, he is struggling put Joey in and see what he has to offer. This is the game  you decide who you want starting the Mississippi state game. {@Coach Gus Malzahn@} ‍♂️",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516891053580288"
        ],
        [
         "6",
         "Okay how can I watch the {@Arkansas State Football@} and UNLV game on FB?",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516916554936322"
        ],
        [
         "7",
         "Check out largest crowds ever for a basketball game! Phil Arena aims to set new attendance record for FIBA World Cup final in 2023, targets over 50,000 & if FIBA allows SRO, 60,000! Incredible! {{URL}} {@The Philippine Star@} {{USERNAME}} {@Philstar Sports Hub@} ",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]",
         "['sports']",
         "1170516940902805504"
        ],
        [
         "8",
         "I voted #WeWantNCAAFootball on {{USERNAME}} . It’s the best video game! {{URL}}",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]",
         "['gaming' 'sports']",
         "1170517092489187328"
        ],
        [
         "9",
         "Streaming a new game #minionmasters come stop by and join me!! {{USERNAME}} {{USERNAME}} #FearThePack #supportsmallstreamers #SmallStreamersConnect #smallstreamer #twitchaffiliate #twitch #twitchprime #subtember",
         "2019-09-08",
         "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]",
         "['gaming']",
         "1170546366566846464"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The {@Clinton LumberKings@} beat the {@Cedar R...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516324419866624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would rather hear Eli Gold announce this Aub...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516440690176006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Someone take my phone away, I’m trying to not ...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516543387709440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A year ago, Louisville struggled to beat an FC...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516620466429953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anyone know why the #Dodgers #Orioles game nex...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516711411310592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don’t care. you gave him a shot, he is strug...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516891053580288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Okay how can I watch the {@Arkansas State Foot...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516916554936322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Check out largest crowds ever for a basketball...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sports]</td>\n",
       "      <td>1170516940902805504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I voted #WeWantNCAAFootball on {{USERNAME}} . ...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[gaming, sports]</td>\n",
       "      <td>1170517092489187328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Streaming a new game #minionmasters come stop ...</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[gaming]</td>\n",
       "      <td>1170546366566846464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  The {@Clinton LumberKings@} beat the {@Cedar R...  2019-09-08   \n",
       "1  I would rather hear Eli Gold announce this Aub...  2019-09-08   \n",
       "2  Someone take my phone away, I’m trying to not ...  2019-09-08   \n",
       "3  A year ago, Louisville struggled to beat an FC...  2019-09-08   \n",
       "4  Anyone know why the #Dodgers #Orioles game nex...  2019-09-08   \n",
       "5  I don’t care. you gave him a shot, he is strug...  2019-09-08   \n",
       "6  Okay how can I watch the {@Arkansas State Foot...  2019-09-08   \n",
       "7  Check out largest crowds ever for a basketball...  2019-09-08   \n",
       "8  I voted #WeWantNCAAFootball on {{USERNAME}} . ...  2019-09-08   \n",
       "9  Streaming a new game #minionmasters come stop ...  2019-09-08   \n",
       "\n",
       "                                               label        label_name  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          [sports]   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  [gaming, sports]   \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...          [gaming]   \n",
       "\n",
       "                    id  \n",
       "0  1170516324419866624  \n",
       "1  1170516440690176006  \n",
       "2  1170516543387709440  \n",
       "3  1170516620466429953  \n",
       "4  1170516711411310592  \n",
       "5  1170516891053580288  \n",
       "6  1170516916554936322  \n",
       "7  1170516940902805504  \n",
       "8  1170517092489187328  \n",
       "9  1170546366566846464  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Row count, Column count:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nFirst 10 Rows:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Initial Data Exploration with NLTK\n",
    "\n",
    "Using NLTK to perform basic text exploration and tokenization on a sample of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INITIAL TEXT EXPLORATION WITH NLTK\n",
      "==================================================\n",
      "\n",
      "Sample Tweet Tokenization:\n",
      "Original: The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} 4-0 in Game 1 of the Western Division finals. Evan Edwards hit a 2-run HR. WP Josh Roberson: 5 IP, 3 H, 0 R, 0 BB, 10 K #MWLplayoffs #MWLscoreboard\n",
      "Tokens: ['The', '{', '@', 'Clinton', 'LumberKings', '@', '}', 'beat', 'the', '{', '@', 'Cedar', 'Rapids', 'Kernels', '@', '}', '4-0', 'in', 'Game', '1', 'of', 'the', 'Western', 'Division', 'finals', '.', 'Evan', 'Edwards', 'hit', 'a', '2-run', 'HR', '.', 'WP', 'Josh', 'Roberson', ':', '5', 'IP', ',', '3', 'H', ',', '0', 'R', ',', '0', 'BB', ',', '10', 'K', '#', 'MWLplayoffs', '#', 'MWLscoreboard']\n",
      "\n",
      "Number of English stopwords: 198\n",
      "Sample stopwords: ['this', 'own', 'up', \"you're\", \"he's\", 're', 'too', 'wouldn', 'them', 'mustn']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH NLTK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show sample tokenization on a few tweets\n",
    "print(\"\\nSample Tweet Tokenization:\")\n",
    "sample_text = df['text'].iloc[0]\n",
    "print(f\"Original: {sample_text}\")\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nNumber of English stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Initial Data Exploration with spaCy\n",
    "\n",
    "Using spaCy to perform basic linguistic analysis on sample tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INITIAL TEXT EXPLORATION WITH spaCy\n",
      "==================================================\n",
      "\n",
      "Sample Tweet: I don’t care. you gave him a shot, he is struggling put Joey in and see what he has to offer. This is the game  you decide who you want starting the Mississippi state game. {@Coach Gus Malzahn@} ‍♂️\n",
      "\n",
      "Tokens: ['I', 'do', 'n’t', 'care', '.', 'you', 'gave', 'him', 'a', 'shot', ',', 'he', 'is', 'struggling', 'put', 'Joey', 'in', 'and', 'see', 'what', 'he', 'has', 'to', 'offer', '.', 'This', 'is', 'the', 'game', ' ', 'you', 'decide', 'who', 'you', 'want', 'starting', 'the', 'Mississippi', 'state', 'game', '.', '{', '@Coach', 'Gus', 'Malzahn@', '}', '\\u200d', '♂', '️']\n",
      "\n",
      "POS Tags: [('I', 'PRON'), ('do', 'AUX'), ('n’t', 'NOUN'), ('care', 'VERB'), ('.', 'PUNCT'), ('you', 'PRON'), ('gave', 'VERB'), ('him', 'PRON'), ('a', 'DET'), ('shot', 'NOUN')]\n",
      "\n",
      "Entities: [('Joey', 'PERSON'), ('Mississippi', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH spaCy\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze a sample tweet\n",
    "sample_text = df['text'].iloc[5]\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(f\"\\nSample Tweet: {sample_text}\")\n",
    "print(f\"\\nTokens: {[token.text for token in doc]}\")\n",
    "print(f\"\\nPOS Tags: {[(token.text, token.pos_) for token in doc][:10]}\")\n",
    "print(f\"\\nEntities: {[(ent.text, ent.label_) for ent in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Statistics\n",
    "\n",
    "Computing descriptive statistics on the dataset to understand the data distribution. This includes category distributions, temporal patterns, duplicate analysis, text length statistics, word frequency analysis, and named entity statistics using pandas and NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Category/Topic Distribution Analysis\n",
    "\n",
    "Analyzing how many tweets are available for each topic category using pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TWEETS PER CATEGORY\n",
      "==================================================\n",
      "                   Topic  Count\n",
      "   news_&_social_concern   1782\n",
      "                  sports   1683\n",
      "                   music   1131\n",
      "         film_tv_&_video    953\n",
      " celebrity_&_pop_culture    924\n",
      "    diaries_&_daily_life    866\n",
      "          arts_&_culture    298\n",
      "    science_&_technology    294\n",
      "business_&_entrepreneurs    288\n",
      "        fitness_&_health    275\n",
      "           other_hobbies    265\n",
      "           relationships    264\n",
      "                  family    252\n",
      "                  gaming    245\n",
      "  learning_&_educational    154\n",
      "           food_&_dining    152\n",
      "      travel_&_adventure    108\n",
      "         fashion_&_style    107\n",
      "    youth_&_student_life     94\n",
      "\n",
      "Total unique topics: 19\n",
      "\n",
      "Tweets with 1 topic: 3074 (50.5%)\n",
      "Tweets with 2+ topics: 3015 (49.5%)\n",
      "\n",
      "Max topics per tweet: 6\n",
      "Average topics per tweet: 1.66\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Convert label_name to lists (they're already arrays, just ensure they're lists)\n",
    "df['topics_list'] = df['label_name'].apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "\n",
    "# Count tweets per category\n",
    "all_topics = []\n",
    "for topics in df['topics_list']:\n",
    "    all_topics.extend(topics)\n",
    "\n",
    "topic_counts = Counter(all_topics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "topic_df = pd.DataFrame(\n",
    "    topic_counts.items(), \n",
    "    columns=['Topic', 'Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TWEETS PER CATEGORY\")\n",
    "print(\"=\"*50)\n",
    "print(topic_df.to_string(index=False))\n",
    "print(f\"\\nTotal unique topics: {len(topic_counts)}\")\n",
    "\n",
    "# Multi-label statistics\n",
    "df['num_topics'] = df['topics_list'].apply(len)\n",
    "print(f\"\\nTweets with 1 topic: {(df['num_topics'] == 1).sum()} ({(df['num_topics'] == 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"Tweets with 2+ topics: {(df['num_topics'] > 1).sum()} ({(df['num_topics'] > 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nMax topics per tweet: {df['num_topics'].max()}\")\n",
    "print(f\"Average topics per tweet: {df['num_topics'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Temporal Distribution Analysis\n",
    "\n",
    "Analyzing the distribution of tweets across different time periods using pandas datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPORAL DISTRIBUTION STATISTICS\n",
      "==================================================\n",
      "\n",
      "Tweets per Year:\n",
      "  2019: 1,555 tweets (25.5%)\n",
      "  2020: 3,508 tweets (57.6%)\n",
      "  2021: 1,027 tweets (16.9%)\n",
      "\n",
      "Date range: 2019-09-08 to 2021-08-29\n",
      "Total days covered: 721 days\n",
      "\n",
      "Tweets per Month (across all years):\n",
      "  Jan: 508 tweets\n",
      "  Feb: 491 tweets\n",
      "  Mar: 509 tweets\n",
      "  Apr: 492 tweets\n",
      "  May: 497 tweets\n",
      "  Jun: 504 tweets\n",
      "  Jul: 521 tweets\n",
      "  Aug: 535 tweets\n",
      "  Sep: 494 tweets\n",
      "  Oct: 485 tweets\n",
      "  Nov: 508 tweets\n",
      "  Dec: 546 tweets\n",
      "\n",
      "Tweets per Day of Week:\n",
      "  Sunday: 4,698 tweets (77.1%)\n",
      "  Friday: 238 tweets (3.9%)\n",
      "  Wednesday: 236 tweets (3.9%)\n",
      "  Saturday: 236 tweets (3.9%)\n",
      "  Monday: 234 tweets (3.8%)\n",
      "  Thursday: 228 tweets (3.7%)\n",
      "  Tuesday: 220 tweets (3.6%)\n"
     ]
    }
   ],
   "source": [
    "# Convert date column to datetime\n",
    "df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date_parsed'].dt.year\n",
    "df['month'] = df['date_parsed'].dt.month\n",
    "df['day_of_week'] = df['date_parsed'].dt.day_name()\n",
    "\n",
    "# Count tweets per year\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nTweets per Year:\")\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} tweets ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDate range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")\n",
    "print(f\"Total days covered: {(df['date_parsed'].max() - df['date_parsed'].min()).days} days\")\n",
    "\n",
    "# Month distribution\n",
    "print(\"\\nTweets per Month (across all years):\")\n",
    "month_counts = df['month'].value_counts().sort_index()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for month, count in month_counts.items():\n",
    "    print(f\"  {month_names[month-1]}: {count:,} tweets\")\n",
    "\n",
    "# Day of week distribution\n",
    "print(\"\\nTweets per Day of Week:\")\n",
    "dow_counts = df['day_of_week'].value_counts()\n",
    "for day, count in dow_counts.items():\n",
    "    print(f\"  {day}: {count:,} tweets ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Duplicate Detection Statistics\n",
    "\n",
    "Identifying and quantifying exact and potential near-duplicate tweets using pandas string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DUPLICATE ANALYSIS\n",
      "==================================================\n",
      "Exact duplicate tweets: 0 (0.00%)\n",
      "Duplicate IDs: 0\n",
      "\n",
      "✓ No exact duplicate tweets found!\n",
      "\n",
      "Potential near-duplicates (same first 50 chars): 83 (1.36%)\n",
      "\n",
      "All Near-Duplicate Groups (sorted by group size):\n",
      "\n",
      "Total near-duplicate groups: 38\n",
      "Largest group size: 28\n",
      "\n",
      "Group 1 (Size: 28):\n",
      "  Text preview: check out what i just added to my closet on poshma...\n",
      "  IDs: ['1173082475607707649', '1173203031145345024', '1173294716093251585', '1179938888405487616', '1180872625360265216', '1181054714986684416', '1183524248574296065', '1218953515298484224', '1223968032122265600', '1226409017716592641', '1226593965203312640', '1231577835065602050', '1236652371863678977', '1246699764982239232', '1246915880715313152', '1246946850952732673', '1267153856233738240', '1277330021317668864', '1297499158480207872', '1297500377302982656', '1297500531720466432', '1297531856431140865', '1297594410046119936', '1300070301758140416', '1305110632270684161', '1343222350125285376', '1353492013765648384', '1358348424672813056']\n",
      "\n",
      "Group 2 (Size: 7):\n",
      "  Text preview: earning #cryptocurrency for selling my stuff on {@...\n",
      "  IDs: ['1188333958565650432', '1188364046032134144', '1226500554282717184', '1234024844737597440', '1234114272139018241', '1234174695899045889', '1234175466468188160']\n",
      "\n",
      "Group 3 (Size: 4):\n",
      "  Text preview: check out this listing i just added to my #poshmar...\n",
      "  IDs: ['1373662453645545481', '1396402654490726402', '1421862601680572416', '1427052531734048773']\n",
      "\n",
      "Group 4 (Size: 4):\n",
      "  Text preview: check out my new album  above the rain  distribute...\n",
      "  IDs: ['1198591092205260800', '1198592224981848071', '1198592833546047490', '1198594266613854208']\n",
      "\n",
      "Group 5 (Size: 4):\n",
      "  Text preview: fresh find !! >> discover new music and hot talent...\n",
      "  IDs: ['1429380615699378180', '1429381341221695489', '1429381949844557824', '1429382515769417730']\n",
      "\n",
      "Group 6 (Size: 4):\n",
      "  Text preview: earning gift cards for selling my stuff on {@listi...\n",
      "  IDs: ['1307680273337810945', '1342317709594267648', '1342317709783134208', '1342318714071494657']\n",
      "\n",
      "Group 7 (Size: 4):\n",
      "  Text preview: join me to get {@listia@}  s new $xnk #cryptocurre...\n",
      "  IDs: ['1188334045895254016', '1188454341943988225', '1188514741695795201', '1188604341323739137']\n",
      "\n",
      "Group 8 (Size: 3):\n",
      "  Text preview: california investigators: extend naya rivera s sea...\n",
      "  IDs: ['1282159286462840832', '1282220385447096320', '1282370499734110208']\n",
      "\n",
      "Group 9 (Size: 3):\n",
      "  Text preview: charge the cops who shot jacob blake - sign the pe...\n",
      "  IDs: ['1297805546192424960', '1298105853896093698', '1298741664878653440']\n",
      "\n",
      "Group 10 (Size: 3):\n",
      "  Text preview: the latest the internet of things daily! {{url}} t...\n",
      "  IDs: ['1229072380984774656', '1267032826135920642', '1300009424258576384']\n",
      "\n",
      "Group 11 (Size: 3):\n",
      "  Text preview: shining through the city with a little funk and so...\n",
      "  IDs: ['1297436734355943424', '1297472447508369408', '1297624582891896833']\n",
      "\n",
      "Group 12 (Size: 2):\n",
      "  Text preview: the fire tones -  do they know it s christmas? (fe...\n",
      "  IDs: ['1189933701028814849', '1190506741907238913']\n",
      "\n",
      "Group 13 (Size: 2):\n",
      "  Text preview: the latest boston news and more on the fly! {{url}...\n",
      "  IDs: ['1261957668928925696', '1262138822143459328']\n",
      "\n",
      "Group 14 (Size: 2):\n",
      "  Text preview: report: redskins hiring jennifer king as assistant...\n",
      "  IDs: ['1226352500871254022', '1226561578885099520']\n",
      "\n",
      "Group 15 (Size: 2):\n",
      "  Text preview: ufc 244: daniel cormier, sports world react to nat...\n",
      "  IDs: ['1190869388502523904', '1190870407013961733']\n",
      "\n",
      "Group 16 (Size: 2):\n",
      "  Text preview: uh {{username}} , pence said he was looking for yo...\n",
      "  IDs: ['1208718570353561600', '1208839452967264257']\n",
      "\n",
      "Group 17 (Size: 2):\n",
      "  Text preview: so watch me bring the fire and set the night aligh...\n",
      "  IDs: ['1311815674700861440', '1311999304165253121']\n",
      "\n",
      "Group 18 (Size: 2):\n",
      "  Text preview: united states trends  follow {{username}} for insi...\n",
      "  IDs: ['1295056945586614273', '1325376772859453440']\n",
      "\n",
      "Group 19 (Size: 2):\n",
      "  Text preview: so far the only survivors are the wicket-keepers f...\n",
      "  IDs: ['1360889659446861826', '1360890292157640709']\n",
      "\n",
      "Group 20 (Size: 2):\n",
      "  Text preview: scott morrison says greta thunberg s fiery un spee...\n",
      "  IDs: ['1176707066318196739', '1177008780447371264']\n",
      "\n",
      "Group 21 (Size: 2):\n",
      "  Text preview: russ trolls brady : russell wilson uses custom  ha...\n",
      "  IDs: ['1264615457224605697', '1264616729780588544']\n",
      "\n",
      "Group 22 (Size: 2):\n",
      "  Text preview: request {@arashi@} {@mtv@} #fridaylivestream   o｡｡...\n",
      "  IDs: ['1364604506248343553', '1364665468443795457']\n",
      "\n",
      "Group 23 (Size: 2):\n",
      "  Text preview: #nowplaying the best of 2019 the mixtape (mixed by...\n",
      "  IDs: ['1246920060624687106', '1246920086797115403']\n",
      "\n",
      "Group 24 (Size: 2):\n",
      "  Text preview: icymi: today’s  coast to coast  on {@sportsgrid@} ...\n",
      "  IDs: ['1238293797290852352', '1238636851730276352']\n",
      "\n",
      "Group 25 (Size: 2):\n",
      "  Text preview: kyle rittenhouse gunned down three black lives mat...\n",
      "  IDs: ['1300070037756293123', '1300130570702184448']\n",
      "\n",
      "Group 26 (Size: 2):\n",
      "  Text preview: jerry stiller’s greatest moments on ‘seinfeld’ {{u...\n",
      "  IDs: ['1262043754661371905', '1262047297065512961']\n",
      "\n",
      "Group 27 (Size: 2):\n",
      "  Text preview: jennifer lopez & live nation partner in multi-year...\n",
      "  IDs: ['1226425033993707521', '1226457199607574528']\n",
      "\n",
      "Group 28 (Size: 2):\n",
      "  Text preview: #pastorkumuyisaid    everything god has ordained f...\n",
      "  IDs: ['1254347124797517824', '1254348414634078208']\n",
      "\n",
      "Group 29 (Size: 2):\n",
      "  Text preview: harry styles - watermelon sugar (official audio) {...\n",
      "  IDs: ['1196351926780100608', '1197027442587766784']\n",
      "\n",
      "Group 30 (Size: 2):\n",
      "  Text preview: grassley chides trump, fox news for answer on seco...\n",
      "  IDs: ['1277359771507494912', '1277390646588948483']\n",
      "\n",
      "Group 31 (Size: 2):\n",
      "  Text preview: fox news host tucker carlson said the  hysteria” o...\n",
      "  IDs: ['1278869101541122048', '1278901124918108160']\n",
      "\n",
      "Group 32 (Size: 2):\n",
      "  Text preview: check out my classroom on {@donorschoose@} ! i d l...\n",
      "  IDs: ['1274181388061814784', '1291975985760546816']\n",
      "\n",
      "Group 33 (Size: 2):\n",
      "  Text preview: boris johnson: more protection for black trans wom...\n",
      "  IDs: ['1269448554319360002', '1270322787383681024']\n",
      "\n",
      "Group 34 (Size: 2):\n",
      "  Text preview: articles on nosql best practices from the internet...\n",
      "  IDs: ['1221339792740319233', '1241633409795076097']\n",
      "\n",
      "Group 35 (Size: 2):\n",
      "  Text preview: action not words is what is required from the worl...\n",
      "  IDs: ['1185888299220643840', '1185946858767540225']\n",
      "\n",
      "Group 36 (Size: 2):\n",
      "  Text preview: a year ago you helped us with the unveiling of the...\n",
      "  IDs: ['1178308278889127936', '1178368839328948224']\n",
      "\n",
      "Group 37 (Size: 2):\n",
      "  Text preview: 2019 winter break basketball camp starts tom morni...\n",
      "  IDs: ['1208745261541007360', '1208747321455976448']\n",
      "\n",
      "Group 38 (Size: 2):\n",
      "  Text preview: woman charged after taking maga hat from boy outsi...\n",
      "  IDs: ['1297378611197083649', '1297441805860888576']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for exact text duplicates\n",
    "duplicate_texts = df['text'].duplicated().sum()\n",
    "print(f\"Exact duplicate tweets: {duplicate_texts} ({duplicate_texts / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = df['id'].duplicated().sum()\n",
    "print(f\"Duplicate IDs: {duplicate_ids}\")\n",
    "\n",
    "# Show sample duplicates if any exist\n",
    "if duplicate_texts > 0:\n",
    "    print(\"\\nSample Duplicate Tweets:\")\n",
    "    duplicated_mask = df['text'].duplicated(keep=False)\n",
    "    sample_duplicates = df[duplicated_mask].groupby('text').head(2)\n",
    "    print(sample_duplicates[['text', 'date', 'label_name']].head(6).to_string())\n",
    "else:\n",
    "    print(\"\\n✓ No exact duplicate tweets found!\")\n",
    "\n",
    "# Near-duplicate detection (same first 50 characters)\n",
    "df['text_start'] = df['text'].str.lower().str.strip().str[:50]\n",
    "potential_near_dupes = df['text_start'].duplicated().sum()\n",
    "print(f\"\\nPotential near-duplicates (same first 50 chars): {potential_near_dupes} ({potential_near_dupes / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# List all near-duplicate IDs grouped\n",
    "if potential_near_dupes > 0:\n",
    "    print(\"\\nAll Near-Duplicate Groups (sorted by group size):\")\n",
    "    near_dupe_mask = df['text_start'].duplicated(keep=False)\n",
    "    near_dupe_df = df[near_dupe_mask][['text_start', 'id', 'text']]\n",
    "    \n",
    "    # Group by text_start and collect IDs\n",
    "    grouped = near_dupe_df.groupby('text_start')\n",
    "    \n",
    "    # Sort groups by size (largest first)\n",
    "    group_sizes = grouped.size().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal near-duplicate groups: {len(group_sizes)}\")\n",
    "    print(f\"Largest group size: {group_sizes.max()}\\n\")\n",
    "    \n",
    "    for i, (text_start, size) in enumerate(group_sizes.items(), 1):\n",
    "        group_ids = grouped.get_group(text_start)['id'].tolist()\n",
    "        print(f\"Group {i} (Size: {size}):\")\n",
    "        print(f\"  Text preview: {text_start}...\")\n",
    "        print(f\"  IDs: {group_ids}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n✓ No potential near-duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Text Length and Composition Statistics\n",
    "\n",
    "Analyzing text characteristics using pandas string methods and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEXT LENGTH AND COMPOSITION STATISTICS\n",
      "==================================================\n",
      "\n",
      "Character Length Statistics:\n",
      "  Mean: 170.2 characters\n",
      "  Median: 161.5 characters\n",
      "  Std Dev: 68.8 characters\n",
      "  Min: 38 characters\n",
      "  Max: 364 characters\n",
      "\n",
      "Word Count Statistics:\n",
      "  Mean: 28.2 words\n",
      "  Median: 26.0 words\n",
      "  Std Dev: 12.3 words\n",
      "  Min: 6 words\n",
      "  Max: 62 words\n",
      "\n",
      "Average Word Length:\n",
      "  Mean: 5.17 characters per word\n",
      "  Median: 5.03 characters per word\n",
      "\n",
      "Special Character Statistics:\n",
      "  Tweets with hashtags: 2408 (39.5%)\n",
      "  Average hashtags per tweet: 0.87\n",
      "  Tweets with mentions: 4235 (69.5%)\n",
      "  Average mentions per tweet: 1.87\n",
      "  Tweets with URLs: 0 (0.0%)\n",
      "\n",
      "Extreme Examples:\n",
      "\n",
      "Shortest tweet (38 chars):\n",
      "  an old pic huh okay king  {{USERNAME}}\n",
      "\n",
      "Longest tweet (364 chars, first 150):\n",
      "  Oh yeah that s {{USERNAME}} I #asked Jo back in {{USERNAME}} which is many #yrs {{USERNAME}} #now was he {{USERNAME}} #too {{USERNAME}} and he #replie...\n"
     ]
    }
   ],
   "source": [
    "# Calculate text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "df['avg_word_length'] = df['text'].str.replace(' ', '').str.len() / df['word_count']\n",
    "\n",
    "# Count special characters\n",
    "df['hashtag_count'] = df['text'].str.count('#')\n",
    "df['mention_count'] = df['text'].str.count('@')\n",
    "df['url_count'] = df['text'].str.count('http')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT LENGTH AND COMPOSITION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nCharacter Length Statistics:\")\n",
    "print(f\"  Mean: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {df['text_length'].median():.1f} characters\")\n",
    "print(f\"  Std Dev: {df['text_length'].std():.1f} characters\")\n",
    "print(f\"  Min: {df['text_length'].min()} characters\")\n",
    "print(f\"  Max: {df['text_length'].max()} characters\")\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(f\"  Mean: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"  Median: {df['word_count'].median():.1f} words\")\n",
    "print(f\"  Std Dev: {df['word_count'].std():.1f} words\")\n",
    "print(f\"  Min: {df['word_count'].min()} words\")\n",
    "print(f\"  Max: {df['word_count'].max()} words\")\n",
    "\n",
    "print(\"\\nAverage Word Length:\")\n",
    "print(f\"  Mean: {df['avg_word_length'].mean():.2f} characters per word\")\n",
    "print(f\"  Median: {df['avg_word_length'].median():.2f} characters per word\")\n",
    "\n",
    "print(\"\\nSpecial Character Statistics:\")\n",
    "print(f\"  Tweets with hashtags: {(df['hashtag_count'] > 0).sum()} ({(df['hashtag_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average hashtags per tweet: {df['hashtag_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with mentions: {(df['mention_count'] > 0).sum()} ({(df['mention_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average mentions per tweet: {df['mention_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with URLs: {(df['url_count'] > 0).sum()} ({(df['url_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show extreme examples\n",
    "print(\"\\nExtreme Examples:\")\n",
    "print(f\"\\nShortest tweet ({df['text_length'].min()} chars):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmin(), 'text']}\")\n",
    "print(f\"\\nLongest tweet ({df['text_length'].max()} chars, first 150):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmax(), 'text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Word Frequency Statistics (NLTK)\n",
    "\n",
    "Analyzing word frequency patterns using NLTK tokenization and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WORD FREQUENCY STATISTICS (NLTK)\n",
      "==================================================\n",
      "\n",
      "Vocabulary Statistics:\n",
      "  Total tokens (after filtering): 103,399\n",
      "  Unique words (vocabulary size): 22,205\n",
      "  Average token frequency: 4.66\n",
      "  Words appearing only once: 13,736\n",
      "  Words appearing 10+ times: 1,730\n",
      "\n",
      "Top 30 Most Common Words:\n",
      "    Word  Frequency\n",
      "username       3680\n",
      "     url       2955\n",
      "     via       1098\n",
      "     new        593\n",
      "     ...        432\n",
      "    love        429\n",
      "     day        395\n",
      "    game        346\n",
      "   happy        338\n",
      "   music        331\n",
      "    time        324\n",
      "     one        322\n",
      "    news        317\n",
      "    like        298\n",
      "     get        274\n",
      " youtube        272\n",
      "  please        267\n",
      "   today        264\n",
      "     see        261\n",
      "   video        260\n",
      "   first        258\n",
      "   world        257\n",
      "    live        257\n",
      "   great        250\n",
      "    good        245\n",
      "    year        239\n",
      "    back        222\n",
      "  people        219\n",
      "   watch        209\n",
      "  change        207\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORD FREQUENCY STATISTICS (NLTK)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tokenize all tweets\n",
    "all_tokens = []\n",
    "for text in df['text']:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords, keep only meaningful words\n",
    "    tokens = [\n",
    "        t for t in tokens \n",
    "        if t not in string.punctuation \n",
    "        and t not in stop_words \n",
    "        and len(t) > 2\n",
    "    ]\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "top_words = pd.DataFrame(\n",
    "    token_freq.most_common(30), \n",
    "    columns=['Word', 'Frequency']\n",
    ")\n",
    "\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"  Total tokens (after filtering): {len(all_tokens):,}\")\n",
    "print(f\"  Unique words (vocabulary size): {len(token_freq):,}\")\n",
    "print(f\"  Average token frequency: {len(all_tokens) / len(token_freq):.2f}\")\n",
    "print(f\"  Words appearing only once: {sum(1 for count in token_freq.values() if count == 1):,}\")\n",
    "print(f\"  Words appearing 10+ times: {sum(1 for count in token_freq.values() if count >= 10):,}\")\n",
    "\n",
    "print(f\"\\nTop 30 Most Common Words:\")\n",
    "print(top_words.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.6 Named Entity Statistics (spaCy)\n",
    "\n",
    "Analyzing named entity distributions using spaCy NER and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "NAMED ENTITY STATISTICS (spaCy)\n",
      "==================================================\n",
      "\n",
      "Analyzing 6090 tweets... This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 6090/6090 [00:23<00:00, 262.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Detection Statistics:\n",
      "  Tweets with entities: 5,644 (92.7%)\n",
      "  Tweets without entities: 446 (7.3%)\n",
      "  Total entities found: 17,762\n",
      "  Average entities per tweet: 2.92\n",
      "  Median entities per tweet: 3\n",
      "  Max entities in a tweet: 18\n",
      "\n",
      "Entity Type Distribution:\n",
      "Entity Type  Count  Percentage\n",
      "     PERSON   4394        24.7\n",
      "        ORG   3965        22.3\n",
      "   CARDINAL   2418        13.6\n",
      "       DATE   2126        12.0\n",
      "        GPE   1441         8.1\n",
      "      MONEY    892         5.0\n",
      "       TIME    588         3.3\n",
      "       NORP    462         2.6\n",
      "    ORDINAL    395         2.2\n",
      "WORK_OF_ART    265         1.5\n",
      "      EVENT    199         1.1\n",
      "    PRODUCT    182         1.0\n",
      "        LOC    158         0.9\n",
      "        FAC    125         0.7\n",
      "   QUANTITY     53         0.3\n",
      "        LAW     47         0.3\n",
      "    PERCENT     45         0.3\n",
      "   LANGUAGE      7         0.0\n",
      "\n",
      "Entity Type Legend:\n",
      "\n",
      "Entity Type Legend (Complete):\n",
      "  PERSON      = People, including fictional characters\n",
      "  ORG         = Companies, agencies, institutions, organizations\n",
      "  GPE         = Countries, cities, states (Geo-Political Entities)\n",
      "  DATE        = Absolute or relative dates or periods\n",
      "  CARDINAL    = Numerals that do not fall under another type\n",
      "  MONEY       = Monetary values, including unit\n",
      "  TIME        = Times smaller than a day\n",
      "  NORP        = Nationalities, religious or political groups\n",
      "  ORDINAL     = First, second, third, etc.\n",
      "  WORK_OF_ART = Titles of books, songs, movies, etc.\n",
      "  EVENT       = Named hurricanes, battles, wars, sports events\n",
      "  PRODUCT     = Objects, vehicles, foods, etc. (not services)\n",
      "  LOC         = Non-GPE locations, mountain ranges, bodies of water\n",
      "  FAC         = Buildings, airports, highways, bridges, etc.\n",
      "  QUANTITY    = Measurements, as of weight or distance\n",
      "  LAW         = Named documents made into laws\n",
      "  PERCENT     = Percentage, including '%'\n",
      "  LANGUAGE    = Any named language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NAMED ENTITY STATISTICS (spaCy)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAnalyzing {len(df)} tweets... This may take a few minutes...\")\n",
    "\n",
    "# Extract entities from all tweets\n",
    "all_entities = []\n",
    "tweets_with_entities = 0\n",
    "entity_counts_per_tweet = []\n",
    "\n",
    "# Use nlp.pipe for better performance\n",
    "texts = df['text'].apply(lambda x: x[:500]).tolist()\n",
    "\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=50), total=len(df), desc=\"Processing\"):\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    entity_counts_per_tweet.append(len(entities))\n",
    "    if entities:\n",
    "        tweets_with_entities += 1\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "# Add entity count to dataframe\n",
    "df['entity_count'] = entity_counts_per_tweet\n",
    "\n",
    "print(f\"\\nEntity Detection Statistics:\")\n",
    "print(f\"  Tweets with entities: {tweets_with_entities:,} ({tweets_with_entities/len(df)*100:.1f}%)\")\n",
    "print(f\"  Tweets without entities: {len(df) - tweets_with_entities:,} ({(len(df) - tweets_with_entities)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total entities found: {len(all_entities):,}\")\n",
    "print(f\"  Average entities per tweet: {df['entity_count'].mean():.2f}\")\n",
    "print(f\"  Median entities per tweet: {df['entity_count'].median():.0f}\")\n",
    "print(f\"  Max entities in a tweet: {df['entity_count'].max()}\")\n",
    "\n",
    "# Count entity types\n",
    "entity_types = Counter([label for text, label in all_entities])\n",
    "entity_type_df = pd.DataFrame(\n",
    "    entity_types.most_common(), \n",
    "    columns=['Entity Type', 'Count']\n",
    ")\n",
    "entity_type_df['Percentage'] = (entity_type_df['Count'] / len(all_entities) * 100).round(1)\n",
    "\n",
    "print(\"\\nEntity Type Distribution:\")\n",
    "print(entity_type_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nEntity Type Legend:\")\n",
    "print(\"\\nEntity Type Legend (Complete):\")\n",
    "print(\"  PERSON      = People, including fictional characters\")\n",
    "print(\"  ORG         = Companies, agencies, institutions, organizations\")\n",
    "print(\"  GPE         = Countries, cities, states (Geo-Political Entities)\")\n",
    "print(\"  DATE        = Absolute or relative dates or periods\")\n",
    "print(\"  CARDINAL    = Numerals that do not fall under another type\")\n",
    "print(\"  MONEY       = Monetary values, including unit\")\n",
    "print(\"  TIME        = Times smaller than a day\")\n",
    "print(\"  NORP        = Nationalities, religious or political groups\")\n",
    "print(\"  ORDINAL     = First, second, third, etc.\")\n",
    "print(\"  WORK_OF_ART = Titles of books, songs, movies, etc.\")\n",
    "print(\"  EVENT       = Named hurricanes, battles, wars, sports events\")\n",
    "print(\"  PRODUCT     = Objects, vehicles, foods, etc. (not services)\")\n",
    "print(\"  LOC         = Non-GPE locations, mountain ranges, bodies of water\")\n",
    "print(\"  FAC         = Buildings, airports, highways, bridges, etc.\")\n",
    "print(\"  QUANTITY    = Measurements, as of weight or distance\")\n",
    "print(\"  LAW         = Named documents made into laws\")\n",
    "print(\"  PERCENT     = Percentage, including '%'\")\n",
    "print(\"  LANGUAGE    = Any named language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Filter to Text Only\n",
    "\n",
    "Filtering the dataset to extract only text columns and remove any non-textual data. This step ensures that subsequent processing focuses exclusively on textual content and handles any data type conversions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FILTERING TO TEXT ONLY\n",
      "==================================================\n",
      "\n",
      "Original Dataset:\n",
      "  Shape: (6090, 19)\n",
      "  Columns: ['text', 'date', 'label', 'label_name', 'id', 'topics_list', 'num_topics', 'date_parsed', 'year', 'month', 'day_of_week', 'text_start', 'text_length', 'word_count', 'avg_word_length', 'hashtag_count', 'mention_count', 'url_count', 'entity_count']\n",
      "  Sample text: The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} 4-0 in Game 1 of the Western Division ...\n",
      "\n",
      "Filtered Dataset (Text Only - No Numbers):\n",
      "  Shape: (6090, 2)\n",
      "  Columns: ['text', 'label_name']\n",
      "  Sample text: The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} - in Game of the Western Division fina...\n",
      "\n",
      "Examples of Number Removal:\n",
      "\n",
      "Example 1:\n",
      "  Original:  The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} 4-0 in Game 1 of t...\n",
      "  Filtered:  The {@Clinton LumberKings@} beat the {@Cedar Rapids Kernels@} - in Game of the W...\n",
      "\n",
      "Example 2:\n",
      "  Original:  I would rather hear Eli Gold announce this Auburn game than these dumbasses. {@E...\n",
      "  Filtered:  I would rather hear Eli Gold announce this Auburn game than these dumbasses. {@E...\n",
      "\n",
      "Example 3:\n",
      "  Original:  Someone take my phone away, I’m trying to not look at {@Chicago Blackhawks@} hom...\n",
      "  Filtered:  Someone take my phone away, I’m trying to not look at {@Chicago Blackhawks@} hom...\n",
      "\n",
      "Label Name Comparison:\n",
      "  Original label_name: ['sports']\n",
      "  Filtered label_name: ['sports']\n",
      "\n",
      "Character Statistics:\n",
      "  Original total characters: 1,036,684\n",
      "  Filtered total characters: 1,019,913\n",
      "  Characters removed (numbers): 16,771 (1.62%)\n",
      "\n",
      "✓ Dataset successfully filtered to text only (numbers removed)\n"
     ]
    }
   ],
   "source": [
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name'):\n",
    "    \"\"\"\n",
    "    Filter dataset to only text and label columns, removing all numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe to filter\n",
    "    text_col : str\n",
    "        Name of the text column (default: 'text')\n",
    "    label_col : str\n",
    "        Name of the label column (default: 'label_name')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Filtered dataframe with only text columns and no numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select only the text and label_name columns\n",
    "    df_filtered = dataframe[[text_col, label_col]].copy()\n",
    "    \n",
    "    # Step 2: Remove all numbers from the text column using regex\n",
    "    # This removes all digits (0-9) from the text\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    # Step 3: Handle label_name - convert to string if it's a list, then remove numbers\n",
    "    # First check if it's already a list or needs conversion\n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        # Keep as list, no number removal needed (labels are text)\n",
    "        pass\n",
    "    else:\n",
    "        # If it's a string representation, convert and clean\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    # Step 4: Clean up any extra whitespace created by removing numbers\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Apply the filtering function\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILTERING TO TEXT ONLY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show original dataset info\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Apply the filter\n",
    "df_text_only = filter_to_text_only(df)\n",
    "\n",
    "# Show filtered dataset info\n",
    "print(\"\\nFiltered Dataset (Text Only - No Numbers):\")\n",
    "print(f\"  Shape: {df_text_only.shape}\")\n",
    "print(f\"  Columns: {df_text_only.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df_text_only['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Show examples of number removal\n",
    "print(\"\\nExamples of Number Removal:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original:  {df['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Filtered:  {df_text_only['text'].iloc[i][:80]}...\")\n",
    "\n",
    "# Show label_name comparison\n",
    "print(\"\\nLabel Name Comparison:\")\n",
    "print(f\"  Original label_name: {df['label_name'].iloc[0]}\")\n",
    "print(f\"  Filtered label_name: {df_text_only['label_name'].iloc[0]}\")\n",
    "\n",
    "# Statistics on number removal\n",
    "original_chars = df['text'].str.len().sum()\n",
    "filtered_chars = df_text_only['text'].str.len().sum()\n",
    "chars_removed = original_chars - filtered_chars\n",
    "\n",
    "print(f\"\\nCharacter Statistics:\")\n",
    "print(f\"  Original total characters: {original_chars:,}\")\n",
    "print(f\"  Filtered total characters: {filtered_chars:,}\")\n",
    "print(f\"  Characters removed (numbers): {chars_removed:,} ({chars_removed/original_chars*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Dataset successfully filtered to text only (numbers removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Reusable Loading and Filtering Functions\n",
    "\n",
    "Reusable functions that encapsulate the data loading and text-filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset successfully loaded and filtered to text only\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "tweets_raw = dataset.to_pandas()\n",
    "\n",
    "# Filter the dataset to text only (no numbers)\n",
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name', label_num_col='label'):\n",
    "    df_filtered = dataframe[[text_col, label_col, label_num_col]].copy()\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        pass\n",
    "    else:\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "tweets_text_only = filter_to_text_only(df)\n",
    "\n",
    "print(\"\\n✓ Dataset successfully loaded and filtered to text only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "This section focuses on text preprocessing techniques. We will review common preprocessing methods, apply them systematically using NLTK and SpaCy, analyze how the order of operations affects results, evaluate the usefulness of each method for different scenarios, and create a reusable preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Methods Used (in Order)\n",
    "\n",
    "#### 1. Remove RT, URLs, and Mentions\n",
    "- Removes retweet indicators, web links, and user mentions\n",
    "- **Why:** Don't contribute to topic classification\n",
    "- **Order:** Before tokenization to remove as complete units\n",
    "\n",
    "#### 2. Extract Hashtag Text\n",
    "- Converts `#Gaming` → `gaming`\n",
    "- **Why:** Hashtags contain topic keywords - critical for classification!\n",
    "- **Order:** Before tokenization, preserves topic information\n",
    "\n",
    "#### 3. Normalize & Lowercase\n",
    "- Removes extra spaces and converts to lowercase\n",
    "- **Why:** \"Sports\" = \"sports\", reduces vocabulary by ~30-40%\n",
    "- **Order:** Before tokenization for consistent token matching\n",
    "\n",
    "#### 4. Tokenization (SpaCy)\n",
    "- Breaks text into individual tokens/words\n",
    "- **Why:** Foundation for all token-based processing\n",
    "- **Order:** After text cleaning, before token filtering\n",
    "\n",
    "#### 5. Remove Punctuation\n",
    "- Filters out punctuation tokens\n",
    "- **Why:** Don't contribute semantic meaning for topics\n",
    "- **Order:** After tokenization using `token.is_punct`\n",
    "\n",
    "#### 6. Filter Non-Alphabetic & Short Tokens\n",
    "- Removes non-alphabetic tokens (emojis, special chars) and tokens < 2 characters\n",
    "- **Why:** Reduces noise from social media content\n",
    "- **Order:** After tokenization, before stopword removal\n",
    "\n",
    "#### 7. Custom Stopword Removal (SpaCy)\n",
    "- Removes common words BUT keeps topic-relevant words (game, music, news, sport, film, video, watch, play)\n",
    "- **Why:** Standard stopword lists would remove topic indicators!\n",
    "- **Order:** After lowercase, before lemmatization (more efficient)\n",
    "\n",
    "#### 8. Latin Alphabet Check\n",
    "- Keeps only words with Latin letters (a-z)\n",
    "- **Why:** Removes non-English scripts while keeping modern social media terms (videogames, esports)\n",
    "- **Order:** After alphabetic filtering, before lemmatization\n",
    "\n",
    "#### 9. Lemmatization (SpaCy)\n",
    "- Converts to base form: running → run, games → game\n",
    "- **Why:** Groups related word forms, better than stemming for preserving meaning\n",
    "- **Order:** LAST - operates on clean, filtered tokens\n",
    "\n",
    "### Why This Order Matters\n",
    "\n",
    "**Critical Dependencies:**\n",
    "- Special tokens **before** tokenization → removes as complete units\n",
    "- Lowercase **before** tokenization → consistent stopword matching\n",
    "- Tokenization **before** filtering → need tokens to filter\n",
    "- Lemmatization **last** → final transformation on clean tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Reusable Preprocessing Pipeline\n",
    "\n",
    "Development of a modular, configurable preprocessing function that can be easily reused in future labs. The pipeline allows for flexible selection of preprocessing steps and parameters, making it adaptable to different text analysis tasks and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SpaCy model loaded successfully\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "✓ Processed 6090 tweets\n",
      "✓ Original 'tweets_text_only' unchanged | Processed data in 'tweets_preprocessed_train'\n",
      "\n",
      "✓ DataFrame for training saved under path: ../Data/tweets_preprocessed_train.parquet\n",
      "✓ Features: ['text', 'label_name', 'label']\n",
      "✓ Shape: (6090, 3)\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Load SpaCy model\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"✓ SpaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"Installing SpaCy model...\")\n",
    "    import os\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"✓ SpaCy model loaded successfully\")\n",
    "\n",
    "def is_latin_alphabet(word):\n",
    "    \"\"\"\n",
    "    Check if a word contains only Latin alphabet characters.\n",
    "    Filters out words with Cyrillic, Arabic, Chinese, etc.\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return False\n",
    "    return all(ord('a') <= ord(c.lower()) <= ord('z') for c in word)\n",
    "\n",
    "def segment_camelcase(text):\n",
    "    \"\"\"\n",
    "    Segmentiert CamelCase-Wörter in separate Wörter ohne Regex.\n",
    "    Beispiel: 'GameOfThrones' → 'Game Of Thrones'\n",
    "    Dies ist wichtig für Hashtags wie #GameOfThrones nach Entfernung des #\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        # Füge aktuellen Character hinzu\n",
    "        result.append(char)\n",
    "        \n",
    "        # Prüfe, ob wir ein Leerzeichen einfügen müssen\n",
    "        if i < len(text) - 1:\n",
    "            current = char\n",
    "            next_char = text[i + 1]\n",
    "            \n",
    "            # Fall 1: lowercase → uppercase (z.B. 'e' → 'O' in 'GameOf')\n",
    "            if current.islower() and next_char.isupper():\n",
    "                result.append(' ')\n",
    "            \n",
    "            # Fall 2: uppercase → uppercase → lowercase (z.B. 'HTML' → 'Parser')\n",
    "            elif i < len(text) - 2:\n",
    "                after_next = text[i + 2]\n",
    "                if current.isupper() and next_char.isupper() and after_next.islower():\n",
    "                    result.append(' ')\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"\n",
    "    Topic-optimized preprocessing for tweet classification.\n",
    "    Preserves topic-relevant information while removing noise.\n",
    "    Removes special characters, emojis, and non-Latin script words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Remove RT (retweet indicator)\n",
    "    text = text.replace('RT ', ' ').replace('rt ', ' ')\n",
    "    \n",
    "    # Step 2: Remove URLs and placeholders\n",
    "    text = text.replace('{{URL}}', ' ')\n",
    "    text = text.replace('{{USERNAME}}', ' ')\n",
    "    for protocol in ['https://', 'http://', 'www.']:\n",
    "        if protocol in text:\n",
    "            parts = text.split(protocol)\n",
    "            text = parts[0] + ' ' + ' '.join([' '.join(p.split()[1:]) if p.split() else '' for p in parts[1:]])\n",
    "    \n",
    "    # Step 3: Remove mentions\n",
    "    words_list = text.split()\n",
    "    words_list = [w for w in words_list if not (w.startswith('{@') or w.startswith('@'))]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 4: Extract hashtag text (#Gaming → Gaming, #GameOfThrones → GameOfThrones)\n",
    "    words_list = text.split()\n",
    "    words_list = [w[1:] if w.startswith('#') else w for w in words_list]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 4.5: Segment CamelCase words (WICHTIG: VOR dem Lowercase!)\n",
    "    # GameOfThrones → Game Of Thrones\n",
    "    text = segment_camelcase(text)\n",
    "    \n",
    "    # Step 5: Normalize whitespace and lowercase\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 6: Tokenize with SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 7: Filter and lemmatize tokens\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        # Skip punctuation\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        \n",
    "        # Skip if not alphabetic (removes special characters, emojis, numbers)\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "        \n",
    "        # Skip tokens shorter than 2 characters\n",
    "        if len(token.text) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Remove stopwords (using SpaCy's stopword detection)\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Check if word uses Latin alphabet (filters out Cyrillic, Arabic, Chinese, etc.)\n",
    "        if not is_latin_alphabet(token.text):\n",
    "            continue\n",
    "        \n",
    "        # Use lemmatized form\n",
    "        processed_tokens.append(token.lemma_)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "tweets_preprocessed_train = tweets_text_only.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "tweets_preprocessed_train['text'] = tweets_preprocessed_train['text'].apply(preprocess_tweet)\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"✓ Processed {len(tweets_preprocessed_train)} tweets\")\n",
    "print(f\"✓ Original 'tweets_text_only' unchanged | Processed data in 'tweets_preprocessed_train'\")\n",
    "\n",
    "# Speichere den DataFrame im Data Ordner\n",
    "import os\n",
    "\n",
    "# Erstelle Data Ordner falls nicht vorhanden\n",
    "os.makedirs('../Data', exist_ok=True)\n",
    "\n",
    "# Speichere ab\n",
    "output_path = '../Data/tweets_preprocessed_train.parquet'\n",
    "tweets_preprocessed_train.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ DataFrame for training saved under path: {output_path}\")\n",
    "print(f\"✓ Features: {list(tweets_preprocessed_train.columns)}\")\n",
    "print(f\"✓ Shape: {tweets_preprocessed_train.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
