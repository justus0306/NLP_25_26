{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: CNN Text Classification with Word2Vec Embeddings\n",
    "\n",
    "---\n",
    "## 1. Notebook Overview\n",
    "\n",
    "### 1.1 Objective\n",
    "Replace the classification model from Lab 5 (MLPClassifier with binary feature vectors) with a **Convolutional Neural Network (CNN)** that handles input matrices of word embeddings instead of mean vectors.\n",
    "\n",
    "### 1.2 Key Changes from Lab 5\n",
    "- **Lab 5**: Used binary BoW vectors (1000 features) with MLPClassifier\n",
    "- **Lab 8**: Uses word embedding matrices with 1D CNN\n",
    "\n",
    "### 1.3 Prerequisites\n",
    "This notebook assumes you have already executed:\n",
    "- **Lab 2**: Data preprocessing → `../Data/multi_label/tweets_preprocessed_*.parquet`\n",
    "- **Lab 4**: Feature extraction → `../Data/top_1000_vocabulary.json`\n",
    "- **Lab 5**: Neural network classification (for comparison)\n",
    "\n",
    "### 1.4 Architecture\n",
    "We implement a 1D CNN with:\n",
    "- **Input**: Embedding matrices of shape (seq_len, emb_dim) where emb_dim=300 (Word2Vec)\n",
    "- **Conv1D Layer**: 100 filters with kernel size 3\n",
    "- **Pooling**: Global Max Pooling\n",
    "- **Output**: Multi-label classification with 6 classes using Sigmoid activation\n",
    "\n",
    "### 1.5 Section Overview\n",
    "1. **Section 2** – Load and prepare data\n",
    "2. **Section 3** – Load Word2Vec model\n",
    "3. **Section 4** – Create embedding matrices\n",
    "4. **Section 5** – Define and train CNN model\n",
    "5. **Section 6** – Evaluation and comparison with Lab 5\n",
    "6. **Section 7** – Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup and Data Loading\n",
    "\n",
    "### 2.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install gensim torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    hamming_loss\n",
    ")\n",
    "\n",
    "# Constants\n",
    "TRAIN_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_train.parquet\"\n",
    "TEST_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_test.parquet\"\n",
    "VALIDATION_DATA_PATH = \"../Data/multi_label/tweets_preprocessed_validation.parquet\"\n",
    "VOCABULARY_PATH = \"../Data/top_1000_vocabulary.json\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load Preprocessed Datasets from Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(value) -> List[str]:\n",
    "    \"\"\"Parse label_name column into consistent Python lists.\"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, tuple):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.startswith('[') and value.endswith(']'):\n",
    "            inner = value[1:-1].strip()\n",
    "            if not inner:\n",
    "                return []\n",
    "            inner = inner.replace(\"'\", \"\").replace('\"', '')\n",
    "            labels = [l.strip() for l in inner.split() if l.strip()]\n",
    "            return labels\n",
    "        try:\n",
    "            parsed = ast.literal_eval(value)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(v) for v in parsed]\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "        return [value] if value else []\n",
    "    return [str(value)] if value else []\n",
    "\n",
    "def parse_binary_label(value) -> np.ndarray:\n",
    "    \"\"\"Parse binary label array from string representation.\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        inner = value.strip()[1:-1]\n",
    "        return np.array([int(x) for x in inner.split()])\n",
    "    return np.array(value)\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalize the label columns.\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_binary\"] = df[\"label\"].apply(parse_binary_label)\n",
    "    return df\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading preprocessed datasets from Lab 2...\")\n",
    "df_train = load_dataset(TRAIN_DATA_PATH)\n",
    "df_test = load_dataset(TEST_DATA_PATH)\n",
    "df_validation = load_dataset(VALIDATION_DATA_PATH)\n",
    "\n",
    "print(f\"✓ Training set: {len(df_train):,} samples\")\n",
    "print(f\"✓ Test set: {len(df_test):,} samples\")\n",
    "print(f\"✓ Validation set: {len(df_validation):,} samples\")\n",
    "print(f\"\\nSample preprocessed text:\")\n",
    "print(f\"  '{df_train['text'].iloc[0][:80]}...'\")\n",
    "print(f\"  Labels: {df_train['labels'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extract Class Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically determine the number of classes from the data\n",
    "NUM_CLASSES = len(df_train['label_binary'].iloc[0])\n",
    "\n",
    "# Extract all unique class names\n",
    "all_class_names = set()\n",
    "for df in [df_train, df_test, df_validation]:\n",
    "    for labels in df['labels']:\n",
    "        all_class_names.update(labels)\n",
    "\n",
    "TOPIC_CLASSES = sorted(list(all_class_names))\n",
    "\n",
    "print(f\"✓ Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"✓ Classes: {TOPIC_CLASSES}\")\n",
    "\n",
    "# Label statistics\n",
    "y_train_multi = np.vstack(df_train['label_binary'].values)\n",
    "print(f\"\\n✓ Label distribution (Training):\")\n",
    "print(f\"  Average labels per sample: {y_train_multi.sum(axis=1).mean():.2f}\")\n",
    "print(f\"  Samples per class:\")\n",
    "for i, class_name in enumerate(TOPIC_CLASSES):\n",
    "    count = y_train_multi[:, i].sum()\n",
    "    print(f\"    {class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Word2Vec Model\n",
    "\n",
    "We use the pre-trained Google News Word2Vec model (300-dimensional embeddings) to convert words into dense vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec model (GoogleNews-vectors-negative300)...\")\n",
    "print(\"This may take a few minutes on first run as the model is downloaded.\")\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "EMBEDDING_DIM = w2v.vector_size\n",
    "print(f\"\\n✓ Word2Vec model loaded successfully\")\n",
    "print(f\"✓ Vocabulary size: {len(w2v):,} words\")\n",
    "print(f\"✓ Embedding dimension: {EMBEDDING_DIM}\")\n",
    "\n",
    "# Test the embeddings\n",
    "test_word = \"happy\"\n",
    "if test_word in w2v:\n",
    "    print(f\"\\n✓ Example: '{test_word}' vector shape: {w2v[test_word].shape}\")\n",
    "    print(f\"  First 5 dimensions: {w2v[test_word][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Embedding Matrices\n",
    "\n",
    "### 4.1 Document Embedding Function\n",
    "\n",
    "Instead of using mean vectors (as in simpler approaches), we create a **matrix of embeddings** for each document. Each row in the matrix represents a word's embedding vector. This preserves sequential information that CNNs can exploit.\n",
    "\n",
    "**Key difference from Lab 5:**\n",
    "- Lab 5: Binary BoW vectors (1000 dimensions, loses word semantics)\n",
    "- Lab 8: Embedding matrices (seq_len × 300 dimensions, preserves semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for CNN input\n",
    "MAX_SEQ_LEN = 50  # Most tweets are shorter than 50 tokens after preprocessing\n",
    "\n",
    "def embed_document(text: str, max_len: int = MAX_SEQ_LEN) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a document into an embedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Preprocessed text (whitespace-tokenized)\n",
    "    max_len : int\n",
    "        Maximum sequence length (pad/truncate to this length)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Embedding matrix of shape (max_len, EMBEDDING_DIM)\n",
    "    \"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    vecs = []\n",
    "    \n",
    "    for tok in tokens[:max_len]:\n",
    "        if tok in w2v:\n",
    "            vecs.append(w2v[tok])\n",
    "        else:\n",
    "            # Use zero vector for OOV words\n",
    "            vecs.append(np.zeros(EMBEDDING_DIM))\n",
    "    \n",
    "    # Pad sequences shorter than max_len\n",
    "    while len(vecs) < max_len:\n",
    "        vecs.append(np.zeros(EMBEDDING_DIM))\n",
    "    \n",
    "    return np.array(vecs)\n",
    "\n",
    "# Test the function\n",
    "sample_text = df_train['text'].iloc[0]\n",
    "sample_embedding = embed_document(sample_text)\n",
    "print(f\"Sample text: '{sample_text[:60]}...'\")\n",
    "print(f\"Embedding matrix shape: {sample_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Embedding Matrices for All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrices(texts: pd.Series, max_len: int = MAX_SEQ_LEN) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create embedding matrices for all documents in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : pd.Series\n",
    "        Series of preprocessed text strings\n",
    "    max_len : int\n",
    "        Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        3D array of shape (n_samples, max_len, EMBEDDING_DIM)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Creating embeddings\"):\n",
    "        embeddings.append(embed_document(text, max_len))\n",
    "    return np.stack(embeddings)\n",
    "\n",
    "print(\"Creating embedding matrices for all datasets...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "X_train_emb = create_embedding_matrices(df_train['text'])\n",
    "X_test_emb = create_embedding_matrices(df_test['text'])\n",
    "X_val_emb = create_embedding_matrices(df_validation['text'])\n",
    "\n",
    "print(f\"\\n✓ Training embeddings shape: {X_train_emb.shape}\")\n",
    "print(f\"✓ Test embeddings shape: {X_test_emb.shape}\")\n",
    "print(f\"✓ Validation embeddings shape: {X_val_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multi-label targets\n",
    "y_train = np.vstack(df_train['label_binary'].values).astype(np.float32)\n",
    "y_test = np.vstack(df_test['label_binary'].values).astype(np.float32)\n",
    "y_val = np.vstack(df_validation['label_binary'].values).astype(np.float32)\n",
    "\n",
    "print(f\"✓ Label shapes:\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. CNN Model Definition and Training\n",
    "\n",
    "### 5.1 TextCNN Model Architecture\n",
    "\n",
    "The CNN architecture processes embedding matrices through:\n",
    "1. **Conv1D Layer**: Applies 1D convolution over the sequence\n",
    "2. **ReLU Activation**: Non-linear activation\n",
    "3. **Global Max Pooling**: Extracts the most important features\n",
    "4. **Fully Connected Layer**: Maps to output classes\n",
    "\n",
    "For multi-label classification, we use **Sigmoid** activation (not Softmax) and **Binary Cross-Entropy** loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN for text classification using word embeddings.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: (batch, seq_len, emb_dim)\n",
    "    - Conv1D: (batch, emb_dim, seq_len) -> (batch, num_filters, seq_len-kernel_size+1)\n",
    "    - MaxPool: (batch, num_filters, 1) -> (batch, num_filters)\n",
    "    - FC: (batch, num_filters) -> (batch, num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim: int = 300, seq_len: int = 50, \n",
    "                 num_classes: int = 6, num_filters: int = 100, \n",
    "                 kernel_size: int = 3, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1D Convolution over the sequence dimension\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=emb_dim, \n",
    "            out_channels=num_filters, \n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Global Max Pooling\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(num_filters, num_classes)\n",
    "        \n",
    "        # Sigmoid for multi-label classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, emb_dim)\n",
    "        \n",
    "        # Permute for Conv1d: (batch, emb_dim, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Convolution + activation\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Global max pooling: (batch, num_filters, 1) -> (batch, num_filters)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Sigmoid for multi-label probabilities\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Display model architecture\n",
    "print(\"=\"*60)\n",
    "print(\"CNN MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input: Embedding matrix ({MAX_SEQ_LEN}, {EMBEDDING_DIM})\")\n",
    "print(f\"Conv1D: {EMBEDDING_DIM} -> 100 filters, kernel_size=3\")\n",
    "print(f\"Activation: ReLU\")\n",
    "print(f\"Pooling: Global Max Pooling\")\n",
    "print(f\"Dropout: 0.5\")\n",
    "print(f\"Output: {NUM_CLASSES} classes (Sigmoid activation)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_emb, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_emb, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_emb, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✓ DataLoaders created\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = TextCNN(\n",
    "    emb_dim=EMBEDDING_DIM,\n",
    "    seq_len=MAX_SEQ_LEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_filters=100,\n",
    "    kernel_size=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Binary Cross-Entropy for multi-label classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"✓ Model initialized\")\n",
    "print(f\"✓ Optimizer: Adam (lr=1e-3)\")\n",
    "print(f\"✓ Loss function: Binary Cross-Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CNN MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation and Comparison\n",
    "\n",
    "### 6.1 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "THRESHOLD = 0.5  # Classification threshold for multi-label\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        # Apply threshold to get binary predictions\n",
    "        preds = (outputs.cpu().numpy() > THRESHOLD).astype(int)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(batch_y.numpy())\n",
    "\n",
    "y_pred_cnn = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "\n",
    "print(f\"✓ Predictions made on {len(y_pred_cnn)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Calculate CNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "cnn_metrics = {\n",
    "    'Subset Accuracy': accuracy_score(y_true, y_pred_cnn),\n",
    "    'Hamming Loss': hamming_loss(y_true, y_pred_cnn),\n",
    "    'Micro F1': f1_score(y_true, y_pred_cnn, average='micro', zero_division=0),\n",
    "    'Macro F1': f1_score(y_true, y_pred_cnn, average='macro', zero_division=0),\n",
    "    'Micro Precision': precision_score(y_true, y_pred_cnn, average='micro', zero_division=0),\n",
    "    'Micro Recall': recall_score(y_true, y_pred_cnn, average='micro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CNN EVALUATION RESULTS (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in cnn_metrics.items():\n",
    "    print(f\"{metric:<20}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "def labels_to_names(binary_vec):\n",
    "    \"\"\"Convert binary vector to list of class names.\"\"\"\n",
    "    names = [TOPIC_CLASSES[i] for i, v in enumerate(binary_vec) if v == 1]\n",
    "    return tuple(names) if names else ('none',)\n",
    "\n",
    "print(\"\\nSample CNN Predictions:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(5):\n",
    "    text = df_test['text'].iloc[i][:60]\n",
    "    true_labels = labels_to_names(y_true[i])\n",
    "    pred_labels = labels_to_names(y_pred_cnn[i])\n",
    "    match = \"✓\" if set(true_labels) == set(pred_labels) else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{match} Sample {i+1}:\")\n",
    "    print(f\"   Text: {text}...\")\n",
    "    print(f\"   True: {true_labels}\")\n",
    "    print(f\"   Pred: {pred_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Comparison with Lab 5 Results\n",
    "\n",
    "We compare our CNN results with the Neural Network (MLPClassifier) results from Lab 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 5 Multi-Label Neural Network results (from lab5.ipynb output)\n",
    "# These are the results we compare against\n",
    "lab5_nn_metrics = {\n",
    "    'Subset Accuracy': 0.4527,\n",
    "    'Hamming Loss': 0.1442,\n",
    "    'Micro F1': 0.6447,\n",
    "    'Macro F1': 0.5636,\n",
    "    'Micro Precision': 0.6985,\n",
    "    'Micro Recall': 0.5987\n",
    "}\n",
    "\n",
    "# Lab 5 Naive Bayes results\n",
    "lab5_nb_metrics = {\n",
    "    'Subset Accuracy': 0.4917,\n",
    "    'Hamming Loss': 0.1337,\n",
    "    'Micro F1': 0.6811,\n",
    "    'Macro F1': 0.6210,\n",
    "    'Micro Precision': 0.7114,\n",
    "    'Micro Recall': 0.6532\n",
    "}\n",
    "\n",
    "# Create comparison table\n",
    "print(\"=\"*90)\n",
    "print(\"MODEL COMPARISON: CNN (Lab 8) vs Neural Network & Naive Bayes (Lab 5)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\n{'Metric':<20} {'CNN (Lab 8)':<15} {'NN (Lab 5)':<15} {'NB (Lab 5)':<15} {'CNN vs NN':<12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for metric in cnn_metrics.keys():\n",
    "    cnn_val = cnn_metrics[metric]\n",
    "    nn_val = lab5_nn_metrics[metric]\n",
    "    nb_val = lab5_nb_metrics[metric]\n",
    "    \n",
    "    # Calculate improvement (for Hamming Loss, lower is better)\n",
    "    if metric == 'Hamming Loss':\n",
    "        diff = nn_val - cnn_val  # Positive = CNN is better\n",
    "    else:\n",
    "        diff = cnn_val - nn_val  # Positive = CNN is better\n",
    "    \n",
    "    diff_str = f\"+{diff:.4f}\" if diff > 0 else f\"{diff:.4f}\"\n",
    "    print(f\"{metric:<20} {cnn_val:<15.4f} {nn_val:<15.4f} {nb_val:<15.4f} {diff_str:<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Note: For Hamming Loss, lower is better. For all other metrics, higher is better.\")\n",
    "print(\"      'CNN vs NN' shows improvement (positive = CNN is better).\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS PERFORMANCE (CNN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true, \n",
    "    y_pred_cnn, \n",
    "    target_names=TOPIC_CLASSES,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary\n",
    "\n",
    "### 7.1 What was accomplished\n",
    "\n",
    "1. **Loaded preprocessed data** from Lab 2 (parquet files)\n",
    "2. **Loaded Word2Vec model** (GoogleNews-300) for word embeddings\n",
    "3. **Created embedding matrices** instead of binary BoW vectors\n",
    "4. **Implemented TextCNN** with:\n",
    "   - 1D Convolution (100 filters, kernel size 3)\n",
    "   - Global Max Pooling\n",
    "   - Sigmoid activation for multi-label classification\n",
    "5. **Trained and evaluated** the CNN model\n",
    "6. **Compared results** with Lab 5's MLPClassifier and Naive Bayes\n",
    "\n",
    "### 7.2 Key Findings\n",
    "\n",
    "| Aspect | Lab 5 (MLPClassifier) | Lab 8 (CNN) |\n",
    "|--------|----------------------|-------------|\n",
    "| Input | Binary BoW (1000 dims) | Embedding matrix (50×300) |\n",
    "| Word Semantics | Lost | Preserved |\n",
    "| Sequential Info | Lost | Captured by Conv1D |\n",
    "| Model Size | ~130K params | ~30K params |\n",
    "\n",
    "### 7.3 Advantages of CNN with Word Embeddings\n",
    "\n",
    "1. **Semantic representation**: Word2Vec captures semantic relationships (e.g., \"happy\" is similar to \"joyful\")\n",
    "2. **Local pattern detection**: Conv1D captures n-gram patterns in the text\n",
    "3. **Transfer learning**: Pre-trained embeddings provide knowledge from large corpora\n",
    "4. **Fewer parameters**: CNN is more parameter-efficient than fully-connected layers\n",
    "\n",
    "### 7.4 Potential Improvements\n",
    "\n",
    "- Use multiple kernel sizes (3, 4, 5) to capture different n-gram patterns\n",
    "- Add more convolutional layers for deeper feature extraction\n",
    "- Use pre-trained models like BERT for contextual embeddings\n",
    "- Experiment with different pooling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"LAB 8 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: CNN with Word2Vec Embeddings\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LEN}\")\n",
    "print(f\"Training samples: {len(df_train):,}\")\n",
    "print(f\"Test samples: {len(df_test):,}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nCNN Performance (Test Set):\")\n",
    "print(f\"  Subset Accuracy: {cnn_metrics['Subset Accuracy']:.4f}\")\n",
    "print(f\"  Micro F1: {cnn_metrics['Micro F1']:.4f}\")\n",
    "print(f\"  Macro F1: {cnn_metrics['Macro F1']:.4f}\")\n",
    "print(f\"  Hamming Loss: {cnn_metrics['Hamming Loss']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
