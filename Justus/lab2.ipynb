{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Preprocessing Pipeline\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook provides a comprehensive workflow for text data analysis and preprocessing.\n",
    "\n",
    "- **Data Loading**: Set up the environment, load and explore text datasets, perform statistical analysis, and create reusable data loading functions\n",
    "- **Preprocessing**: Apply various NLP preprocessing techniques using NLTK and SpaCy, analyze the impact of different methods, and develop a reusable preprocessing pipeline\n",
    "- **Dataset Creation**: Create Multi-Label and Single-Label datasets for use in subsequent labs (Lab 4, Lab 5)\n",
    "\n",
    "## Output Files\n",
    "\n",
    "This notebook creates the following datasets for use in Lab 4 and Lab 5:\n",
    "\n",
    "**Multi-Label Datasets:**\n",
    "- `../Data/multi_label/tweets_preprocessed_train.parquet`\n",
    "- `../Data/multi_label/tweets_preprocessed_test.parquet`\n",
    "- `../Data/multi_label/tweets_preprocessed_validation.parquet`\n",
    "\n",
    "**Single-Label Datasets (using Claude Haiku for intelligent label assignment):**\n",
    "- `../Data/single_label/tweets_single_label_train.parquet`\n",
    "- `../Data/single_label/tweets_single_label_test.parquet`\n",
    "- `../Data/single_label/tweets_single_label_validation.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "This section covers the initial setup and data loading phase. We will install necessary libraries, load the dataset, explore its structure and characteristics, perform statistical analysis, filter the data to text-only content, and create reusable functions for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Explore Dataset\n",
    "\n",
    "Loading the dataset into a pandas DataFrame and performing initial exploration. This includes viewing the first few rows, checking data types, examining the shape of the dataset, and identifying any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Basic Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Row count, Column count:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nFirst 10 Rows:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Initial Data Exploration with NLTK\n",
    "\n",
    "Using NLTK to perform basic text exploration and tokenization on a sample of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH NLTK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show sample tokenization on a few tweets\n",
    "print(\"\\nSample Tweet Tokenization:\")\n",
    "sample_text = df['text'].iloc[0]\n",
    "print(f\"Original: {sample_text}\")\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nNumber of English stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Initial Data Exploration with spaCy\n",
    "\n",
    "Using spaCy to perform basic linguistic analysis on sample tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL TEXT EXPLORATION WITH spaCy\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze a sample tweet\n",
    "sample_text = df['text'].iloc[5]\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(f\"\\nSample Tweet: {sample_text}\")\n",
    "print(f\"\\nTokens: {[token.text for token in doc]}\")\n",
    "print(f\"\\nPOS Tags: {[(token.text, token.pos_) for token in doc][:10]}\")\n",
    "print(f\"\\nEntities: {[(ent.text, ent.label_) for ent in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculate Statistics\n",
    "\n",
    "Computing descriptive statistics on the dataset to understand the data distribution. This includes category distributions, temporal patterns, duplicate analysis, text length statistics, word frequency analysis, and named entity statistics using pandas and NLP libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Category/Topic Distribution Analysis\n",
    "\n",
    "Analyzing how many tweets are available for each topic category using pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Convert label_name to lists (they're already arrays, just ensure they're lists)\n",
    "df['topics_list'] = df['label_name'].apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "\n",
    "# Count tweets per category\n",
    "all_topics = []\n",
    "for topics in df['topics_list']:\n",
    "    all_topics.extend(topics)\n",
    "\n",
    "topic_counts = Counter(all_topics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "topic_df = pd.DataFrame(\n",
    "    topic_counts.items(), \n",
    "    columns=['Topic', 'Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TWEETS PER CATEGORY\")\n",
    "print(\"=\"*50)\n",
    "print(topic_df.to_string(index=False))\n",
    "print(f\"\\nTotal unique topics: {len(topic_counts)}\")\n",
    "\n",
    "# Multi-label statistics\n",
    "df['num_topics'] = df['topics_list'].apply(len)\n",
    "print(f\"\\nTweets with 1 topic: {(df['num_topics'] == 1).sum()} ({(df['num_topics'] == 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"Tweets with 2+ topics: {(df['num_topics'] > 1).sum()} ({(df['num_topics'] > 1).sum() / len(df) * 100:.1f}%)\")\n",
    "print(f\"\\nMax topics per tweet: {df['num_topics'].max()}\")\n",
    "print(f\"Average topics per tweet: {df['num_topics'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Single-Label vs Multi-Label Analysis per Topic\n",
    "\n",
    "Analyzing which labels appear as single labels (sole topic of a tweet) versus only appearing in multi-label tweets (always combined with other topics). This is important for understanding class imbalance in single-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SINGLE-LABEL vs MULTI-LABEL ANALYSIS PER TOPIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate single-label and multi-label tweets\n",
    "single_label_tweets = df[df['num_topics'] == 1]\n",
    "multi_label_tweets = df[df['num_topics'] > 1]\n",
    "\n",
    "print(f\"\\nTotal tweets: {len(df)}\")\n",
    "print(f\"Single-label tweets: {len(single_label_tweets)} ({len(single_label_tweets)/len(df)*100:.1f}%)\")\n",
    "print(f\"Multi-label tweets: {len(multi_label_tweets)} ({len(multi_label_tweets)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Count occurrences in single-label vs multi-label tweets for each topic\n",
    "single_label_counts = Counter()\n",
    "multi_label_counts = Counter()\n",
    "\n",
    "for topics in single_label_tweets['topics_list']:\n",
    "    for topic in topics:\n",
    "        single_label_counts[topic] += 1\n",
    "\n",
    "for topics in multi_label_tweets['topics_list']:\n",
    "    for topic in topics:\n",
    "        multi_label_counts[topic] += 1\n",
    "\n",
    "# Create analysis DataFrame\n",
    "analysis_data = []\n",
    "all_topics_set = set(single_label_counts.keys()) | set(multi_label_counts.keys())\n",
    "\n",
    "for topic in all_topics_set:\n",
    "    single_count = single_label_counts.get(topic, 0)\n",
    "    multi_count = multi_label_counts.get(topic, 0)\n",
    "    total = single_count + multi_count\n",
    "    single_pct = (single_count / total * 100) if total > 0 else 0\n",
    "    \n",
    "    analysis_data.append({\n",
    "        'Topic': topic,\n",
    "        'Single-Label': single_count,\n",
    "        'Multi-Label': multi_count,\n",
    "        'Total': total,\n",
    "        'Single-Label %': single_pct,\n",
    "        'Status': 'âš ï¸ ONLY MULTI-LABEL' if single_count == 0 else ('âœ“ Has Single-Label' if single_count > 50 else 'âš ï¸ Few Single-Labels')\n",
    "    })\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_data).sort_values('Total', ascending=True)\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Data for plotting\n",
    "topics = analysis_df['Topic'].tolist()\n",
    "single_counts = analysis_df['Single-Label'].tolist()\n",
    "multi_counts = analysis_df['Multi-Label'].tolist()\n",
    "\n",
    "y_pos = np.arange(len(topics))\n",
    "bar_height = 0.35\n",
    "\n",
    "# Create bars\n",
    "bars_single = ax.barh(y_pos - bar_height/2, single_counts, bar_height, \n",
    "                       label='Single-Label', color='#3498db', edgecolor='black', linewidth=0.5)\n",
    "bars_multi = ax.barh(y_pos + bar_height/2, multi_counts, bar_height, \n",
    "                      label='Multi-Label', color='#e74c3c', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add count annotations\n",
    "for i, (single, multi) in enumerate(zip(single_counts, multi_counts)):\n",
    "    ax.annotate(f'{single}', xy=(single + 10, i - bar_height/2), va='center', ha='left', fontsize=9, color='#2980b9')\n",
    "    ax.annotate(f'{multi}', xy=(multi + 10, i + bar_height/2), va='center', ha='left', fontsize=9, color='#c0392b')\n",
    "\n",
    "# Customize chart\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(topics)\n",
    "ax.set_xlabel('Number of Tweets', fontsize=12)\n",
    "ax.set_ylabel('Topic', fontsize=12)\n",
    "ax.set_title('Single-Label vs Multi-Label Tweet Distribution per Topic\\n(Blue: single label | Red: multi label)', fontsize=14)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_xlim(0, max(max(single_counts), max(multi_counts)) * 1.15)\n",
    "\n",
    "# Add gridlines for better readability\n",
    "ax.xaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Label Distribution: Single-Label vs Multi-Label Tweets\")\n",
    "print(\"-\"*60)\n",
    "print(analysis_df.sort_values('Single-Label', ascending=True).to_string(index=False))\n",
    "\n",
    "# Highlight problematic labels\n",
    "only_multi_labels = analysis_df[analysis_df['Single-Label'] == 0]['Topic'].tolist()\n",
    "few_single_labels = analysis_df[(analysis_df['Single-Label'] > 0) & (analysis_df['Single-Label'] < 50)]['Topic'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âš ï¸ POTENTIAL ISSUES FOR SINGLE-LABEL CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if only_multi_labels:\n",
    "    print(f\"\\nðŸš¨ Labels that NEVER appear as single-label ({len(only_multi_labels)}):\")\n",
    "    print(\"   These labels will have 0 training samples in single-label mode!\")\n",
    "    for label in only_multi_labels:\n",
    "        total = analysis_df[analysis_df['Topic'] == label]['Total'].values[0]\n",
    "        print(f\"   - {label} (total: {total}, always combined with other labels)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All labels have at least some single-label tweets\")\n",
    "\n",
    "if few_single_labels:\n",
    "    print(f\"\\nâš ï¸ Labels with very few single-label tweets ({len(few_single_labels)}):\")\n",
    "    print(\"   These labels may be underrepresented in single-label classification!\")\n",
    "    for label in few_single_labels:\n",
    "        single = analysis_df[analysis_df['Topic'] == label]['Single-Label'].values[0]\n",
    "        total = analysis_df[analysis_df['Topic'] == label]['Total'].values[0]\n",
    "        print(f\"   - {label} (single: {single}, total: {total}, {single/total*100:.1f}% as single-label)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Temporal Distribution Analysis\n",
    "\n",
    "Analyzing the distribution of tweets across different time periods using pandas datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date_parsed'].dt.year\n",
    "df['month'] = df['date_parsed'].dt.month\n",
    "df['day_of_week'] = df['date_parsed'].dt.day_name()\n",
    "\n",
    "# Count tweets per year\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nTweets per Year:\")\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} tweets ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDate range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")\n",
    "print(f\"Total days covered: {(df['date_parsed'].max() - df['date_parsed'].min()).days} days\")\n",
    "\n",
    "# Month distribution\n",
    "print(\"\\nTweets per Month (across all years):\")\n",
    "month_counts = df['month'].value_counts().sort_index()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for month, count in month_counts.items():\n",
    "    print(f\"  {month_names[month-1]}: {count:,} tweets\")\n",
    "\n",
    "# Day of week distribution\n",
    "print(\"\\nTweets per Day of Week:\")\n",
    "dow_counts = df['day_of_week'].value_counts()\n",
    "for day, count in dow_counts.items():\n",
    "    print(f\"  {day}: {count:,} tweets ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Duplicate Detection Statistics\n",
    "\n",
    "Identifying and quantifying exact and potential near-duplicate tweets using pandas string operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for exact text duplicates\n",
    "duplicate_texts = df['text'].duplicated().sum()\n",
    "print(f\"Exact duplicate tweets: {duplicate_texts} ({duplicate_texts / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = df['id'].duplicated().sum()\n",
    "print(f\"Duplicate IDs: {duplicate_ids}\")\n",
    "\n",
    "# Show sample duplicates if any exist\n",
    "if duplicate_texts > 0:\n",
    "    print(\"\\nSample Duplicate Tweets:\")\n",
    "    duplicated_mask = df['text'].duplicated(keep=False)\n",
    "    sample_duplicates = df[duplicated_mask].groupby('text').head(2)\n",
    "    print(sample_duplicates[['text', 'date', 'label_name']].head(6).to_string())\n",
    "else:\n",
    "    print(\"\\nâœ“ No exact duplicate tweets found!\")\n",
    "\n",
    "# Near-duplicate detection (same first 50 characters)\n",
    "df['text_start'] = df['text'].str.lower().str.strip().str[:50]\n",
    "potential_near_dupes = df['text_start'].duplicated().sum()\n",
    "print(f\"\\nPotential near-duplicates (same first 50 chars): {potential_near_dupes} ({potential_near_dupes / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# List all near-duplicate IDs grouped\n",
    "if potential_near_dupes > 0:\n",
    "    print(\"\\nAll Near-Duplicate Groups (sorted by group size):\")\n",
    "    near_dupe_mask = df['text_start'].duplicated(keep=False)\n",
    "    near_dupe_df = df[near_dupe_mask][['text_start', 'id', 'text']]\n",
    "    \n",
    "    # Group by text_start and collect IDs\n",
    "    grouped = near_dupe_df.groupby('text_start')\n",
    "    \n",
    "    # Sort groups by size (largest first)\n",
    "    group_sizes = grouped.size().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal near-duplicate groups: {len(group_sizes)}\")\n",
    "    print(f\"Largest group size: {group_sizes.max()}\\n\")\n",
    "    \n",
    "    for i, (text_start, size) in enumerate(group_sizes.items(), 1):\n",
    "        group_ids = grouped.get_group(text_start)['id'].tolist()\n",
    "        print(f\"Group {i} (Size: {size}):\")\n",
    "        print(f\"  Text preview: {text_start}...\")\n",
    "        print(f\"  IDs: {group_ids}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nâœ“ No potential near-duplicates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Text Length and Composition Statistics\n",
    "\n",
    "Analyzing text characteristics using pandas string methods and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "df['avg_word_length'] = df['text'].str.replace(' ', '').str.len() / df['word_count']\n",
    "\n",
    "# Count special characters\n",
    "df['hashtag_count'] = df['text'].str.count('#')\n",
    "df['mention_count'] = df['text'].str.count('@')\n",
    "df['url_count'] = df['text'].str.count('http')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT LENGTH AND COMPOSITION STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nCharacter Length Statistics:\")\n",
    "print(f\"  Mean: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {df['text_length'].median():.1f} characters\")\n",
    "print(f\"  Std Dev: {df['text_length'].std():.1f} characters\")\n",
    "print(f\"  Min: {df['text_length'].min()} characters\")\n",
    "print(f\"  Max: {df['text_length'].max()} characters\")\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(f\"  Mean: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"  Median: {df['word_count'].median():.1f} words\")\n",
    "print(f\"  Std Dev: {df['word_count'].std():.1f} words\")\n",
    "print(f\"  Min: {df['word_count'].min()} words\")\n",
    "print(f\"  Max: {df['word_count'].max()} words\")\n",
    "\n",
    "print(\"\\nAverage Word Length:\")\n",
    "print(f\"  Mean: {df['avg_word_length'].mean():.2f} characters per word\")\n",
    "print(f\"  Median: {df['avg_word_length'].median():.2f} characters per word\")\n",
    "\n",
    "print(\"\\nSpecial Character Statistics:\")\n",
    "print(f\"  Tweets with hashtags: {(df['hashtag_count'] > 0).sum()} ({(df['hashtag_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average hashtags per tweet: {df['hashtag_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with mentions: {(df['mention_count'] > 0).sum()} ({(df['mention_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average mentions per tweet: {df['mention_count'].mean():.2f}\")\n",
    "print(f\"  Tweets with URLs: {(df['url_count'] > 0).sum()} ({(df['url_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show extreme examples\n",
    "print(\"\\nExtreme Examples:\")\n",
    "print(f\"\\nShortest tweet ({df['text_length'].min()} chars):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmin(), 'text']}\")\n",
    "print(f\"\\nLongest tweet ({df['text_length'].max()} chars, first 150):\")\n",
    "print(f\"  {df.loc[df['text_length'].idxmax(), 'text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Word Frequency Statistics (NLTK)\n",
    "\n",
    "Analyzing word frequency patterns using NLTK tokenization and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORD FREQUENCY STATISTICS (NLTK)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tokenize all tweets\n",
    "all_tokens = []\n",
    "for text in df['text']:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords, keep only meaningful words\n",
    "    tokens = [\n",
    "        t for t in tokens \n",
    "        if t not in string.punctuation \n",
    "        and t not in stop_words \n",
    "        and len(t) > 2\n",
    "    ]\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "top_words = pd.DataFrame(\n",
    "    token_freq.most_common(30), \n",
    "    columns=['Word', 'Frequency']\n",
    ")\n",
    "\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"  Total tokens (after filtering): {len(all_tokens):,}\")\n",
    "print(f\"  Unique words (vocabulary size): {len(token_freq):,}\")\n",
    "print(f\"  Average token frequency: {len(all_tokens) / len(token_freq):.2f}\")\n",
    "print(f\"  Words appearing only once: {sum(1 for count in token_freq.values() if count == 1):,}\")\n",
    "print(f\"  Words appearing 10+ times: {sum(1 for count in token_freq.values() if count >= 10):,}\")\n",
    "\n",
    "print(f\"\\nTop 30 Most Common Words:\")\n",
    "print(top_words.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6 Named Entity Statistics (spaCy)\n",
    "\n",
    "Analyzing named entity distributions using spaCy NER and pandas aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NAMED ENTITY STATISTICS (spaCy)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAnalyzing {len(df)} tweets... This may take a few minutes...\")\n",
    "\n",
    "# Extract entities from all tweets\n",
    "all_entities = []\n",
    "tweets_with_entities = 0\n",
    "entity_counts_per_tweet = []\n",
    "\n",
    "# Use nlp.pipe for better performance\n",
    "texts = df['text'].apply(lambda x: x[:500]).tolist()\n",
    "\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=50), total=len(df), desc=\"Processing\"):\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    entity_counts_per_tweet.append(len(entities))\n",
    "    if entities:\n",
    "        tweets_with_entities += 1\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "# Add entity count to dataframe\n",
    "df['entity_count'] = entity_counts_per_tweet\n",
    "\n",
    "print(f\"\\nEntity Detection Statistics:\")\n",
    "print(f\"  Tweets with entities: {tweets_with_entities:,} ({tweets_with_entities/len(df)*100:.1f}%)\")\n",
    "print(f\"  Tweets without entities: {len(df) - tweets_with_entities:,} ({(len(df) - tweets_with_entities)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total entities found: {len(all_entities):,}\")\n",
    "print(f\"  Average entities per tweet: {df['entity_count'].mean():.2f}\")\n",
    "print(f\"  Median entities per tweet: {df['entity_count'].median():.0f}\")\n",
    "print(f\"  Max entities in a tweet: {df['entity_count'].max()}\")\n",
    "\n",
    "# Count entity types\n",
    "entity_types = Counter([label for text, label in all_entities])\n",
    "entity_type_df = pd.DataFrame(\n",
    "    entity_types.most_common(), \n",
    "    columns=['Entity Type', 'Count']\n",
    ")\n",
    "entity_type_df['Percentage'] = (entity_type_df['Count'] / len(all_entities) * 100).round(1)\n",
    "\n",
    "print(\"\\nEntity Type Distribution:\")\n",
    "print(entity_type_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nEntity Type Legend:\")\n",
    "print(\"\\nEntity Type Legend (Complete):\")\n",
    "print(\"  PERSON      = People, including fictional characters\")\n",
    "print(\"  ORG         = Companies, agencies, institutions, organizations\")\n",
    "print(\"  GPE         = Countries, cities, states (Geo-Political Entities)\")\n",
    "print(\"  DATE        = Absolute or relative dates or periods\")\n",
    "print(\"  CARDINAL    = Numerals that do not fall under another type\")\n",
    "print(\"  MONEY       = Monetary values, including unit\")\n",
    "print(\"  TIME        = Times smaller than a day\")\n",
    "print(\"  NORP        = Nationalities, religious or political groups\")\n",
    "print(\"  ORDINAL     = First, second, third, etc.\")\n",
    "print(\"  WORK_OF_ART = Titles of books, songs, movies, etc.\")\n",
    "print(\"  EVENT       = Named hurricanes, battles, wars, sports events\")\n",
    "print(\"  PRODUCT     = Objects, vehicles, foods, etc. (not services)\")\n",
    "print(\"  LOC         = Non-GPE locations, mountain ranges, bodies of water\")\n",
    "print(\"  FAC         = Buildings, airports, highways, bridges, etc.\")\n",
    "print(\"  QUANTITY    = Measurements, as of weight or distance\")\n",
    "print(\"  LAW         = Named documents made into laws\")\n",
    "print(\"  PERCENT     = Percentage, including '%'\")\n",
    "print(\"  LANGUAGE    = Any named language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filter to Text Only\n",
    "\n",
    "Filtering the dataset to extract only text columns and remove any non-textual data. This step ensures that subsequent processing focuses exclusively on textual content and handles any data type conversions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name'):\n",
    "    \"\"\"\n",
    "    Filter dataset to only text and label columns, removing all numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Input dataframe to filter\n",
    "    text_col : str\n",
    "        Name of the text column (default: 'text')\n",
    "    label_col : str\n",
    "        Name of the label column (default: 'label_name')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Filtered dataframe with only text columns and no numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select only the text and label_name columns\n",
    "    df_filtered = dataframe[[text_col, label_col]].copy()\n",
    "    \n",
    "    # Step 2: Remove all numbers from the text column using regex\n",
    "    # This removes all digits (0-9) from the text\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    # Step 3: Handle label_name - convert to string if it's a list, then remove numbers\n",
    "    # First check if it's already a list or needs conversion\n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        # Keep as list, no number removal needed (labels are text)\n",
    "        pass\n",
    "    else:\n",
    "        # If it's a string representation, convert and clean\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    # Step 4: Clean up any extra whitespace created by removing numbers\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Apply the filtering function\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILTERING TO TEXT ONLY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show original dataset info\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Apply the filter\n",
    "df_text_only = filter_to_text_only(df)\n",
    "\n",
    "# Show filtered dataset info\n",
    "print(\"\\nFiltered Dataset (Text Only - No Numbers):\")\n",
    "print(f\"  Shape: {df_text_only.shape}\")\n",
    "print(f\"  Columns: {df_text_only.columns.tolist()}\")\n",
    "print(f\"  Sample text: {df_text_only['text'].iloc[0][:100]}...\")\n",
    "\n",
    "# Show examples of number removal\n",
    "print(\"\\nExamples of Number Removal:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original:  {df['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Filtered:  {df_text_only['text'].iloc[i][:80]}...\")\n",
    "\n",
    "# Show label_name comparison\n",
    "print(\"\\nLabel Name Comparison:\")\n",
    "print(f\"  Original label_name: {df['label_name'].iloc[0]}\")\n",
    "print(f\"  Filtered label_name: {df_text_only['label_name'].iloc[0]}\")\n",
    "\n",
    "# Statistics on number removal\n",
    "original_chars = df['text'].str.len().sum()\n",
    "filtered_chars = df_text_only['text'].str.len().sum()\n",
    "chars_removed = original_chars - filtered_chars\n",
    "\n",
    "print(f\"\\nCharacter Statistics:\")\n",
    "print(f\"  Original total characters: {original_chars:,}\")\n",
    "print(f\"  Filtered total characters: {filtered_chars:,}\")\n",
    "print(f\"  Characters removed (numbers): {chars_removed:,} ({chars_removed/original_chars*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ Dataset successfully filtered to text only (numbers removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create Reusable Loading and Filtering Functions\n",
    "\n",
    "Reusable functions that encapsulate the data loading and text-filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from Hugging Face using the datasets library\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"train_all\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "tweets_raw = dataset.to_pandas()\n",
    "\n",
    "# Filter the dataset to text only (no numbers)\n",
    "def filter_to_text_only(dataframe, text_col='text', label_col='label_name', label_num_col='label'):\n",
    "    df_filtered = dataframe[[text_col, label_col, label_num_col]].copy()\n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\d+', '', regex=True)\n",
    "    \n",
    "    if isinstance(df_filtered[label_col].iloc[0], list):\n",
    "        pass\n",
    "    else:\n",
    "        df_filtered[label_col] = df_filtered[label_col].astype(str)\n",
    "    \n",
    "    df_filtered[text_col] = df_filtered[text_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "tweets_text_only = filter_to_text_only(tweets_raw)\n",
    "\n",
    "print(\"\\nâœ“ Dataset successfully loaded and filtered to text only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "This section focuses on text preprocessing techniques. We will review common preprocessing methods, apply them systematically using NLTK and SpaCy, analyze how the order of operations affects results, evaluate the usefulness of each method for different scenarios, and create a reusable preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Methods Used (in Order)\n",
    "\n",
    "#### 1. **Remove RT indicator**\n",
    "   - **Purpose:** Removes retweet markers that don't carry semantic meaning\n",
    "   - **Order:** FIRST - removes noise before any text processing\n",
    "\n",
    "#### 2. **Remove placeholders (USERNAME, URL, mentions)**\n",
    "   - **Purpose:** Removes dataset-specific placeholders that don't contribute to topic classification\n",
    "   - **Order:** EARLY - clean structural noise before text normalization\n",
    "\n",
    "#### 3. **Convert emojis to text**\n",
    "   - **Purpose:** Transforms emojis into descriptive words that preserve semantic meaning (ðŸŽ® â†’ video game)\n",
    "   - **Order:** AFTER placeholders, BEFORE hashtags - emojis may appear anywhere in text\n",
    "\n",
    "#### 4. **Extract hashtag text**\n",
    "   - **Purpose:** Preserves topic-relevant keywords from hashtags (#Gaming â†’ Gaming)\n",
    "   - **Order:** AFTER emoji conversion - hashtags rarely contain emojis but order matters for consistency\n",
    "\n",
    "#### 5. **Segment CamelCase words**\n",
    "   - **Purpose:** Splits compound words for better tokenization (GameOfThrones â†’ Game Of Thrones)\n",
    "   - **Order:** BEFORE lowercase - capitalization patterns guide segmentation\n",
    "\n",
    "#### 6. **Normalize whitespace and lowercase**\n",
    "   - **Purpose:** Standardizes text format for consistent processing\n",
    "   - **Order:** AFTER special token handling - ensures uniform text before tokenization\n",
    "\n",
    "#### 7. **Tokenize with SpaCy**\n",
    "   - **Purpose:** Splits text into linguistic tokens with POS and lemma information\n",
    "   - **Order:** AFTER normalization - requires clean, lowercase text\n",
    "\n",
    "#### 8. **Filter and lemmatize tokens**\n",
    "   - **Purpose:** Removes noise tokens and reduces words to base forms\n",
    "   - **Order:** LAST - operates on clean, filtered tokens\n",
    "\n",
    "### Why did we choose this order?\n",
    "\n",
    "**Critical Dependencies:**\n",
    "- Emoji conversion **before** lowercase â†’ emoji descriptions use underscores that get normalized\n",
    "- Special tokens **before** tokenization â†’ removes as complete units\n",
    "- Lowercase **before** tokenization â†’ consistent stopword matching\n",
    "- Tokenization **before** filtering â†’ need tokens to filter\n",
    "- Lemmatization **last** â†’ final transformation on clean tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reusable Preprocessing Pipeline\n",
    "\n",
    "Development of a modular, configurable preprocessing function that can be easily reused in future labs. The pipeline allows for flexible selection of preprocessing steps and parameters, making it adaptable to different text analysis tasks and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Load SpaCy model\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"âœ“ SpaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"Installing SpaCy model...\")\n",
    "    import os\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"âœ“ SpaCy model loaded successfully\")\n",
    "\n",
    "# Import emoji package for emoji handling\n",
    "import emoji\n",
    "\n",
    "def is_latin_alphabet(word):\n",
    "    \"\"\"\n",
    "    Check if a word contains only Latin alphabet characters.\n",
    "    Filters out words with Cyrillic, Arabic, Chinese, etc.\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return False\n",
    "    return all(ord('a') <= ord(c.lower()) <= ord('z') for c in word)\n",
    "\n",
    "def segment_camelcase(text):\n",
    "    \"\"\"\n",
    "    Segment CamelCase words into separate words without regex.\n",
    "    Example: 'GameOfThrones' â†’ 'Game Of Thrones'\n",
    "    This is important for hashtags like #GameOfThrones after removing the #\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        # Add current character\n",
    "        result.append(char)\n",
    "        \n",
    "        # Check if we need to insert a space\n",
    "        if i < len(text) - 1:\n",
    "            current = char\n",
    "            next_char = text[i + 1]\n",
    "            \n",
    "            # Case 1: lowercase â†’ uppercase (e.g., 'e' â†’ 'O' in 'GameOf')\n",
    "            if current.islower() and next_char.isupper():\n",
    "                result.append(' ')\n",
    "            \n",
    "            # Case 2: uppercase â†’ uppercase â†’ lowercase (e.g., 'HTML' â†’ 'Parser')\n",
    "            elif i < len(text) - 2:\n",
    "                after_next = text[i + 2]\n",
    "                if current.isupper() and next_char.isupper() and after_next.islower():\n",
    "                    result.append(' ')\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"\n",
    "    Topic-optimized preprocessing for tweet classification.\n",
    "    Preserves topic-relevant information while removing noise.\n",
    "    Removes special characters, emojis, and non-Latin script words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Step 1: Remove RT (retweet indicator)\n",
    "    text = text.replace('RT ', ' ').replace('rt ', ' ')\n",
    "    \n",
    "    # Step 2: Remove URLs and placeholders\n",
    "    text = text.replace('{{URL}}', ' ')\n",
    "    text = text.replace('{{USERNAME}}', ' ')\n",
    "    for protocol in ['https://', 'http://', 'www.']:\n",
    "        if protocol in text:\n",
    "            parts = text.split(protocol)\n",
    "            text = parts[0] + ' ' + ' '.join([' '.join(p.split()[1:]) if p.split() else '' for p in parts[1:]])\n",
    "    \n",
    "    # Step 3: Remove mentions\n",
    "    words_list = text.split()\n",
    "    words_list = [w for w in words_list if not (w.startswith('{@') or w.startswith('@'))]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 3.5: Convert emojis to text descriptions (ðŸŽ® â†’ video game)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = text.replace('_', ' ')\n",
    "    \n",
    "    # Step 4: Extract hashtag text (#Gaming â†’ Gaming, #GameOfThrones â†’ GameOfThrones)\n",
    "    words_list = text.split()\n",
    "    words_list = [w[1:] if w.startswith('#') else w for w in words_list]\n",
    "    text = ' '.join(words_list)\n",
    "    \n",
    "    # Step 4.5: Segment CamelCase words\n",
    "    # GameOfThrones â†’ Game Of Thrones\n",
    "    text = segment_camelcase(text)\n",
    "    \n",
    "    # Step 5: Normalize whitespace and lowercase\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 6: Tokenize with SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 7: Filter and lemmatize tokens\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        # Skip punctuation\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        \n",
    "        # Skip if not alphabetic (removes special characters, emojis, numbers)\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "        \n",
    "        # Skip tokens shorter than 2 characters\n",
    "        if len(token.text) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Remove stopwords (using SpaCy's stopword detection)\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        # Check if word uses Latin alphabet (filters out Cyrillic, Arabic, Chinese, etc.)\n",
    "        if not is_latin_alphabet(token.text):\n",
    "            continue\n",
    "        \n",
    "        # Use lemmatized form\n",
    "        processed_tokens.append(token.lemma_)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "tweets_preprocessed_train = tweets_text_only.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "tweets_preprocessed_train['text'] = tweets_preprocessed_train['text'].apply(preprocess_tweet)\n",
    "\n",
    "print(\"\\nâœ“ Preprocessing complete!\")\n",
    "print(f\"âœ“ Processed {len(tweets_preprocessed_train)} tweets\")\n",
    "print(f\"âœ“ Original 'tweets_text_only' unchanged | Processed data in 'tweets_preprocessed_train'\")\n",
    "\n",
    "# Save the DataFrame to the Data folder\n",
    "import os\n",
    "\n",
    "# Create Data folder if it does not exist\n",
    "os.makedirs('../Data', exist_ok=True)\n",
    "\n",
    "# Save the data as parquet\n",
    "output_path = '../Data/multi_label/tweets_preprocessed_train.parquet'\n",
    "tweets_preprocessed_train.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_train.shape}\")\n",
    "print(f\"\\nâœ“ Training DataFrame saved to: {output_path}\")\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing for Test and Validation Splits\n",
    "\n",
    "Apply the same preprocessing pipeline to the test and validation splits from HuggingFace to ensure consistency between training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and validation splits from HuggingFace\n",
    "print(\"Loading test and validation splits from HuggingFace...\")\n",
    "test_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"test_2021\")\n",
    "val_dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\", split=\"validation_2021\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "test_raw = test_dataset.to_pandas()\n",
    "val_raw = val_dataset.to_pandas()\n",
    "\n",
    "print(f\"Test samples: {len(test_raw):,}\")\n",
    "print(f\"Validation samples: {len(val_raw):,}\")\n",
    "\n",
    "# Apply the same text-only filter (remove numbers)\n",
    "test_text_only = filter_to_text_only(test_raw)\n",
    "val_text_only = filter_to_text_only(val_raw)\n",
    "\n",
    "# Apply the same preprocessing pipeline\n",
    "tweets_preprocessed_test = test_text_only.copy()\n",
    "tweets_preprocessed_test['text'] = tweets_preprocessed_test['text'].apply(preprocess_tweet)\n",
    "\n",
    "tweets_preprocessed_validation = val_text_only.copy()\n",
    "tweets_preprocessed_validation['text'] = tweets_preprocessed_validation['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Save test data as parquet\n",
    "test_output_path = '../Data/multi_label/tweets_preprocessed_test.parquet'\n",
    "tweets_preprocessed_test.to_parquet(test_output_path, index=False)\n",
    "\n",
    "# Save validation data as parquet\n",
    "val_output_path = '../Data/multi_label/tweets_preprocessed_validation.parquet'\n",
    "tweets_preprocessed_validation.to_parquet(val_output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Test data preprocessing complete!\")\n",
    "print(f\"âœ“ Saved to: {test_output_path}\")\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_test.shape}\")\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_test.columns)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Validation data preprocessing complete!\")\n",
    "print(f\"âœ“ Saved to: {val_output_path}\")\n",
    "print(f\"âœ“ Shape: {tweets_preprocessed_validation.shape}\")\n",
    "print(f\"âœ“ Features: {list(tweets_preprocessed_validation.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Data for Training\n",
    "\n",
    "This section addresses a common issue in multi-label classification: **class imbalance**. Some labels in our dataset have very few training samples, which can negatively impact model performance during training.\n",
    "\n",
    "**Why this matters:**\n",
    "- Labels with too few samples cannot be learned effectively by the model\n",
    "- Class imbalance can lead to biased predictions toward majority classes\n",
    "- Having consistent labels across train/validation/test splits is crucial for proper evaluation\n",
    "\n",
    "**What this function does:**\n",
    "1. **Visualizes** the distribution of labels using matplotlib to identify potential imbalances\n",
    "2. **Removes** labels with fewer than a specified threshold (default: 180 tweets) from all data splits\n",
    "3. **Updates** the binary label vectors to reflect the removed labels\n",
    "4. **Ensures consistency** by applying the same filtering to train, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Label Co-occurrence Analysis\n",
    "\n",
    "This section analyzes which labels frequently appear together in multi-label tweets. Understanding label co-occurrence patterns can provide insights into:\n",
    "- **Topic relationships**: Which topics are semantically related or commonly discussed together\n",
    "- **Dataset characteristics**: Natural clustering of content themes\n",
    "- **Model design considerations**: Whether certain label combinations should be treated specially\n",
    "\n",
    "For tweets with 3+ labels, we count all possible label pairs (e.g., for labels A, B, C, we count pairs: A-B, A-C, B-C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LABEL CO-OCCURRENCE ANALYSIS\n",
    "# ============================================================\n",
    "# Analysiert welche Label-Paare am hÃ¤ufigsten gemeinsam auftreten\n",
    "# Bei Tweets mit 3+ Labels werden alle mÃ¶glichen Paare gezÃ¤hlt\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_label_names_cooc(label_str):\n",
    "    \"\"\"Parse the label_name string into a list of labels.\"\"\"\n",
    "    if isinstance(label_str, str):\n",
    "        label_str = label_str.strip()\n",
    "        if label_str.startswith('[') and label_str.endswith(']'):\n",
    "            content = label_str[1:-1]\n",
    "            items = re.findall(r\"'([^']*)'\", content)\n",
    "            return items\n",
    "    return []\n",
    "\n",
    "# Verwende die gespeicherten preprocessed Daten\n",
    "df_cooc = tweets_preprocessed_train.copy()\n",
    "df_cooc['labels'] = df_cooc['label_name'].apply(parse_label_names_cooc)\n",
    "df_cooc['label_count'] = df_cooc['labels'].apply(len)\n",
    "\n",
    "# Nur Multi-Label Tweets analysieren (2+ Labels)\n",
    "multi_label_tweets_cooc = df_cooc[df_cooc['label_count'] >= 2].copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LABEL CO-OCCURRENCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nâœ“ Gesamt Tweets: {len(df_cooc):,}\")\n",
    "print(f\"âœ“ Multi-Label Tweets (2+ Labels): {len(multi_label_tweets_cooc):,}\")\n",
    "print(f\"âœ“ Prozent Multi-Label: {100 * len(multi_label_tweets_cooc) / len(df_cooc):.1f}%\")\n",
    "\n",
    "# ZÃ¤hle alle Label-Paare\n",
    "label_pair_counts = Counter()\n",
    "\n",
    "for labels in multi_label_tweets_cooc['labels']:\n",
    "    # FÃ¼r jeden Tweet: alle mÃ¶glichen 2er-Kombinationen erstellen\n",
    "    # sorted() stellt sicher, dass (A,B) und (B,A) als gleich behandelt werden\n",
    "    pairs = combinations(sorted(labels), 2)\n",
    "    for pair in pairs:\n",
    "        label_pair_counts[pair] += 1\n",
    "\n",
    "# Top 20 hÃ¤ufigste Label-Paare\n",
    "top_pairs = label_pair_counts.most_common(20)\n",
    "\n",
    "print(f\"\\nâœ“ Gefundene einzigartige Label-Paare: {len(label_pair_counts)}\")\n",
    "print(f\"\\n\" + \"-\" * 70)\n",
    "print(\"TOP 20 HÃ„UFIGSTE LABEL-KOMBINATIONEN\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Rang':<6} {'Label-Paar':<55} {'Anzahl':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rank, (pair, count) in enumerate(top_pairs, 1):\n",
    "    pair_str = f\"{pair[0]} + {pair[1]}\"\n",
    "    print(f\"{rank:<6} {pair_str:<55} {count:,}\")\n",
    "\n",
    "# Visualisierung als Heatmap\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LABEL CO-OCCURRENCE HEATMAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Alle einzigartigen Labels aus den Multi-Label Tweets\n",
    "all_labels = sorted(set(label for labels in multi_label_tweets_cooc['labels'] for label in labels))\n",
    "n_labels = len(all_labels)\n",
    "label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "# Co-occurrence Matrix erstellen\n",
    "cooccurrence_matrix = np.zeros((n_labels, n_labels), dtype=int)\n",
    "\n",
    "for labels in multi_label_tweets_cooc['labels']:\n",
    "    pairs = combinations(sorted(labels), 2)\n",
    "    for label1, label2 in pairs:\n",
    "        idx1, idx2 = label_to_idx[label1], label_to_idx[label2]\n",
    "        cooccurrence_matrix[idx1, idx2] += 1\n",
    "        cooccurrence_matrix[idx2, idx1] += 1  # Symmetrisch\n",
    "\n",
    "# Diagonale = Anzahl Tweets mit diesem Label (fÃ¼r Kontext)\n",
    "for labels in multi_label_tweets_cooc['labels']:\n",
    "    for label in labels:\n",
    "        cooccurrence_matrix[label_to_idx[label], label_to_idx[label]] += 1\n",
    "\n",
    "# Heatmap erstellen\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "im = ax.imshow(cooccurrence_matrix, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Achsenbeschriftung\n",
    "ax.set_xticks(range(n_labels))\n",
    "ax.set_yticks(range(n_labels))\n",
    "ax.set_xticklabels(all_labels, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(all_labels, fontsize=9)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('Co-occurrence Count', fontsize=11)\n",
    "\n",
    "# Werte in Zellen anzeigen (nur fÃ¼r signifikante Werte)\n",
    "for i in range(n_labels):\n",
    "    for j in range(n_labels):\n",
    "        value = cooccurrence_matrix[i, j]\n",
    "        if value > 0 and i != j:  # Nur Off-Diagonal-Werte\n",
    "            text_color = 'white' if value > cooccurrence_matrix.max() * 0.5 else 'black'\n",
    "            ax.text(j, i, f'{value}', ha='center', va='center', \n",
    "                   fontsize=7, color=text_color)\n",
    "\n",
    "ax.set_title('Label Co-occurrence Matrix\\n(Multi-Label Tweets)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Label', fontsize=11)\n",
    "ax.set_ylabel('Label', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ZusÃ¤tzliche Statistiken\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ZUSÃ„TZLICHE STATISTIKEN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Labels die am hÃ¤ufigsten mit anderen kombiniert werden\n",
    "label_combination_freq = Counter()\n",
    "for labels in multi_label_tweets_cooc['labels']:\n",
    "    for label in labels:\n",
    "        label_combination_freq[label] += 1\n",
    "\n",
    "print(\"\\nâœ“ Labels die am hÃ¤ufigsten in Multi-Label Tweets vorkommen:\")\n",
    "print(\"-\" * 50)\n",
    "for label, count in label_combination_freq.most_common():\n",
    "    pct = 100 * count / len(multi_label_tweets_cooc)\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}% der Multi-Label Tweets)\")\n",
    "\n",
    "# Durchschnittliche Anzahl verschiedener Partner-Labels pro Label\n",
    "print(\"\\nâœ“ Partner-DiversitÃ¤t pro Label:\")\n",
    "print(\"-\" * 50)\n",
    "for label in sorted(all_labels):\n",
    "    partners = sum(1 for pair, count in label_pair_counts.items() \n",
    "                   if label in pair and count > 0)\n",
    "    print(f\"  {label}: kombiniert mit {partners} verschiedenen anderen Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def parse_label_names(label_str):\n",
    "    \"\"\"\n",
    "    Parse the label_name string into a list of labels.\n",
    "    The format is numpy-style: ['label1' 'label2'] instead of ['label1', 'label2']\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    label_str : str\n",
    "        String representation of labels in numpy array format\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of label strings\n",
    "    \"\"\"\n",
    "    if isinstance(label_str, str):\n",
    "        label_str = label_str.strip()\n",
    "        if label_str.startswith('[') and label_str.endswith(']'):\n",
    "            content = label_str[1:-1]\n",
    "            items = re.findall(r\"'([^']*)'\" , content)\n",
    "            return items\n",
    "    return []\n",
    "\n",
    "\n",
    "def prepare_data_for_training(df_train, df_validation, df_test, min_samples=180, show_plot=True):\n",
    "    \"\"\"\n",
    "    Prepare datasets for training by removing labels with insufficient samples.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Counts the number of tweets per label in the training set\n",
    "    2. Visualizes the label distribution using matplotlib\n",
    "    3. Identifies labels with fewer than min_samples tweets\n",
    "    4. Removes tweets containing only the underrepresented labels from all splits\n",
    "    5. Updates the label vectors and label_name columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training dataframe with 'text', 'label_name', and 'label' columns\n",
    "    df_validation : pd.DataFrame\n",
    "        Validation dataframe with same columns\n",
    "    df_test : pd.DataFrame\n",
    "        Test dataframe with same columns\n",
    "    min_samples : int, default=180\n",
    "        Minimum number of tweets required for a label to be kept\n",
    "    show_plot : bool, default=True\n",
    "        Whether to display the matplotlib visualization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (pd.DataFrame, pd.DataFrame, pd.DataFrame, list)\n",
    "        Filtered train, validation, test dataframes, and list of removed labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    train = df_train.copy()\n",
    "    val = df_validation.copy()\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    # Parse labels for all datasets\n",
    "    train['parsed_labels'] = train['label_name'].apply(parse_label_names)\n",
    "    val['parsed_labels'] = val['label_name'].apply(parse_label_names)\n",
    "    test['parsed_labels'] = test['label_name'].apply(parse_label_names)\n",
    "    \n",
    "    # Count tweets per label in training set\n",
    "    label_counts = Counter()\n",
    "    for labels in train['parsed_labels']:\n",
    "        for label in labels:\n",
    "            label_counts[label] += 1\n",
    "    \n",
    "    # Sort labels by count for visualization\n",
    "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels_list = [item[0] for item in sorted_labels]\n",
    "    counts_list = [item[1] for item in sorted_labels]\n",
    "    \n",
    "    # Identify labels to remove\n",
    "    labels_to_remove = [label for label, count in label_counts.items() if count < min_samples]\n",
    "    labels_to_keep = [label for label, count in label_counts.items() if count >= min_samples]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LABEL DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal unique labels: {len(label_counts)}\")\n",
    "    print(f\"Minimum samples threshold: {min_samples}\")\n",
    "    print(f\"Labels to keep: {len(labels_to_keep)}\")\n",
    "    print(f\"Labels to remove: {len(labels_to_remove)}\")\n",
    "    \n",
    "    if labels_to_remove:\n",
    "        print(f\"\\nLabels being removed (< {min_samples} tweets):\")\n",
    "        for label in sorted(labels_to_remove, key=lambda x: label_counts[x], reverse=True):\n",
    "            print(f\"  - {label}: {label_counts[label]} tweets\")\n",
    "    \n",
    "    # Visualization with matplotlib\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Create colors based on whether label will be kept or removed\n",
    "        colors = ['#2ecc71' if count >= min_samples else '#e74c3c' for count in counts_list]\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(labels_list))\n",
    "        bars = ax.barh(y_pos, counts_list, color=colors, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axvline(x=min_samples, color='#3498db', linestyle='--', linewidth=2, \n",
    "                   label=f'Threshold ({min_samples} tweets)')\n",
    "        \n",
    "        # Customize chart\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(labels_list)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Number of Tweets', fontsize=12)\n",
    "        ax.set_ylabel('Label', fontsize=12)\n",
    "        ax.set_title('Label Distribution in Training Data\\n(Green: Keep, Red: Remove)', fontsize=14)\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        # Add count annotations\n",
    "        for i, (count, bar) in enumerate(zip(counts_list, bars)):\n",
    "            ax.annotate(f'{count}', xy=(count + 20, bar.get_y() + bar.get_height()/2),\n",
    "                       va='center', ha='left', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"\\nâœ“ Plot displayed\")\n",
    "    \n",
    "    # Define the original label order (from the dataset)\n",
    "    all_labels_ordered = [\n",
    "        'arts_&_culture', 'business_&_entrepreneurs', 'celebrity_&_pop_culture',\n",
    "        'diaries_&_daily_life', 'family', 'fashion_&_style', 'film_tv_&_video',\n",
    "        'fitness_&_health', 'food_&_dining', 'gaming', 'learning_&_educational',\n",
    "        'music', 'news_&_social_concern', 'other_hobbies', 'relationships',\n",
    "        'science_&_technology', 'sports', 'travel_&_adventure', 'youth_&_student_life'\n",
    "    ]\n",
    "    \n",
    "    # Get indices of labels to keep\n",
    "    keep_indices = [i for i, label in enumerate(all_labels_ordered) if label in labels_to_keep]\n",
    "    new_labels_ordered = [all_labels_ordered[i] for i in keep_indices]\n",
    "    \n",
    "    def filter_and_update_labels(df, labels_to_remove_set, keep_indices, new_labels_ordered):\n",
    "        \"\"\"\n",
    "        Filter out rows where all labels are in labels_to_remove and update label vectors.\n",
    "        \"\"\"\n",
    "        # Check if tweet has at least one label that will be kept\n",
    "        def has_valid_label(parsed_labels):\n",
    "            return any(label not in labels_to_remove_set for label in parsed_labels)\n",
    "        \n",
    "        # Filter rows\n",
    "        mask = df['parsed_labels'].apply(has_valid_label)\n",
    "        df_filtered = df[mask].copy()\n",
    "        \n",
    "        # Update label_name to only include kept labels\n",
    "        def update_label_names(parsed_labels):\n",
    "            kept = [label for label in parsed_labels if label not in labels_to_remove_set]\n",
    "            return str(kept).replace(', ', ' ').replace(',', '')\n",
    "        \n",
    "        df_filtered['label_name'] = df_filtered['parsed_labels'].apply(update_label_names)\n",
    "        \n",
    "        # Update label vectors - parse old vector and create new one with only kept indices\n",
    "        def update_label_vector(label_str):\n",
    "            # Parse the vector string like '[0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]'\n",
    "            label_str = str(label_str).strip()\n",
    "            if label_str.startswith('[') and label_str.endswith(']'):\n",
    "                values = label_str[1:-1].split()\n",
    "                values = [int(v) for v in values]\n",
    "                # Keep only the values at keep_indices\n",
    "                new_values = [values[i] for i in keep_indices]\n",
    "                return '[' + ' '.join(map(str, new_values)) + ']'\n",
    "            return label_str\n",
    "        \n",
    "        df_filtered['label'] = df_filtered['label'].apply(update_label_vector)\n",
    "        \n",
    "        # Drop the temporary parsed_labels column\n",
    "        df_filtered = df_filtered.drop(columns=['parsed_labels'])\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    labels_to_remove_set = set(labels_to_remove)\n",
    "    \n",
    "    # Apply filtering to all datasets\n",
    "    train_filtered = filter_and_update_labels(train, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    val_filtered = filter_and_update_labels(val, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    test_filtered = filter_and_update_labels(test, labels_to_remove_set, keep_indices, new_labels_ordered)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FILTERING RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"  Before: {len(train)} tweets\")\n",
    "    print(f\"  After:  {len(train_filtered)} tweets ({len(train) - len(train_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nValidation set:\")\n",
    "    print(f\"  Before: {len(val)} tweets\")\n",
    "    print(f\"  After:  {len(val_filtered)} tweets ({len(val) - len(val_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nTest set:\")\n",
    "    print(f\"  Before: {len(test)} tweets\")\n",
    "    print(f\"  After:  {len(test_filtered)} tweets ({len(test) - len(test_filtered)} removed)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Label vector size reduced from {len(all_labels_ordered)} to {len(new_labels_ordered)}\")\n",
    "    print(f\"\\nRemaining labels ({len(new_labels_ordered)}):\")\n",
    "    for label in new_labels_ordered:\n",
    "        print(f\"  - {label}\")\n",
    "    \n",
    "    return train_filtered, val_filtered, test_filtered, labels_to_remove\n",
    "\n",
    "\n",
    "# Apply the prepare_data_for_training function\n",
    "print(\"Loading preprocessed datasets...\\n\")\n",
    "\n",
    "# Load the preprocessed datasets (from multi_label folder)\n",
    "train_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_train.parquet')\n",
    "val_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_validation.parquet')\n",
    "test_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_test.parquet')\n",
    "\n",
    "print(f\"Loaded train: {len(train_df)} tweets\")\n",
    "print(f\"Loaded validation: {len(val_df)} tweets\")\n",
    "print(f\"Loaded test: {len(test_df)} tweets\\n\")\n",
    "\n",
    "# Prepare data for training (remove labels with < 500 tweets)\n",
    "train_filtered, val_filtered, test_filtered, removed_labels = prepare_data_for_training(\n",
    "    train_df, val_df, test_df, \n",
    "    min_samples=500, \n",
    "    show_plot=True\n",
    ")\n",
    "\n",
    "# Save the filtered datasets to multi_label folder\n",
    "train_filtered.to_parquet('../Data/multi_label/tweets_preprocessed_train.parquet', index=False)\n",
    "val_filtered.to_parquet('../Data/multi_label/tweets_preprocessed_validation.parquet', index=False)\n",
    "test_filtered.to_parquet('../Data/multi_label/tweets_preprocessed_test.parquet', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVED FILES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ“ Filtered datasets saved to:\")\n",
    "print(\"  - ../Data/multi_label/tweets_preprocessed_train.parquet\")\n",
    "print(\"  - ../Data/multi_label/tweets_preprocessed_validation.parquet\")\n",
    "print(\"  - ../Data/multi_label/tweets_preprocessed_test.parquet\")\n",
    "print(\"\\nâœ“ Data preparation for training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Single-Label Dataset Creation\n",
    "\n",
    "### 3.1 Intelligent Single-Label Assignment with Claude Haiku\n",
    "\n",
    "For training a Single-Label classifier (needed in Lab 5), we need to convert multi-label samples to single-label. Instead of simply taking the first label, we use **Claude Haiku** to intelligently decide which label best represents the tweet's main topic.\n",
    "\n",
    "**Strategy:**\n",
    "1. **Single-label tweets**: Keep the label as-is\n",
    "2. **Multi-label tweets**: Use Claude Haiku to analyze the tweet and select the most appropriate single label\n",
    "3. **Caching**: Results are cached to avoid redundant API calls and reduce costs\n",
    "\n",
    "**Output Files:**\n",
    "- `../Data/single_label/tweets_single_label_train.parquet`\n",
    "- `../Data/single_label/tweets_single_label_test.parquet`\n",
    "- `../Data/single_label/tweets_single_label_validation.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTELLIGENTE SINGLE-LABEL ZUWEISUNG MIT CLAUDE HAIKU\n",
    "# ============================================================\n",
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Cache-Pfad fÃ¼r API-Ergebnisse (spart Kosten bei erneutem AusfÃ¼hren)\n",
    "SINGLE_LABEL_CACHE_PATH = Path(\"../Data/single_label/single_label_cache.json\")\n",
    "\n",
    "# Anthropic Client initialisieren\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n",
    "\n",
    "if ANTHROPIC_API_KEY is None:\n",
    "    print(\"âš ï¸ ANTHROPIC_API_KEY nicht gefunden!\")\n",
    "    print(\"   Bitte setze die Umgebungsvariable oder gib den Key hier ein:\")\n",
    "    print(\"   export ANTHROPIC_API_KEY='dein-api-key'\")\n",
    "    USE_LLM = False\n",
    "else:\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    USE_LLM = True\n",
    "    print(\"âœ“ Anthropic Client initialisiert\")\n",
    "\n",
    "def parse_labels_for_single(value) -> List[str]:\n",
    "    \"\"\"Parse label_name column into consistent Python lists.\"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, tuple):\n",
    "        return [str(v) for v in value]\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.startswith('[') and value.endswith(']'):\n",
    "            inner = value[1:-1].strip()\n",
    "            if not inner:\n",
    "                return []\n",
    "            inner = inner.replace(\"'\", \"\").replace('\"', '')\n",
    "            labels = [l.strip() for l in inner.split() if l.strip()]\n",
    "            return labels\n",
    "        try:\n",
    "            parsed = ast.literal_eval(value)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return [str(v) for v in parsed]\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "        return [value] if value else []\n",
    "    return [str(value)] if value else []\n",
    "\n",
    "def load_single_label_cache() -> dict:\n",
    "    \"\"\"Lade gecachte Single-Label Entscheidungen.\"\"\"\n",
    "    if SINGLE_LABEL_CACHE_PATH.exists():\n",
    "        with open(SINGLE_LABEL_CACHE_PATH, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_single_label_cache(cache: dict):\n",
    "    \"\"\"Speichere Cache auf Disk.\"\"\"\n",
    "    os.makedirs(SINGLE_LABEL_CACHE_PATH.parent, exist_ok=True)\n",
    "    with open(SINGLE_LABEL_CACHE_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def get_single_label_cache_key(tweet_id: str) -> str:\n",
    "    \"\"\"Erstelle einen eindeutigen Cache-Key nur aus der Tweet-ID.\"\"\"\n",
    "    return hashlib.md5(str(tweet_id).encode()).hexdigest()\n",
    "\n",
    "def classify_with_haiku(tweet_id: str, text: str, labels: list, cache: dict) -> str:\n",
    "    \"\"\"\n",
    "    Verwende Claude Haiku um das passendste Label auszuwÃ¤hlen.\n",
    "    Mit Caching um API-Kosten zu minimieren.\n",
    "    \"\"\"\n",
    "    cache_key = get_single_label_cache_key(tweet_id)\n",
    "    \n",
    "    # PrÃ¼fe Cache\n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "    \n",
    "    # Falls LLM nicht verfÃ¼gbar, nimm erstes Label\n",
    "    if not USE_LLM:\n",
    "        return labels[0]\n",
    "    \n",
    "    # Erstelle Prompt\n",
    "    system_prompt = \"\"\"Du bist ein Experte fÃ¼r Tweet-Klassifizierung. \n",
    "Deine Aufgabe ist es, das EINE passendste Label fÃ¼r einen Tweet auszuwÃ¤hlen.\n",
    "Antworte NUR mit dem gewÃ¤hlten Label, nichts anderes.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Tweet: \"{text}\"\n",
    "\n",
    "MÃ¶gliche Labels: {', '.join(labels)}\n",
    "\n",
    "Welches Label passt am besten zu diesem Tweet? Antworte nur mit dem Label.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=50,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            system=system_prompt\n",
    "        )\n",
    "        \n",
    "        result = response.content[0].text.strip()\n",
    "        \n",
    "        # Validiere dass das Ergebnis ein gÃ¼ltiges Label ist\n",
    "        if result in labels:\n",
    "            cache[cache_key] = result\n",
    "            return result\n",
    "        else:\n",
    "            # Falls LLM ungÃ¼ltiges Label zurÃ¼ckgibt, finde beste Ãœbereinstimmung\n",
    "            for label in labels:\n",
    "                if label.lower() in result.lower():\n",
    "                    cache[cache_key] = label\n",
    "                    return label\n",
    "            # Fallback: erstes Label\n",
    "            cache[cache_key] = labels[0]\n",
    "            return labels[0]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return labels[0]  # Fallback\n",
    "\n",
    "def assign_single_labels_smart(df, cache: dict, desc: str = \"Processing\") -> list:\n",
    "    \"\"\"\n",
    "    Weise jedem Sample das beste Single-Label zu.\n",
    "    Verwendet LLM nur fÃ¼r Samples mit mehreren Labels.\n",
    "    \"\"\"\n",
    "    single_labels = []\n",
    "    api_calls = 0\n",
    "    cache_hits = 0\n",
    "    \n",
    "    # PrÃ¼fe ob 'id' Spalte existiert\n",
    "    has_id_column = 'id' in df.columns\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        labels = row['labels']\n",
    "        # Verwende 'id' Spalte falls vorhanden, sonst den Index\n",
    "        tweet_id = row['id'] if has_id_column else str(idx)\n",
    "        \n",
    "        if len(labels) == 1:\n",
    "            # Nur ein Label - keine Entscheidung nÃ¶tig\n",
    "            single_labels.append(labels[0])\n",
    "        else:\n",
    "            # Mehrere Labels - LLM entscheidet (oder Cache)\n",
    "            cache_key = get_single_label_cache_key(tweet_id)\n",
    "            if cache_key in cache:\n",
    "                cache_hits += 1\n",
    "            else:\n",
    "                api_calls += 1\n",
    "            \n",
    "            best_label = classify_with_haiku(tweet_id, row['text'], labels, cache)\n",
    "            single_labels.append(best_label)\n",
    "            \n",
    "            # Speichere Cache regelmÃ¤ÃŸig\n",
    "            if api_calls > 0 and api_calls % 100 == 0:\n",
    "                save_single_label_cache(cache)\n",
    "                print(f\"\\n  ðŸ’¾ Cache gespeichert ({api_calls} API calls, {cache_hits} cache hits)\")\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š Statistik: {api_calls} API calls, {cache_hits} cache hits, {len(df) - api_calls - cache_hits} single-label samples\")\n",
    "    return single_labels\n",
    "\n",
    "print(\"\\nâœ“ Single-Label Funktionen definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate or Load Single-Label Datasets\n",
    "\n",
    "Set `REGENERATE_SINGLE_LABELS = True` to regenerate the single-label datasets using Claude Haiku.\n",
    "If the files already exist and regeneration is disabled, they will be loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SINGLE-LABEL DATEN GENERIEREN ODER LADEN\n",
    "# ============================================================\n",
    "# Setze REGENERATE_SINGLE_LABELS = True um die LLM-Klassifizierung \n",
    "# erneut durchzufÃ¼hren und die Dateien zu aktualisieren.\n",
    "\n",
    "REGENERATE_SINGLE_LABELS = False  # â† Auf True setzen um LLM-Klassifizierung zu starten\n",
    "\n",
    "# Definiere Pfade\n",
    "SINGLE_LABEL_TRAIN_PATH = \"../Data/single_label/tweets_single_label_train.parquet\"\n",
    "SINGLE_LABEL_TEST_PATH = \"../Data/single_label/tweets_single_label_test.parquet\"\n",
    "SINGLE_LABEL_VALIDATION_PATH = \"../Data/single_label/tweets_single_label_validation.parquet\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SINGLE-LABEL DATASET ERSTELLUNG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Lade die gefilterten Multi-Label Daten\n",
    "print(\"\\nðŸ“‚ Lade gefilterte Multi-Label Daten...\")\n",
    "sl_train_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_train.parquet')\n",
    "sl_test_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_test.parquet')\n",
    "sl_val_df = pd.read_parquet('../Data/multi_label/tweets_preprocessed_validation.parquet')\n",
    "\n",
    "# Parse Labels\n",
    "sl_train_df['labels'] = sl_train_df['label_name'].apply(parse_labels_for_single)\n",
    "sl_test_df['labels'] = sl_test_df['label_name'].apply(parse_labels_for_single)\n",
    "sl_val_df['labels'] = sl_val_df['label_name'].apply(parse_labels_for_single)\n",
    "\n",
    "print(f\"âœ“ Training: {len(sl_train_df):,} Samples\")\n",
    "print(f\"âœ“ Test: {len(sl_test_df):,} Samples\")\n",
    "print(f\"âœ“ Validation: {len(sl_val_df):,} Samples\")\n",
    "\n",
    "# PrÃ¼fe ob gespeicherte Dateien existieren\n",
    "train_exists = Path(SINGLE_LABEL_TRAIN_PATH).exists()\n",
    "test_exists = Path(SINGLE_LABEL_TEST_PATH).exists()\n",
    "val_exists = Path(SINGLE_LABEL_VALIDATION_PATH).exists()\n",
    "all_files_exist = train_exists and test_exists and val_exists\n",
    "\n",
    "if not REGENERATE_SINGLE_LABELS and all_files_exist:\n",
    "    # ============================================================\n",
    "    # OPTION 1: Lade gespeicherte Single-Label Dateien\n",
    "    # ============================================================\n",
    "    print(\"\\nðŸ“‚ Lade gespeicherte Single-Label Dateien...\")\n",
    "    \n",
    "    sl_train_result = pd.read_parquet(SINGLE_LABEL_TRAIN_PATH)\n",
    "    sl_test_result = pd.read_parquet(SINGLE_LABEL_TEST_PATH)\n",
    "    sl_val_result = pd.read_parquet(SINGLE_LABEL_VALIDATION_PATH)\n",
    "    \n",
    "    print(f\"âœ“ Training Set geladen: {len(sl_train_result):,} Samples\")\n",
    "    print(f\"âœ“ Test Set geladen: {len(sl_test_result):,} Samples\")\n",
    "    print(f\"âœ“ Validation Set geladen: {len(sl_val_result):,} Samples\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Single-Label Verteilung (Training):\")\n",
    "    print(sl_train_result['single_label'].value_counts())\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Tipp: Setze REGENERATE_SINGLE_LABELS = True um die Dateien zu aktualisieren\")\n",
    "\n",
    "else:\n",
    "    # ============================================================\n",
    "    # OPTION 2: Generiere Single-Labels mit LLM\n",
    "    # ============================================================\n",
    "    if not all_files_exist:\n",
    "        print(\"\\nâš ï¸ Single-Label Dateien nicht gefunden - generiere mit LLM...\")\n",
    "    else:\n",
    "        print(\"\\nðŸ”„ REGENERATE_SINGLE_LABELS = True - generiere Single-Labels mit LLM...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INTELLIGENTE SINGLE-LABEL ZUWEISUNG MIT CLAUDE HAIKU\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Lade Cache\n",
    "    cache = load_single_label_cache()\n",
    "    print(f\"âœ“ Cache geladen: {len(cache)} EintrÃ¤ge\")\n",
    "    \n",
    "    # Konvertiere alle Datasets\n",
    "    print(\"\\nðŸ“Œ Training Set:\")\n",
    "    sl_train_df['single_label'] = assign_single_labels_smart(sl_train_df, cache, desc=\"Training Set\")\n",
    "    \n",
    "    print(\"\\nðŸ“Œ Test Set:\")\n",
    "    sl_test_df['single_label'] = assign_single_labels_smart(sl_test_df, cache, desc=\"Test Set\")\n",
    "    \n",
    "    print(\"\\nðŸ“Œ Validation Set:\")\n",
    "    sl_val_df['single_label'] = assign_single_labels_smart(sl_val_df, cache, desc=\"Validation Set\")\n",
    "    \n",
    "    # Speichere finalen Cache\n",
    "    save_single_label_cache(cache)\n",
    "    print(f\"\\nâœ“ Cache gespeichert: {len(cache)} EintrÃ¤ge\")\n",
    "    \n",
    "    # Statistiken anzeigen\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SINGLE-LABEL VERTEILUNG (Training Set)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(sl_train_df['single_label'].value_counts())\n",
    "    \n",
    "    # Vergleich: Original erstes Label vs. LLM-Auswahl\n",
    "    if USE_LLM:\n",
    "        original_first = sl_train_df['labels'].apply(lambda x: x[0] if x else None)\n",
    "        llm_selected = sl_train_df['single_label']\n",
    "        changed = (original_first != llm_selected).sum()\n",
    "        print(f\"\\nâœ“ LLM hat {changed:,} Labels anders gewÃ¤hlt als 'erstes Label' ({100*changed/len(sl_train_df):.1f}%)\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # SPEICHERE SINGLE-LABEL DATEN\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SPEICHERE SINGLE-LABEL DATASETS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Erstelle Ordner falls nicht vorhanden\n",
    "    os.makedirs('../Data/single_label', exist_ok=True)\n",
    "    \n",
    "    # Speichere Training Set\n",
    "    sl_train_df.to_parquet(SINGLE_LABEL_TRAIN_PATH, index=False)\n",
    "    print(f\"âœ“ Training Set gespeichert: {SINGLE_LABEL_TRAIN_PATH}\")\n",
    "    \n",
    "    # Speichere Test Set\n",
    "    sl_test_df.to_parquet(SINGLE_LABEL_TEST_PATH, index=False)\n",
    "    print(f\"âœ“ Test Set gespeichert: {SINGLE_LABEL_TEST_PATH}\")\n",
    "    \n",
    "    # Speichere Validation Set\n",
    "    sl_val_df.to_parquet(SINGLE_LABEL_VALIDATION_PATH, index=False)\n",
    "    print(f\"âœ“ Validation Set gespeichert: {SINGLE_LABEL_VALIDATION_PATH}\")\n",
    "    \n",
    "    sl_train_result = sl_train_df\n",
    "    sl_test_result = sl_test_df\n",
    "    sl_val_result = sl_val_df\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ“ Single-Label Datasets erstellt/geladen!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“Š Output Dateien:\")\n",
    "print(f\"  - {SINGLE_LABEL_TRAIN_PATH}\")\n",
    "print(f\"  - {SINGLE_LABEL_TEST_PATH}\")\n",
    "print(f\"  - {SINGLE_LABEL_VALIDATION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
