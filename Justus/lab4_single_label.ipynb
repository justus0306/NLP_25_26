{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6b8ff8",
   "metadata": {},
   "source": [
    "# Lab 4: Text Classification\n",
    "\n",
    "---\n",
    "## 1. Notebook Overview\n",
    "\n",
    "### 1.1 Focus of this notebook\n",
    "We revisit the preprocessed tweets from Lab 2 and perform token analysis and classification:\n",
    "\n",
    "1. Load the dataset and normalise the label lists.\n",
    "2.  Derive the 1000 most frequent tokens and save them for later use.\n",
    "3. Train a Naive Bayes classifier for multi-label classification.\n",
    "\n",
    "### 1.2 Dataset\n",
    "- Source: `../Data/tweets_preprocessed_train.parquet` (output from Lab 2)\n",
    "- Columns: `text` (whitespace-tokenised strings), `label_name` (list of categories), `label` (binary vector)\n",
    "\n",
    "### 1.3 Output\n",
    "- `../Data/top_1000_vocabulary.json` - The top 1000 tokens for use in Lab 5\n",
    "\n",
    "### 1.4 Section overview\n",
    "1. `Section 2` – Load/prepare the data frame.\n",
    "2. `Section 3` – Reuse the Lab 3 helper classes (`UnigramLM`).\n",
    "3. `Section 4` – Compute the top 1000 tokens and save to file.\n",
    "4. `Section 5` – Train Naive Bayes classifier.\n",
    "5. `Section 6` – Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a969f",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "### 2.1 Goal\n",
    "Load the preprocessed training and test data from Lab 2 and standardise the label column.\n",
    "\n",
    "### 2.2 Steps\n",
    "1. Import libraries (Pandas, NumPy, collections helper).\n",
    "2. Load training data from `../Data/tweets_preprocessed_train.parquet`.\n",
    "3. Load test data from `../Data/tweets_preprocessed_test.parquet`.\n",
    "4. Convert `label_name` into consistent Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27917adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,090 documents from ../Data/tweets_preprocessed_train.parquet. \n",
      "Columns: ['text', 'label_name', 'label', 'labels', 'label_count', 'primary_label']\n",
      "                                                text  label_name  \\\n",
      "0  lumber beat rapid game western division final ...  ['sports']   \n",
      "1         hear eli gold announce auburn game dumbass  ['sports']   \n",
      "2       phone away try look home game ticket october  ['sports']   \n",
      "\n",
      "                                               label    labels  label_count  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [sports]            1   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [sports]            1   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [sports]            1   \n",
      "\n",
      "  primary_label  \n",
      "0        sports  \n",
      "1        sports  \n",
      "2        sports  \n",
      "\n",
      "Single-label subset: 6,089 rows\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Updated path to match Lab 2 output\n",
    "DATA_PATH = \"../Data/tweets_preprocessed_train.parquet\"\n",
    "TEST_DATA_PATH = \"../Data/tweets_preprocessed_test.parquet\"\n",
    "VOCABULARY_OUTPUT_PATH = \"../Data/top_1000_vocabulary.json\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalise the label column.\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    def parse_labels(value) -> List[str]:\n",
    "        if isinstance(value, list):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, tuple):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(v) for v in parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                return [value]\n",
    "        return [str(value)]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_count\"] = df[\"labels\"].apply(len)\n",
    "    df[\"primary_label\"] = df[\"labels\"].apply(lambda items: items[0] if items else \"unknown\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df_raw = load_dataset(DATA_PATH)\n",
    "print(f\"Loaded {len(df_raw):,} documents from {DATA_PATH}. \")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(df_raw.head(3))\n",
    "\n",
    "# Create single-label subset for vocabulary extraction\n",
    "single_label_df = df_raw[df_raw[\"label_count\"] == 1][[\"text\", \"primary_label\"]]. rename(\n",
    "    columns={\"primary_label\": \"label\"}\n",
    ")\n",
    "print(f\"\\nSingle-label subset: {len(single_label_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,679 test documents from ../Data/tweets_preprocessed_test.parquet\n",
      "Training set: 6,090 samples\n",
      "Test set: 1,679 samples\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed test data from Lab 2\n",
    "df_test = load_dataset(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(df_test):,} test documents from {TEST_DATA_PATH}\")\n",
    "\n",
    "# Use all training data for training\n",
    "X_train = df_raw[\"text\"]\n",
    "y_train = df_raw[\"labels\"]\n",
    "\n",
    "# Use preprocessed test data for evaluation\n",
    "X_test = df_test[\"text\"]\n",
    "y_test = df_test[\"labels\"]\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118d7e",
   "metadata": {},
   "source": [
    "## 3.  Reusing Language-Model Helpers (Lab 3)\n",
    "\n",
    "### 3.1 Background\n",
    "`lab3.ipynb` defined a `UnigramLM` class that counts token frequencies and computes Laplace-smoothed log probabilities. We reuse the same implementation here to keep the logic consistent.\n",
    "\n",
    "### 3.2 How it works\n",
    "- `ensure_tokens` converts strings to token lists.\n",
    "- `UnigramLM` aggregates token counts (`self.unigram_counts`) across the corpus.\n",
    "- Calling `. unigram_counts. most_common(n)` returns the top-n tokens along with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c9d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Sequence, Union\n",
    "import math\n",
    "\n",
    "\n",
    "def ensure_tokens(sentence: Union[Sequence[str], str]) -> List[str]:\n",
    "    \"\"\"Convert whitespace-separated text or token sequences into a list.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "class UnigramLM:\n",
    "    \"\"\"Laplace-smoothed unigram language model operating in log-space.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: Sequence[Sequence[str]]):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            tokens = ensure_tokens(sentence)\n",
    "            self.unigram_counts.update(tokens)\n",
    "            self.total_tokens += len(tokens)\n",
    "            self. vocab.update(tokens)\n",
    "\n",
    "        if self.total_tokens == 0:\n",
    "            raise ValueError(\"Cannot train a UnigramLM on an empty corpus. \")\n",
    "\n",
    "        self.vocab_size = len(self. vocab)\n",
    "\n",
    "    def log_prob(self, word: str) -> float:\n",
    "        count = self.unigram_counts.get(word, 0)\n",
    "        return math. log((count + 1) / (self.total_tokens + self.vocab_size))\n",
    "\n",
    "    def sentence_log_prob(self, sentence: Union[Sequence[str], str]) -> float:\n",
    "        tokens = ensure_tokens(sentence)\n",
    "        if not tokens:\n",
    "            return float('-inf')\n",
    "        return sum(self.log_prob(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072f95e",
   "metadata": {},
   "source": [
    "## 4. Task – Top 1000 Tokens\n",
    "\n",
    "### 4. 1 Goal\n",
    "Identify the most frequent tokens in the preprocessed corpus and save them for use in Lab 5 (Neural Network).\n",
    "\n",
    "### 4. 2 Approach\n",
    "1. Train the `UnigramLM` on the single-label subset.\n",
    "2. Retrieve `most_common(1000)` and inspect the first items.\n",
    "3. Save the vocabulary to `../Data/top_1000_vocabulary.json`.\n",
    "4. Optionally preview top tokens per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa338605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected top 1000 tokens (showing the first 20):\n",
      "  new             -> 598\n",
      "  day             -> 515\n",
      "  love            -> 514\n",
      "  good            -> 453\n",
      "  game            -> 439\n",
      "  year            -> 414\n",
      "  time            -> 401\n",
      "  watch           -> 385\n",
      "  happy           -> 361\n",
      "  music           -> 346\n",
      "  come            -> 330\n",
      "  like            -> 322\n",
      "  win             -> 311\n",
      "  live            -> 307\n",
      "  thank           -> 303\n",
      "  great           -> 297\n",
      "  go              -> 295\n",
      "  video           -> 287\n",
      "  play            -> 277\n",
      "  world           -> 268\n",
      "\n",
      "Per-class token preview (Top 10 tokens for the most frequent labels):\n",
      "- sports (1181 docs): game (255), win (180), team (147), ufc (111), good (108), today (91), go (85), time (85), vs (84), final (82)\n",
      "- news_&_social_concern (625 docs): trump (100), president (77), news (60), people (56), black (49), change (45), world (45), woman (44), year (42), day (42)\n",
      "- music (439 docs): new (151), music (148), album (111), song (83), live (58), video (56), listen (56), love (54), play (48), spotify (37)\n",
      "\n",
      "Vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 1000\n",
    "\n",
    "# Build vocabulary from single-label subset\n",
    "corpus_tokens = [ensure_tokens(text) for text in single_label_df[\"text\"]]\n",
    "unigram_model = UnigramLM(corpus_tokens)\n",
    "\n",
    "# Extract top 1000 tokens\n",
    "top_unigrams = unigram_model.unigram_counts.most_common(MAX_FEATURES)\n",
    "TOP_VOCABULARY = [token for token, _ in top_unigrams]\n",
    "\n",
    "print(f\"Collected top {len(top_unigrams)} tokens (showing the first 20):\")\n",
    "for token, freq in top_unigrams[:20]:\n",
    "    print(f\"  {token:<15} -> {freq}\")\n",
    "\n",
    "# Per-class token preview for the three most frequent labels\n",
    "label_counts = single_label_df[\"label\"].value_counts(). head(3)\n",
    "print(\"\\nPer-class token preview (Top 10 tokens for the most frequent labels):\")\n",
    "for label, count in label_counts.items():\n",
    "    label_corpus = [ensure_tokens(text) for text in single_label_df. loc[single_label_df[\"label\"] == label, \"text\"]]\n",
    "    label_model = UnigramLM(label_corpus)\n",
    "    label_top = label_model.unigram_counts. most_common(10)\n",
    "    formatted = \", \".join([f\"{tok} ({freq})\" for tok, freq in label_top])\n",
    "    print(f\"- {label} ({count} docs): {formatted}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(TOP_VOCABULARY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "save_vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vocabulary saved to: ../Data/top_1000_vocabulary.json\n",
      "✓ Contains 1000 tokens\n",
      "✓ First 10 tokens: ['new', 'day', 'love', 'good', 'game', 'year', 'time', 'watch', 'happy', 'music']\n"
     ]
    }
   ],
   "source": [
    "# Save the top 1000 vocabulary to JSON file for use in Lab 5\n",
    "os.makedirs(os.path.dirname(VOCABULARY_OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "vocabulary_data = {\n",
    "    \"description\": \"Top 1000 most frequent tokens from preprocessed tweets (Lab 4)\",\n",
    "    \"source\": DATA_PATH,\n",
    "    \"count\": len(TOP_VOCABULARY),\n",
    "    \"tokens\": TOP_VOCABULARY\n",
    "}\n",
    "\n",
    "with open(VOCABULARY_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json. dump(vocabulary_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Vocabulary saved to: {VOCABULARY_OUTPUT_PATH}\")\n",
    "print(f\"✓ Contains {len(TOP_VOCABULARY)} tokens\")\n",
    "print(f\"✓ First 10 tokens: {TOP_VOCABULARY[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207133",
   "metadata": {},
   "source": [
    "## 5.  Naive Bayes Classification\n",
    "\n",
    "### 5. 1 Goal\n",
    "Train a Naive Bayes classifier using the top 1000 vocabulary as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5533d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix (Train): (6090, 1000)\n",
      "Feature matrix (Test): (1679, 1000)\n",
      "Sample features: ['new' 'day' 'love' 'good' 'game' 'year' 'time' 'watch' 'happy' 'music']\n"
     ]
    }
   ],
   "source": [
    "from sklearn. feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use the top 1000 vocabulary for feature extraction\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=TOP_VOCABULARY,\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "\n",
    "# Transform text to Bag-of-Words\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.fit_transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix (Train): {X_train_bow.shape}\")\n",
    "print(f\"Feature matrix (Test): {X_test_bow.shape}\")\n",
    "print(f\"Sample features: {vectorizer.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f18d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 449\n",
      "Classes: ['arts_&_culture' 'arts_&_culturebusiness_&_entrepreneurs'\n",
      " 'arts_&_culturebusiness_&_entrepreneurscelebrity_&_pop_culturefilm_tv_&_video'\n",
      " 'arts_&_culturebusiness_&_entrepreneursdiaries_&_daily_life'\n",
      " 'arts_&_culturebusiness_&_entrepreneursdiaries_&_daily_lifefamilyrelationships'\n",
      " 'arts_&_culturebusiness_&_entrepreneursfilm_tv_&_video'\n",
      " 'arts_&_culturebusiness_&_entrepreneursfood_&_dining'\n",
      " 'arts_&_culturecelebrity_&_pop_culture'\n",
      " 'arts_&_culturecelebrity_&_pop_culturediaries_&_daily_lifefilm_tv_&_video'\n",
      " 'arts_&_culturecelebrity_&_pop_culturediaries_&_daily_lifemusic']... \n",
      "y_train shape: (6090, 449)\n",
      "y_test shape: (1679, 449)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['arts_&_culturelearning_&_educationalrelationships', 'arts_&_culturenews_&_social_concernother_hobbies', 'business_&_entrepreneursfashion_&_stylenews_&_social_concern', 'business_&_entrepreneursnews_&_social_concernother_hobbies', 'celebrity_&_pop_culturefashion_&_stylefilm_tv_&_videomusicnews_&_social_concern', 'celebrity_&_pop_culturefashion_&_stylemusicnews_&_social_concern', 'celebrity_&_pop_culturefitness_&_healthmusicother_hobbies', 'celebrity_&_pop_culturefitness_&_healthrelationships', 'celebrity_&_pop_culturelearning_&_educational', 'celebrity_&_pop_cultureother_hobbiesrelationships', 'diaries_&_daily_lifefamilylearning_&_educationalrelationships', 'diaries_&_daily_lifefashion_&_stylenews_&_social_concern', 'diaries_&_daily_lifefitness_&_healthnews_&_social_concernrelationships', 'familylearning_&_educationalyouth_&_student_life', 'film_tv_&_videofood_&_diningmusic', 'film_tv_&_videolearning_&_educationalyouth_&_student_life', 'film_tv_&_videomusicother_hobbies', 'fitness_&_healthgamingsports', 'fitness_&_healthlearning_&_educationalsportsyouth_&_student_life', 'fitness_&_healthnews_&_social_concernscience_&_technologyyouth_&_student_life', 'food_&_diningmusicother_hobbies', 'musicnews_&_social_concerntravel_&_adventure', 'science_&_technologytravel_&_adventure'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Multi-Label encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train)\n",
    "y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "print(f\"Number of classes: {len(mlb.classes_)}\")\n",
    "print(f\"Classes: {mlb. classes_[:10]}... \")\n",
    "print(f\"y_train shape: {y_train_bin.shape}\")\n",
    "print(f\"y_test shape: {y_test_bin.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a093478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Label Naive Bayes model trained! \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Train Multi-Label Naive Bayes\n",
    "nb_clf = OneVsRestClassifier(MultinomialNB(alpha=1.0))\n",
    "nb_clf.fit(X_train_bow, y_train_bin)\n",
    "\n",
    "print(\"✅ Multi-Label Naive Bayes model trained! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b653f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "Text: philadelphia clearly page game playbook fire net oppose goalie beat mi...\n",
      "→ Predicted labels: ('sports',)\n",
      "\n",
      "Text: sure bay face flyer man experience versus blue jacket year help lot ve...\n",
      "→ Predicted labels: ('sports',)\n",
      "\n",
      "Text: tizamagician put cherry kentucky derby day winner pie take del mar fin...\n",
      "→ Predicted labels: ('sports',)\n",
      "\n",
      "Text: flyer give false hope absolutely destroy islander go to destroy real t...\n",
      "→ Predicted labels: ('sports',)\n",
      "\n",
      "Text: flyer tremendous season face excited season go to well thank unforgett...\n",
      "→ Predicted labels: ('sports',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample predictions\n",
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for text, pred in zip(X_test[:5], y_pred_labels[:5]):\n",
    "    print(f\"Text: {text[:70]}...\")\n",
    "    print(f\"→ Predicted labels: {pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169817fd",
   "metadata": {},
   "source": [
    "## 6.  Evaluation\n",
    "\n",
    "Evaluate the Naive Bayes classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23fe5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NAIVE BAYES EVALUATION (Test Set)\n",
      "============================================================\n",
      "Subset Accuracy     : 0.2662\n",
      "Hamming Loss        : 0.0024\n",
      "Micro F1            : 0.4246\n",
      "Macro F1            : 0.0168\n",
      "Micro Precision     : 0.4562\n",
      "Micro Recall        : 0.3971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss\n",
    "\n",
    "# Predictions\n",
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "\n",
    "# Metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"NAIVE BAYES EVALUATION (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Subset Accuracy':<20}: {accuracy_score(y_test_bin, y_pred_bin):.4f}\")\n",
    "print(f\"{'Hamming Loss':<20}: {hamming_loss(y_test_bin, y_pred_bin):.4f}\")\n",
    "print(f\"{'Micro F1':<20}: {f1_score(y_test_bin, y_pred_bin, average='micro', zero_division=0):.4f}\")\n",
    "print(f\"{'Macro F1':<20}: {f1_score(y_test_bin, y_pred_bin, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"{'Micro Precision':<20}: {precision_score(y_test_bin, y_pred_bin, average='micro', zero_division=0):.4f}\")\n",
    "print(f\"{'Micro Recall':<20}: {recall_score(y_test_bin, y_pred_bin, average='micro', zero_division=0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sample_comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed sample predictions:\n",
      "\n",
      "✗ Sample 1:\n",
      "   Text: philadelphia clearly page game playbook fire net oppose goalie beat minute leave...\n",
      "   True: ('gamingnews_&_social_concernsports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✓ Sample 2:\n",
      "   Text: sure bay face flyer man experience versus blue jacket year help lot versus islan...\n",
      "   True: ('sports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✗ Sample 3:\n",
      "   Text: tizamagician put cherry kentucky derby day winner pie take del mar finale richar...\n",
      "   True: ('news_&_social_concernsports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✗ Sample 4:\n",
      "   Text: flyer give false hope absolutely destroy islander go to destroy real team series...\n",
      "   True: ('news_&_social_concernsports',)\n",
      "   Pred: ('sports',)\n",
      "\n",
      "✗ Sample 5:\n",
      "   Text: flyer tremendous season face excited season go to well thank unforgettable seaso...\n",
      "   True: ('news_&_social_concernsports',)\n",
      "   Pred: ('sports',)\n"
     ]
    }
   ],
   "source": [
    "# Detailed sample comparison\n",
    "y_true_labels = mlb.inverse_transform(y_test_bin)\n",
    "\n",
    "print(\"\\nDetailed sample predictions:\")\n",
    "for i, (text, true, pred) in enumerate(zip(X_test[:5], y_true_labels[:5], y_pred_labels[:5])):\n",
    "    match = \"✓\" if set(true) == set(pred) else \"✗\"\n",
    "    print(f\"\\n{match} Sample {i+1}:\")\n",
    "    print(f\"   Text: {text[:80]}...\")\n",
    "    print(f\"   True: {true}\")\n",
    "    print(f\"   Pred: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7.  Summary\n",
    "\n",
    "### What was accomplished\n",
    "1. Loaded preprocessed data from Lab 2\n",
    "2. Extracted the top 1000 most frequent tokens\n",
    "3. **Saved vocabulary to `../Data/top_1000_vocabulary.json`** for use in Lab 5\n",
    "4.  Trained a Naive Bayes multi-label classifier\n",
    "5. Evaluated performance on test set\n",
    "\n",
    "### Files created\n",
    "- `../Data/top_1000_vocabulary.json` - Contains the top 1000 tokens for feature extraction in Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LAB 4 SUMMARY\n",
      "============================================================\n",
      "Input: ../Data/tweets_preprocessed_train.parquet\n",
      "Output: ../Data/top_1000_vocabulary.json\n",
      "Vocabulary size: 1000\n",
      "Training samples: 6,090\n",
      "Test samples: 1,679\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LAB 4 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input: {DATA_PATH}\")\n",
    "print(f\"Output: {VOCABULARY_OUTPUT_PATH}\")\n",
    "print(f\"Vocabulary size: {len(TOP_VOCABULARY)}\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
