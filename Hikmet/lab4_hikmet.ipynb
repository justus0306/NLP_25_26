{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6b8ff8",
   "metadata": {},
   "source": [
    "# Text Classification Lab (Hikmet)\n",
    "\n",
    "---\n",
    "## 1. Notebook Orientation\n",
    "\n",
    "### 1.1 Focus of this notebook\n",
    "We revisit the preprocessed tweets from Lab 3 and limit ourselves to the token analysis stage:\n",
    "\n",
    "1. Load the dataset and normalise the label lists.\n",
    "2. Derive the 1000 most frequent tokens, with optional per-class previews.\n",
    "\n",
    "Later tasks (training a Naive Bayes classifier, evaluating it) remain intentionally open and appear only as placeholders.\n",
    "\n",
    "### 1.2 Dataset\n",
    "- Source: `../Data/df_preprocessed.parquet`\n",
    "- Columns: `text` (whitespace-tokenised strings) and `label_name` (list of categories)\n",
    "\n",
    "### 1.3 Section overview\n",
    "1. **Section 2** – Load/prepare the data frame.\n",
    "2. **Section 3** – Reuse the Lab 3 helper classes (`UnigramLM`).\n",
    "3. **Section 4** – Compute the top 1000 tokens globally and preview them per class.\n",
    "4. **Sections 5 & 6** – Placeholders for future classification steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a969f",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "### 2.1 Goal\n",
    "Load the preprocessed tweets, standardise the label column, and create a single-label view that can act as training data later on.\n",
    "\n",
    "### 2.2 Steps\n",
    "1. Import libraries (Pandas, NumPy, collections helper).\n",
    "2. Convert `label_name` into consistent Python lists.\n",
    "3. Build a DataFrame with a `label` column for single-label examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27917adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,090 documents from ../Data/df_preprocessed.parquet.\n",
      "                                                text  label_name    labels  \\\n",
      "0  beat rapid game western division final evan ed...  ['sports']  [sports]   \n",
      "1         hear eli gold announce auburn game dumbass  ['sports']  [sports]   \n",
      "2       phone away try look home game ticket october  ['sports']  [sports]   \n",
      "\n",
      "   label_count primary_label  \n",
      "0            1        sports  \n",
      "1            1        sports  \n",
      "2            1        sports  \n",
      "Single-label subset: 6,089 rows (label column = 'label').\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"../Data/df_preprocessed.parquet\"\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalise the label column.\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    def parse_labels(value) -> List[str]:\n",
    "        if isinstance(value, list):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, tuple):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(v) for v in parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                return [value]\n",
    "        return [str(value)]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_count\"] = df[\"labels\"].apply(len)\n",
    "    df[\"primary_label\"] = df[\"labels\"].apply(lambda items: items[0] if items else \"unknown\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_raw = load_dataset(DATA_PATH)\n",
    "print(f\"Loaded {len(df_raw):,} documents from {DATA_PATH}.\")\n",
    "print(df_raw.head(3))\n",
    "\n",
    "single_label_df = df_raw[df_raw[\"label_count\"] == 1][[\"text\", \"primary_label\"]].rename(\n",
    "    columns={\"primary_label\": \"label\"}\n",
    ")\n",
    "print(f\"Single-label subset: {len(single_label_df):,} rows (label column = 'label').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118d7e",
   "metadata": {},
   "source": [
    "## 3. Reusing Language-Model Helpers (Lab 3)\n",
    "\n",
    "### 3.1 Background\n",
    "`lab3_sunny.ipynb` defined a `UnigramLM` class that counts token frequencies and computes Laplace-smoothed log probabilities. We reuse the same implementation here to keep the logic consistent across notebooks.\n",
    "\n",
    "### 3.2 How it works\n",
    "- `ensure_tokens` converts strings to token lists.\n",
    "- `UnigramLM` aggregates token counts (`self.unigram_counts`) across the corpus.\n",
    "- Calling `.unigram_counts.most_common(n)` returns the top-n tokens along with their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c9d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Sequence, Union\n",
    "import math\n",
    "\n",
    "\n",
    "def ensure_tokens(sentence: Union[Sequence[str], str]) -> List[str]:\n",
    "    \"\"\"Convert whitespace-separated text or token sequences into a list.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "class UnigramLM:\n",
    "    \"\"\"Laplace-smoothed unigram language model operating in log-space.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: Sequence[Sequence[str]]):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            tokens = ensure_tokens(sentence)\n",
    "            self.unigram_counts.update(tokens)\n",
    "            self.total_tokens += len(tokens)\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        if self.total_tokens == 0:\n",
    "            raise ValueError(\"Cannot train a UnigramLM on an empty corpus.\")\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def log_prob(self, word: str) -> float:\n",
    "        count = self.unigram_counts.get(word, 0)\n",
    "        return math.log((count + 1) / (self.total_tokens + self.vocab_size))\n",
    "\n",
    "    def sentence_log_prob(self, sentence: Union[Sequence[str], str]) -> float:\n",
    "        tokens = ensure_tokens(sentence)\n",
    "        if not tokens:\n",
    "            return float('-inf')\n",
    "        return sum(self.log_prob(token) for token in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072f95e",
   "metadata": {},
   "source": [
    "## 4. Task – Top 1000 Tokens\n",
    "\n",
    "### 4.1 Goal\n",
    "Identify the most frequent tokens in the corpus (with optional class-wise previews) and store them for later feature engineering.\n",
    "\n",
    "### 4.2 Approach\n",
    "1. Train the `UnigramLM` on the single-label subset.\n",
    "2. Retrieve `most_common(1000)` and inspect the first items.\n",
    "3. Optionally repeat the process for the most frequent classes to understand their characteristic vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa338605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected top 1000 tokens (showing the first 20):\n",
      "  new             -> 571\n",
      "  love            -> 499\n",
      "  day             -> 466\n",
      "  good            -> 431\n",
      "  game            -> 427\n",
      "  make            -> 412\n",
      "  year            -> 405\n",
      "  time            -> 394\n",
      "  watch           -> 383\n",
      "  happy           -> 344\n",
      "  come            -> 329\n",
      "  music           -> 319\n",
      "  like            -> 318\n",
      "  win             -> 307\n",
      "  great           -> 295\n",
      "  thank           -> 292\n",
      "  go              -> 292\n",
      "  video           -> 275\n",
      "  live            -> 272\n",
      "  today           -> 261\n",
      "\n",
      "Per-class token preview (Top 10 tokens for the most frequent labels):\n",
      "- sports (1181 docs): game (248), win (178), team (143), ufc (110), good (107), today (91), go (85), vs (83), time (82), make (81)\n",
      "- news_&_social_concern (625 docs): trump (97), president (76), news (57), people (55), world (44), woman (42), change (42), year (42), know (41), black (41)\n",
      "- music (439 docs): new (145), music (137), album (111), song (83), love (53), listen (52), video (51), live (44), spotify (37), check (35)\n",
      "\n",
      "Stored vocabulary length: 1000\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 1000\n",
    "\n",
    "# Gesamtvokabular\n",
    "corpus_tokens = [ensure_tokens(text) for text in single_label_df[\"text\"]]\n",
    "unigram_model = UnigramLM(corpus_tokens)\n",
    "\n",
    "top_unigrams = unigram_model.unigram_counts.most_common(MAX_FEATURES)\n",
    "print(f\"Collected top {len(top_unigrams)} tokens (showing the first 20):\")\n",
    "for token, freq in top_unigrams[:20]:\n",
    "    print(f\"  {token:<15} -> {freq}\")\n",
    "\n",
    "# Optional: per class preview for the three most frequent labels\n",
    "label_counts = single_label_df[\"label\"].value_counts().head(3)\n",
    "print(\"\\nPer-class token preview (Top 10 tokens for the most frequent labels):\")\n",
    "for label, count in label_counts.items():\n",
    "    label_corpus = [ensure_tokens(text) for text in single_label_df.loc[single_label_df[\"label\"] == label, \"text\"]]\n",
    "    label_model = UnigramLM(label_corpus)\n",
    "    label_top = label_model.unigram_counts.most_common(10)\n",
    "    formatted = \", \".join([f\"{tok} ({freq})\" for tok, freq in label_top])\n",
    "    print(f\"- {label} ({count} docs): {formatted}\")\n",
    "\n",
    "# Speichern des Vokabulars für spätere Schritte (falls benötigt)\n",
    "TOP_VOCABULARY = [token for token, _ in top_unigrams]\n",
    "print(f\"\\nStored vocabulary length: {len(TOP_VOCABULARY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207133",
   "metadata": {},
   "source": [
    "## 5. Task – Naive Bayes Setup (Placeholder)\n",
    "\n",
    "> To be added later: build the pipeline, split the data, and train the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169817fd",
   "metadata": {},
   "source": [
    "## 6. Task – Evaluation & Error Analysis (Placeholder)\n",
    "\n",
    "> Once a classifier is trained, we will add metrics and example analyses here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
