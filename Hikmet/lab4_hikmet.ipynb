{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6b8ff8",
   "metadata": {},
   "source": [
    "# Text Classification Lab (Hikmet)\n",
    "\n",
    "---\n",
    "## 1. Notebook Orientation\n",
    "\n",
    "### 1.1 Focus of this notebook\n",
    "We revisit the preprocessed tweets from Lab 3 and limit ourselves to the token analysis stage:\n",
    "\n",
    "1. Load the dataset and normalise the label lists.\n",
    "2. Derive the 1000 most frequent tokens, with optional per-class previews.\n",
    "\n",
    "Later tasks (training a Naive Bayes classifier, evaluating it) remain intentionally open and appear only as placeholders.\n",
    "\n",
    "### 1.2 Dataset\n",
    "- Source: `../Data/df_preprocessed.parquet`\n",
    "- Columns: `text` (whitespace-tokenised strings) and `label_name` (list of categories)\n",
    "\n",
    "### 1.3 Section overview\n",
    "1. **Section 2** – Load/prepare the data frame.\n",
    "2. **Section 3** – Reuse the Lab 3 helper classes (`UnigramLM`).\n",
    "3. **Section 4** – Compute the top 1000 tokens globally and preview them per class.\n",
    "4. **Sections 5 & 6** – Placeholders for future classification steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a969f",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "### 2.1 Goal\n",
    "Load the preprocessed tweets, standardise the label column, and create a single-label view that can act as training data later on.\n",
    "\n",
    "### 2.2 Steps\n",
    "1. Import libraries (Pandas, NumPy, collections helper).\n",
    "2. Convert `label_name` into consistent Python lists.\n",
    "3. Build a DataFrame with a `label` column for single-label examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27917adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/df_preprocessed.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m items: items[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m items \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 36\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m load_dataset(DATA_PATH)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_raw)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_raw\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load tweets from parquet and normalise the label column.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(path)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_labels\u001b[39m(value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/df_preprocessed.parquet'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"../Data/df_preprocessed.parquet\"\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load tweets from parquet and normalise the label column.\"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    def parse_labels(value) -> List[str]:\n",
    "        if isinstance(value, list):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, tuple):\n",
    "            return [str(v) for v in value]\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(v) for v in parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                return [value]\n",
    "        return [str(value)]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = df[\"label_name\"].apply(parse_labels)\n",
    "    df[\"label_count\"] = df[\"labels\"].apply(len)\n",
    "    df[\"primary_label\"] = df[\"labels\"].apply(lambda items: items[0] if items else \"unknown\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_raw = load_dataset(DATA_PATH)\n",
    "print(f\"Loaded {len(df_raw):,} documents from {DATA_PATH}.\")\n",
    "print(df_raw.head(3))\n",
    "\n",
    "single_label_df = df_raw[df_raw[\"label_count\"] == 1][[\"text\", \"primary_label\"]].rename(\n",
    "    columns={\"primary_label\": \"label\"}\n",
    ")\n",
    "print(f\"Single-label subset: {len(single_label_df):,} rows (label column = 'label').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118d7e",
   "metadata": {},
   "source": [
    "## 3. Reusing Language-Model Helpers (Lab 3)\n",
    "\n",
    "### 3.1 Background\n",
    "`lab3_sunny.ipynb` defined a `UnigramLM` class that counts token frequencies and computes Laplace-smoothed log probabilities. We reuse the same implementation here to keep the logic consistent across notebooks.\n",
    "\n",
    "### 3.2 How it works\n",
    "- `ensure_tokens` converts strings to token lists.\n",
    "- `UnigramLM` aggregates token counts (`self.unigram_counts`) across the corpus.\n",
    "- Calling `.unigram_counts.most_common(n)` returns the top-n tokens along with their frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55c9d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Sequence, Union\n",
    "import math\n",
    "\n",
    "\n",
    "def ensure_tokens(sentence: Union[Sequence[str], str]) -> List[str]:\n",
    "    \"\"\"Convert whitespace-separated text or token sequences into a list.\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "class UnigramLM:\n",
    "    \"\"\"Laplace-smoothed unigram language model operating in log-space.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: Sequence[Sequence[str]]):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            tokens = ensure_tokens(sentence)\n",
    "            self.unigram_counts.update(tokens)\n",
    "            self.total_tokens += len(tokens)\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        if self.total_tokens == 0:\n",
    "            raise ValueError(\"Cannot train a UnigramLM on an empty corpus.\")\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def log_prob(self, word: str) -> float:\n",
    "        count = self.unigram_counts.get(word, 0)\n",
    "        return math.log((count + 1) / (self.total_tokens + self.vocab_size))\n",
    "\n",
    "    def sentence_log_prob(self, sentence: Union[Sequence[str], str]) -> float:\n",
    "        tokens = ensure_tokens(sentence)\n",
    "        if not tokens:\n",
    "            return float('-inf')\n",
    "        return sum(self.log_prob(token) for token in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072f95e",
   "metadata": {},
   "source": [
    "## 4. Task – Top 1000 Tokens\n",
    "\n",
    "### 4.1 Goal\n",
    "Identify the most frequent tokens in the corpus (with optional class-wise previews) and store them for later feature engineering.\n",
    "\n",
    "### 4.2 Approach\n",
    "1. Train the `UnigramLM` on the single-label subset.\n",
    "2. Retrieve `most_common(1000)` and inspect the first items.\n",
    "3. Optionally repeat the process for the most frequent classes to understand their characteristic vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa338605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected top 1000 tokens (showing the first 20):\n",
      "  new             -> 571\n",
      "  love            -> 499\n",
      "  day             -> 466\n",
      "  good            -> 431\n",
      "  game            -> 427\n",
      "  make            -> 412\n",
      "  year            -> 405\n",
      "  time            -> 394\n",
      "  watch           -> 383\n",
      "  happy           -> 344\n",
      "  come            -> 329\n",
      "  music           -> 319\n",
      "  like            -> 318\n",
      "  win             -> 307\n",
      "  great           -> 295\n",
      "  thank           -> 292\n",
      "  go              -> 292\n",
      "  video           -> 275\n",
      "  live            -> 272\n",
      "  today           -> 261\n",
      "\n",
      "Per-class token preview (Top 10 tokens for the most frequent labels):\n",
      "- sports (1181 docs): game (248), win (178), team (143), ufc (110), good (107), today (91), go (85), vs (83), time (82), make (81)\n",
      "- news_&_social_concern (625 docs): trump (97), president (76), news (57), people (55), world (44), woman (42), change (42), year (42), know (41), black (41)\n",
      "- music (439 docs): new (145), music (137), album (111), song (83), love (53), listen (52), video (51), live (44), spotify (37), check (35)\n",
      "\n",
      "Stored vocabulary length: 1000\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 1000\n",
    "\n",
    "# Gesamtvokabular\n",
    "corpus_tokens = [ensure_tokens(text) for text in single_label_df[\"text\"]]\n",
    "unigram_model = UnigramLM(corpus_tokens)\n",
    "\n",
    "top_unigrams = unigram_model.unigram_counts.most_common(MAX_FEATURES)\n",
    "print(f\"Collected top {len(top_unigrams)} tokens (showing the first 20):\")\n",
    "for token, freq in top_unigrams[:20]:\n",
    "    print(f\"  {token:<15} -> {freq}\")\n",
    "\n",
    "# Optional: per class preview for the three most frequent labels\n",
    "label_counts = single_label_df[\"label\"].value_counts().head(3)\n",
    "print(\"\\nPer-class token preview (Top 10 tokens for the most frequent labels):\")\n",
    "for label, count in label_counts.items():\n",
    "    label_corpus = [ensure_tokens(text) for text in single_label_df.loc[single_label_df[\"label\"] == label, \"text\"]]\n",
    "    label_model = UnigramLM(label_corpus)\n",
    "    label_top = label_model.unigram_counts.most_common(10)\n",
    "    formatted = \", \".join([f\"{tok} ({freq})\" for tok, freq in label_top])\n",
    "    print(f\"- {label} ({count} docs): {formatted}\")\n",
    "\n",
    "# Speichern des Vokabulars für spätere Schritte (falls benötigt)\n",
    "TOP_VOCABULARY = [token for token, _ in top_unigrams]\n",
    "print(f\"\\nStored vocabulary length: {len(TOP_VOCABULARY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207133",
   "metadata": {},
   "source": [
    "## 5. Task – Naive Bayes Setup (Placeholder)\n",
    "\n",
    "> To be added later: build the pipeline, split the data, and train the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'love', 'day', 'good', 'game', 'make', 'year', 'time', 'watch', 'happy', 'come', 'music', 'like', 'win', 'great', 'thank', 'go', 'video', 'live', 'today', 'world', 'get', 'look', 'need', 'know', 'play', 'people', 'show', 'work', 'team', 'family', 'think', 'check', 'hope', 'man', 'news', 'want', 'say', 'life', 'change', 'woman', 'night', 'morning', 'trump', 'album', 'song', 'th', 'listen', 'week', 'fight', 'let', 'help', 'guy', 'right', 'ufc', 'vs', 'bad', 'remember', 'way', 'tonight', 'season', 'home', 'big', 'stay', 'break', 'follow', 'climate', 'state', 'star', 'tell', 'sign', 'see', 'president', 'end', 'don', 'friend', 'stream', 'join', 'thing', 'power', 'talk', 'final', 'feel', 'fan', 'second', 'take', 'amazing', 'official', 'weekend', 'hour', 'league', 'wait', 'well', 'start', 'war', 'send', 'movie', 'stop', 'black', 'find', 'hard', 'boy', 'miss', 'story', 'pm', 'sunday', 'city', 'give', 'run', 'line', 'hear', 'try', 'vote', 'free', 'long', 'sta', 'school', 'country', 'lose', 'bring', 'decade', 'easter', 'fire', 'late', 'christmas', 'head', 'safe', 'super', 'open', 'white', 'series', 'leave', 'brown', 'post', 'social', 'st', 'sure', 'covid', 've', 'house', 'god', 'lot', 'app', 'united', 'match', 'episode', 'drop', 'place', 'jesus', 'internet', 'turn', 'forward', 'high', 'eid', 'sport', 'pa', 'close', 'half', 'release', 'away', 'player', 'proud', 'nfl', 'football', 'harry', 'little', 'book', 'light', 'call', 'wish', 'perfect', 'set', 'month', 'round', 'share', 'lead', 'girl', 'read', 'point', 'to', 'mother', 'petition', 'event', 'wake', 'future', 'national', 'ask', 'ground', 'pick', 'king', 'hit', 'hold', 'expect', 'update', 'real', 'sell', 'sugar', 'save', 'understand', 'add', 'film', 'bowl', 'main', 'sister', 'suppo', 'fall', 'government', 'strong', 'states', 'old', 'tomorrow', 'fun', 'tv', 'money', 'medium', 'rise', 'link', 'dear', 'not', 'mask', 'beat', 'cup', 'twitter', 'happen', 'll', 'york', 'use', 'job', 'enjoy', 'kid', 'moment', 'celebrate', 'watermelon', 'christ', 'distancing', 'write', 'chief', 'deserve', 'continue', 'early', 'beautiful', 'bay', 'blue', 'spotify', 'mean', 'field', 'tweet', 'bts', 'return', 'sad', 'daily', 'health', 'service', 'rest', 'dance', 'stand', 'coronavirus', 'race', 'party', 'apple', 'word', 'plan', 'bill', 'child', 'hand', 'style', 'mubarak', 'review', 'coach', 'police', 'youtube', 'pass', 'box', 'baby', 'father', 'catch', 'special', 'club', 'soon', 'single', 'far', 'die', 'dead', 'south', 'goal', 'office', 'order', 'july', 'kill', 'reason', 'hi', 'matter', 'bank', 'title', 'performance', 'red', 'saturday', 'request', 'face', 'question', 'wow', 'champion', 'congratulation', 'huge', 'oh', 'america', 'cancer', 'nowplaye', 'ago', 'care', 'finish', 'action', 'drive', 'uk', 'act', 'have', 'law', 'finally', 'fox', 'tune', 'food', 'cause', 'protest', 'fact', 'community', 'million', 'online', 'yes', 'awesome', 'friday', 'favorite', 'include', 'hea', 'cover', 'minute', 'card', 'storm', 'lord', 'number', 'entire', 'voice', 'yesterday', 'person', 'nice', 'student', 'support', 'lockdown', 'interview', 'host', 'history', 'poshmark', 'extra', 'issue', 'walk', 'roll', 'global', 'problem', 'young', 'didn', 'view', 'death', 'radio', 'score', 'record', 'allow', 'chance', 'believe', 'group', 'keep', 'podcast', 'air', 'de', 'mail', 'shut', 'feat', 'shoot', 'sit', 'american', 'pretty', 'anthony', 'pride', 'ok', 'report', 'speak', 'list', 'sir', 'later', 'joshua', 'track', 'kick', 'wilder', 'shopmycloset', 'excited', 'learn', 'gift', 'message', 'summer', 'raise', 'wonderful', 'virus', 'feature', 'earn', 'ahead', 'plus', 'case', 'past', 'experience', 'stuff', 'ready', 'rock', 'channel', 'room', 'travel', 'actually', 'eat', 'forget', 'bit', 'staff', 'winter', 'permission', 'pandemic', 'supreme', 'ticket', 'pay', 'conference', 'ohio', 'force', 'rule', 'closet', 'championship', 'grow', 'ice', 'dinner', 'animal', 'absolutely', 'ref', 'level', 'especially', 'step', 'pre', 'result', 'fury', 'premier', 'idea', 'drink', 'true', 'bear', 'charge', 'business', 'spend', 'article', 'class', 'sorry', 'member', 'mind', 'available', 'fly', 'wear', 'spread', 'offer', 'hot', 'answer', 'wonder', 'able', 'battle', 'trend', 'code', 'move', 'winner', 'reach', 'perform', 'store', 'queen', 'cou', 'cowboy', 'photo', 'pro', 'tyson', 'heat', 'create', 'dream', 'sound', 'meet', 'bless', 'breast', 'congrat', 'mr', 'er', 'glad', 'monday', 'enter', 'chris', 'thought', 'company', 'green', 'leader', 'artist', 'loss', 'la', 'respect', 'announce', 'brother', 'draw', 'nba', 'road', 'tag', 'street', 'human', 'important', 'cool', 'yo', 'bt', 'account', 'lyric', 'west', 'okay', 'playoff', 'area', 'deontay', 'local', 'comment', 'begin', 'crazy', 'justice', 'award', 'fake', 'holiday', 'throw', 'maybe', 'donate', 'lie', 'guess', 'provide', 'legend', 'international', 'xfl', 'cop', 'tie', 'sleep', 'completely', 'build', 'highlight', 'truly', 'different', 'john', 'present', 'park', 'system', 'protect', 'explain', 'practice', 'corner', 'middle', 'classic', 'clear', 'loud', 'ft', 'quarter', 'inside', 'east', 'delay', 'positive', 'mark', 'version', 'ball', 'deal', 'welcome', 'definitely', 'absolute', 'project', 'stage', 'sky', 'vaccine', 'pop', 'valimai', 'outside', 'bet', 'forever', 'laker', 'recommend', 'rip', 'bar', 'laugh', 'struggle', 'basketball', 'china', 'luck', 'cheer', 'campaign', 'defense', 'repo', 'date', 'fantastic', 'water', 'jones', 'speech', 'kind', 'tropical', 'np', 'note', 'master', 'character', 'sing', 'control', 'church', 'andy', 'bird', 'probably', 'defeat', 'distribute', 'mom', 'ur', 'lil', 'rick', 'eve', 'blood', 'shout', 'visit', 'joe', 'clean', 'put', 'public', 'county', 'dark', 'ross', 'officer', 'jr', 'car', 'energy', 'hype', 'draft', 'lovely', 'wanna', 'george', 'wrong', 'test', 'cream', 'twitch', 'train', 'boxing', 'debut', 'dr', 'brand', 'evening', 'peace', 'serve', 'land', 'florida', 'india', 'co', 'driver', 'el', 'fast', 'lady', 'flight', 'detail', 'market', 'sense', 'nd', 'thankful', 'ruiz', 'talent', 'solstice', 'chat', 'anniversary', 'phone', 'shot', 'crowd', 'career', 'college', 'picture', 'original', 'camp', 'prepare', 'seriously', 'key', 'saint', 'angeles', 'cross', 'town', 'body', 'slow', 'flag', 'strike', 'afternoon', 'literally', 'cry', 'lucky', 'low', 'crisis', 'eye', 'brilliant', 'course', 'role', 'highly', 'difference', 'decision', 'economy', 'choose', 'cast', 'truth', 'manchester', 'africa', 'cut', 'wall', 'omg', 'download', 'crossing', 'lopez', 'superstar', 'tran', 'gold', 'ban', 'possible', 'celebration', 'sun', 'non', 'cancel', 'weather', 'preview', 'door', 'tear', 'complete', 'worth', 'figure', 'demand', 'push', 'pray', 'march', 'education', 'science', 'despite', 'yeah', 'dc', 'insight', 'nyc', 'fair', 'knock', 'pls', 'victory', 'dj', 'fix', 'beer', 'joy', 'massive', 'disney', 'san', 'easy', 'couple', 'base', 'spring', 'will', 'age', 'americans', 'band', 'kanye', 'august', 'mix', 'heavyweight', 'skywalker', 'adam', 'wind', 'butter', 'prey', 'jennifer', 'resurrection', 'muslim', 'imagine', 'large', 'one', 'hopefully', 'johnson', 'min', 'giveaway', 'pull', 'bree', 'madrid', 'increase', 'trade', 'gas', 'patrick', 'appreciate', 'launch', 'till', 'crew', 'un', 'impact', 'platform', 'hell', 'table', 'opinion', 'protester', 'stock', 'democratic', 'count', 'hospital', 'england', 'remove', 'convention', 'small', 'streaming', 'champ', 'customer', 'spot', 'dad', 'biden', 'pack', 'foot', 'potter', 'target', 'praise', 'usa', 'sho', 'isn', 'sa', 'size', 'audio', 'tackle', 'discover', 'vibe', 'feeling', 'research', 'smith', 'dude', 'focus', 'david', 'ring', 'buy', 'nigeria', 'shoutout', 'broadcast', 'block', 'april', 'search', 'parent', 'mad', 'rapper', 'mostrequestedlive', 'double', 'rain', 'fighter', 'react', 'guide', 'paul', 'army', 'ya', 'steeler', 'soul', 'nowplaying', 'sb', 'aaron', 'superbowl', 'valentine', 'jacob', 'blake', 'dodger', 'schedule', 'son', 'prime', 'session', 'stadium', 'giant', 'ram', 'na', 'tournament', 'rd', 'vegas', 'org', 'dog', 'piece', 'fine', 'irish', 'remain', 'content', 'opportunity', 'memory', 'tech', 'bro', 'safety', 'notice', 'honestly', 'hero', 'receive', 'hug', 'excuse', 'francisco', 'term', 'powerful', 'tip', 'position', 'design', 'netflix', 'virtual', 'page', 'hey', 'tiger', 'straight', 'self', 'james', 'talented', 'direct', 'urban', 'chinese', 'worker', 'decide', 'incredible', 'blast', 'kansas', 'dress', 'meeting', 'icymi', 'tampa', 'center', 'grant', 'singer', 'prove', 'qb', 'reveal', 'director', 'bbc', 'address', 'hotel', 'tour', 'ride', 'inspire', 'major', 'central', 'bed', 'inspiration', 'copy', 'mental', 'digital', 'anti', 'badge', 'secret', 'healthy', 'kobe', 'board', 'cryptocurrency', 'xnk', 'nascar', 'quality', 'clipper', 'gun', 'google', 'favourite', 'process', 'training', 'actor', 'short', 'situation', 'hernandez', 'nation', 'type', 'dynamite', 'thursday', 'mobile', 'recap', 'fresh', 'raven', 'halftime', 'orleans', 'eagle', 'color', 'contact', 'challenge', 'ray', 'cc', 'humble', 'iran', 'prayer', 'trip', 'awareness', 'limit', 'reduce', 'coverage', 'fail', 'rally', 'earth', 'chart', 'military', 'policy', 'agree', 'accept', 'taylor', 'excellent', 'choice', 'christian', 'mayor', 'treat', 'edition', 'ppl', 'chelsea', 'display', 'payment', 'camino', 'epic', 'stone', 'smile', 'golf']\n"
     ]
    }
   ],
   "source": [
    "print(TOP_VOCABULARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c367c",
   "metadata": {},
   "source": [
    "Schritt 1: Labels vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5533d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-Matrix (Train): (4872, 1000)\n",
      "Beispiel-Features: ['new' 'love' 'day' 'good' 'game' 'make' 'year' 'time' 'watch' 'happy']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=TOP_VOCABULARY,  # hier wird dein gespeichertes Vokabular verwendet\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "\n",
    "# Texte in Zahlen umwandeln (Bag-of-Words)\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow  = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Feature-Matrix (Train):\", X_train_bow.shape)\n",
    "print(\"Beispiel-Features:\", vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c87f5",
   "metadata": {},
   "source": [
    "Multi-Label-Encoding für die Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f18d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Klassen: 405\n",
      "Beispiel Label: ['sports']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_tweets/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['arts_&_culturebusiness_&_entrepreneurs', 'arts_&_culturediaries_&_daily_lifefashion_&_style', 'arts_&_culturefamilyfashion_&_style', 'arts_&_culturefilm_tv_&_videonews_&_social_concernsports', 'arts_&_culturefilm_tv_&_videoother_hobbies', 'arts_&_culturefilm_tv_&_videorelationshipstravel_&_adventure', 'arts_&_culturefilm_tv_&_videotravel_&_adventure', 'arts_&_culturefitness_&_health', 'arts_&_cultureother_hobbiesrelationships', 'arts_&_culturesports', 'business_&_entrepreneurscelebrity_&_pop_culturemusic', 'business_&_entrepreneursdiaries_&_daily_lifefitness_&_healthnews_&_social_concern', 'business_&_entrepreneursdiaries_&_daily_lifefood_&_dining', 'business_&_entrepreneursdiaries_&_daily_lifenews_&_social_concernscience_&_technology', 'business_&_entrepreneursgamingsports', 'celebrity_&_pop_culturediaries_&_daily_lifefilm_tv_&_videomusicrelationships', 'celebrity_&_pop_culturediaries_&_daily_lifefood_&_dining', 'celebrity_&_pop_culturefilm_tv_&_videoother_hobbies', 'celebrity_&_pop_culturefood_&_diningother_hobbies', 'celebrity_&_pop_culturegamingnews_&_social_concernsports', 'celebrity_&_pop_culturemusicnews_&_social_concernrelationships', 'celebrity_&_pop_culturemusicnews_&_social_concernsports', 'celebrity_&_pop_cultureother_hobbiessports', 'diaries_&_daily_lifefamilyfilm_tv_&_videorelationships', 'diaries_&_daily_lifefamilymusicother_hobbies', 'diaries_&_daily_lifefamilynews_&_social_concernrelationships', 'diaries_&_daily_lifefashion_&_styleother_hobbies', 'diaries_&_daily_lifefashion_&_stylerelationships', 'diaries_&_daily_lifefitness_&_healthmusicsports', 'diaries_&_daily_lifefitness_&_healthnews_&_social_concernscience_&_technology', 'diaries_&_daily_lifefood_&_diningmusic', 'diaries_&_daily_lifegamingsports', 'diaries_&_daily_lifeother_hobbiesyouth_&_student_life', 'familyfilm_tv_&_videogamingrelationships', 'familyfilm_tv_&_videomusic', 'fashion_&_styleother_hobbiessports', 'film_tv_&_videofitness_&_healthscience_&_technology', 'film_tv_&_videolearning_&_educationalnews_&_social_concernscience_&_technology', 'film_tv_&_videotravel_&_adventure', 'fitness_&_healthlearning_&_educationalyouth_&_student_life', 'fitness_&_healthnews_&_social_concerntravel_&_adventure', 'gamingrelationships', 'learning_&_educationalmusic', 'other_hobbiesscience_&_technologytravel_&_adventure'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train)\n",
    "y_test_bin  = mlb.transform(y_test)\n",
    "\n",
    "print(\"Anzahl Klassen:\", len(mlb.classes_))\n",
    "print(\"Beispiel Label:\", y_train.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530085c",
   "metadata": {},
   "source": [
    "Naive-Bayes-Klassifikator trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a093478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Label Naive-Bayes-Modell trainiert!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# One-vs-Rest erlaubt Multi-Label-Training\n",
    "nb_clf = OneVsRestClassifier(MultinomialNB(alpha=1.0))\n",
    "nb_clf.fit(X_train_bow, y_train_bin)\n",
    "\n",
    "print(\"✅ Multi-Label Naive-Bayes-Modell trainiert!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d59f9",
   "metadata": {},
   "source": [
    "Erste Test-Vorhersagen prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b653f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: fresh find discover new music hot talent listen popstar dnb remix dooz...\n",
      "→ Vorhergesagte Labels: ('celebrity_&_pop_culturemusic', 'music')\n",
      "\n",
      "Text: stop love thing game tell bill room growth go to look forward see gobi...\n",
      "→ Vorhergesagte Labels: ()\n",
      "\n",
      "Text: putin say nord stream gas pipeline europe complete end year quarter wa...\n",
      "→ Vorhergesagte Labels: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "\n",
    "for text, pred in zip(X_test[:3], y_pred_labels[:3]):\n",
    "    print(f\"Text: {text[:70]}...\")\n",
    "    print(f\"→ Vorhergesagte Labels: {pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169817fd",
   "metadata": {},
   "source": [
    "## 6. Task – Evaluation & Error Analysis (Placeholder)\n",
    "\n",
    "> Once a classifier is trained, we will add metrics and example analyses here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf13550",
   "metadata": {},
   "source": [
    "Vorhersagen für dein Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23fe5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel-Vorhersagen:\n",
      "Text: fresh find discover new music hot talent listen popstar dnb remix doozy hiphop r...\n",
      "  Wahr: ('music',)\n",
      "  Vorhergesagt: ('celebrity_&_pop_culturemusic', 'music')\n",
      "\n",
      "Text: stop love thing game tell bill room growth go to look forward see gobill...\n",
      "  Wahr: ('gamingsports',)\n",
      "  Vorhergesagt: ()\n",
      "\n",
      "Text: putin say nord stream gas pipeline europe complete end year quarter wall street...\n",
      "  Wahr: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "  Vorhergesagt: ('business_&_entrepreneursnews_&_social_concern',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss, classification_report\n",
    "\n",
    "# Vorhersage (0/1-Matrix)\n",
    "y_pred_bin = nb_clf.predict(X_test_bow)\n",
    "\n",
    "# Labels wieder in Textform zurückwandeln\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_bin)\n",
    "y_true_labels = mlb.inverse_transform(y_test_bin)\n",
    "\n",
    "print(\"Beispiel-Vorhersagen:\")\n",
    "for text, true, pred in zip(X_test[:3], y_true_labels[:3], y_pred_labels[:3]):\n",
    "    print(f\"Text: {text[:80]}...\")\n",
    "    print(f\"  Wahr: {true}\")\n",
    "    print(f\"  Vorhergesagt: {pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
